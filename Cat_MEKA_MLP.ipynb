{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f278667b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1069b",
   "metadata": {},
   "source": [
    "## Root and data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068e5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "root_dir = \"/Users/silviumatu/Desktop/Code/Python/Disertatie/Disertatie_Matu_Silviu_v1\"\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "data_dir = os.path.join(root_dir, \"Data\")\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1166a5e",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d6a9787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_age_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_gender_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_previous_psychiatric_diagnostic_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_previous_psychiatric_treatment_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_previous_psychologist_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_curent_psychiatric_treatment_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_current_psychologist_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ABS_irrational_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ABS_rational_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ATS_Generalization_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ATS_High_Standards_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ATS_Self_Criticism_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BAS_Drive_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BAS_Fun_seeking_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BAS_Reward_responsivenes_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Acceptance_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Active_coping_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Behavioral_disengagement_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Denial_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Emotional_support_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Humor_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Informational_support_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Planning_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Positive_reframing_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Religion_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Self_blame_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Self_distraction_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Substance_use_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_Venting_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BIG5_Agreeableness_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BIG5_Conscientiousness_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BIG5_Extraversion_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BIG5_Neuroticism_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BIG5_Openness_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BIS_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_Childhood_Trauma_Screener_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_DASS_Stress_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_DASS21_Anxeity_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_DASS21_Depression_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ERQ_reappraisal_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ERQ_suppression_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_12_Anxiety_Depression_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_12_LossofConfidence_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_12_Social_Dysfunction_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_Insomnia_Severity_Index_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_Loneliness_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PDA_anger_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PDA_anxiety_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PDA_concern_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PDA_depression_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PDA_positive_emotions_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PDA_sadness_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_Perceived_Stress_Scale_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PERMA_Acomplishment_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PERMA_Engagement_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PERMA_Happyness_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PERMA_Health_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PERMA_Loneliness_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PERMA_Meaning_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PERMA_Negative_emotions_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PERMA_Positive_emotions_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PERMA_Relationships_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_POG_Lateral_generalization_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_POG_Social_generalization_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_POG_Upward_generalization_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_RIBS_Irratiobal_performance_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_SBQ_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_Self_Compassion_Scale_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_Somatic_Symptoms_Scale_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_Unsrealistic_standards_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_WHO5_well_being_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BAS_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_avoidant_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_emotion_focused_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BCOPE_problem_focused_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_12_TOTAL_score_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PDA_dysfcuntional_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_PDA_functional_TOTAL_baseline",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_time",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_number_interactions_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_sleep_last_night_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_talked_to_today_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_phone_today_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_1_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_2_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_3_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_4_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_5_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_6_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_7_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_8_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_9_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_10_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_11_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_12_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_13_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_14_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_15_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_16_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_17_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_CERQ_18_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_1_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_2_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_3_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_4_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_5_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_6_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_7_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_8_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_9_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_10_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_11_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_12_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ER_ESM_2_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ER_ESM_3_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ER_ESM_4_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ER_ESM_5_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ER_ESM_6_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ER_ESM_7_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ER_ESM_8_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ER_ESM_9_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ER_ESM_10_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ER_ESM_11_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ER_ESM_12_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_ER_ESM_13_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_Anxiety_and_Depression_TOTAL_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_Loss_of_Confidence_TOTAL_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_GHQ_Social_Dysfunction_TOTAL_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_stress_factors_first_day_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_factors_first_day_10",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_factors_first_day_11",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_factors_first_day_12",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_factors_first_day_13",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_factors_first_day_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_factors_first_day_3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_factors_first_day_4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_factors_first_day_5",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_factors_first_day_6",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_factors_first_day_7",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_factors_first_day_8",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_factors_first_day_9",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_nationality_recoded_baseline_value_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_nationality_recoded_baseline_value_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_nationality_recoded_baseline_value_3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_live_same_city_baseline_value_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_live_same_city_baseline_value_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_live_same_city_baseline_value_3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_live_same_city_baseline_value_4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_live_same_city_baseline_value_5",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_work_status_baseline_value_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_work_status_baseline_value_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_work_status_baseline_value_3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_siblings_baseline_value_0",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_siblings_baseline_value_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_siblings_baseline_value_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_siblings_baseline_value_3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_siblings_baseline_value_4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_siblings_baseline_value_6",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_siblings_baseline_value_7",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_familly_income_baseline_value_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_familly_income_baseline_value_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_familly_income_baseline_value_3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_home_town_type_baseline_value_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_home_town_type_baseline_value_4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_type_study_program_baseline_value_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_type_study_program_baseline_value_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_type_study_program_baseline_value_3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_relationship_baseline_value_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_relationship_baseline_value_4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_relationship_baseline_value_5",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_religion_baseline_value_4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_religion_baseline_value_5",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_religion_baseline_value_6",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_religion_baseline_value_7",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_religion_baseline_value_8",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_religion_baseline_value_10",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_religion_baseline_value_11",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_religion_baseline_value_12",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_religion_baseline_value_13",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_parents_higher_education_baseline_value_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_parents_higher_education_baseline_value_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_parents_higher_education_baseline_value_3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_parents_higher_education_baseline_value_4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_health_last_month_baseline_value_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_health_last_month_baseline_value_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_health_last_month_baseline_value_3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_health_last_month_baseline_value_4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_health_last_month_baseline_value_5",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_mental_health_last_month_baseline_value_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_mental_health_last_month_baseline_value_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_mental_health_last_month_baseline_value_3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_mental_health_last_month_baseline_value_4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_mental_health_last_month_baseline_value_5",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_management_first_day_value_0",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_management_first_day_value_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_stress_management_first_day_value_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_participant_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_GHQ_TOTAL_score_category_first_day",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_time_copy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_time_difference_first_day",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y_GHQ_TOTAL_score_category_next",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_time_difference_first_day_copy",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "4985b52b-63ac-4f01-9f18-a06064f80af5",
       "rows": [
        [
         "0",
         "19.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "45.0",
         "26.0",
         "8.0",
         "3.0",
         "6.0",
         "5.0",
         "9.0",
         "6.0",
         "2.0",
         "8.0",
         "4.0",
         "6.0",
         "6.0",
         "5.0",
         "6.0",
         "5.0",
         "6.0",
         "6.0",
         "8.0",
         "7.0",
         "5.0",
         "7.0",
         "8.0",
         "10.0",
         "10.0",
         "10.0",
         "6.0",
         "13.0",
         "17.0",
         "52.0",
         "30.0",
         "44.0",
         "28.0",
         "7.0",
         "11.0",
         "2.0",
         "14.0",
         "19.0",
         "9.0",
         "22.0",
         "14.0",
         "20.0",
         "31.0",
         "21.0",
         "24.0",
         "15.0",
         "23.0",
         "18.0",
         "5.0",
         "10.0",
         "9.0",
         "14.0",
         "23.0",
         "12.0",
         "26.0",
         "7.0",
         "9.0",
         "9.0",
         "16.0",
         "6.0",
         "47.0",
         "28.0",
         "34.0",
         "68.0",
         "20.0",
         "22.0",
         "34.0",
         "25.0",
         "27.0",
         "38.0",
         "40.0",
         "0.0",
         "4.0",
         "9.0",
         "4.0",
         "10.0",
         "5.0",
         "3.0",
         "4.0",
         "2.0",
         "5.0",
         "3.0",
         "5.0",
         "5.0",
         "3.0",
         "5.0",
         "1.0",
         "2.0",
         "5.0",
         "1.0",
         "1.0",
         "5.0",
         "3.0",
         "5.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "2.0",
         "3.0",
         "0.0",
         "1.0",
         "2.0",
         "2.0",
         "2.0",
         "0.0",
         "4.0",
         "0.0",
         "2.0",
         "2.0",
         "5.0",
         "2.0",
         "3.0",
         "5.0",
         "5.0",
         "3.0",
         "2.0",
         "5.0",
         "7.0",
         "4.0",
         "3.0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0.0",
         "1.0",
         "0.0",
         "1.0"
        ],
        [
         "1",
         "19.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "45.0",
         "26.0",
         "8.0",
         "3.0",
         "6.0",
         "5.0",
         "9.0",
         "6.0",
         "2.0",
         "8.0",
         "4.0",
         "6.0",
         "6.0",
         "5.0",
         "6.0",
         "5.0",
         "6.0",
         "6.0",
         "8.0",
         "7.0",
         "5.0",
         "7.0",
         "8.0",
         "10.0",
         "10.0",
         "10.0",
         "6.0",
         "13.0",
         "17.0",
         "52.0",
         "30.0",
         "44.0",
         "28.0",
         "7.0",
         "11.0",
         "2.0",
         "14.0",
         "19.0",
         "9.0",
         "22.0",
         "14.0",
         "20.0",
         "31.0",
         "21.0",
         "24.0",
         "15.0",
         "23.0",
         "18.0",
         "5.0",
         "10.0",
         "9.0",
         "14.0",
         "23.0",
         "12.0",
         "26.0",
         "7.0",
         "9.0",
         "9.0",
         "16.0",
         "6.0",
         "47.0",
         "28.0",
         "34.0",
         "68.0",
         "20.0",
         "22.0",
         "34.0",
         "25.0",
         "27.0",
         "38.0",
         "40.0",
         "1.0",
         "4.0",
         "8.0",
         "4.0",
         "214.0",
         "4.0",
         "4.0",
         "5.0",
         "1.0",
         "5.0",
         "4.0",
         "4.0",
         "5.0",
         "2.0",
         "4.0",
         "5.0",
         "1.0",
         "5.0",
         "1.0",
         "5.0",
         "5.0",
         "4.0",
         "5.0",
         "2.0",
         "1.0",
         "0.0",
         "1.0",
         "2.0",
         "2.0",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "4.0",
         "1.0",
         "5.0",
         "5.0",
         "5.0",
         "0.0",
         "5.0",
         "5.0",
         "5.0",
         "5.0",
         "5.0",
         "4.0",
         "6.0",
         "0.0",
         "5.0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1.0",
         "3.0",
         "1.0",
         "3.0"
        ],
        [
         "2",
         "19.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "45.0",
         "26.0",
         "8.0",
         "3.0",
         "6.0",
         "5.0",
         "9.0",
         "6.0",
         "2.0",
         "8.0",
         "4.0",
         "6.0",
         "6.0",
         "5.0",
         "6.0",
         "5.0",
         "6.0",
         "6.0",
         "8.0",
         "7.0",
         "5.0",
         "7.0",
         "8.0",
         "10.0",
         "10.0",
         "10.0",
         "6.0",
         "13.0",
         "17.0",
         "52.0",
         "30.0",
         "44.0",
         "28.0",
         "7.0",
         "11.0",
         "2.0",
         "14.0",
         "19.0",
         "9.0",
         "22.0",
         "14.0",
         "20.0",
         "31.0",
         "21.0",
         "24.0",
         "15.0",
         "23.0",
         "18.0",
         "5.0",
         "10.0",
         "9.0",
         "14.0",
         "23.0",
         "12.0",
         "26.0",
         "7.0",
         "9.0",
         "9.0",
         "16.0",
         "6.0",
         "47.0",
         "28.0",
         "34.0",
         "68.0",
         "20.0",
         "22.0",
         "34.0",
         "25.0",
         "27.0",
         "38.0",
         "40.0",
         "4.0",
         "11.0",
         "7.0",
         "9.0",
         "30.0",
         "2.0",
         "4.0",
         "1.0",
         "1.0",
         "5.0",
         "2.0",
         "4.0",
         "5.0",
         "2.0",
         "1.0",
         "5.0",
         "1.0",
         "5.0",
         "1.0",
         "5.0",
         "5.0",
         "3.0",
         "5.0",
         "3.0",
         "1.0",
         "0.0",
         "1.0",
         "3.0",
         "3.0",
         "1.0",
         "2.0",
         "3.0",
         "1.0",
         "1.0",
         "1.0",
         "5.0",
         "2.0",
         "5.0",
         "5.0",
         "5.0",
         "0.0",
         "5.0",
         "5.0",
         "5.0",
         "5.0",
         "3.0",
         "5.0",
         "10.0",
         "2.0",
         "8.0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "4.0",
         "1.0",
         "0.0",
         "1.0"
        ],
        [
         "3",
         "19.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "45.0",
         "26.0",
         "8.0",
         "3.0",
         "6.0",
         "5.0",
         "9.0",
         "6.0",
         "2.0",
         "8.0",
         "4.0",
         "6.0",
         "6.0",
         "5.0",
         "6.0",
         "5.0",
         "6.0",
         "6.0",
         "8.0",
         "7.0",
         "5.0",
         "7.0",
         "8.0",
         "10.0",
         "10.0",
         "10.0",
         "6.0",
         "13.0",
         "17.0",
         "52.0",
         "30.0",
         "44.0",
         "28.0",
         "7.0",
         "11.0",
         "2.0",
         "14.0",
         "19.0",
         "9.0",
         "22.0",
         "14.0",
         "20.0",
         "31.0",
         "21.0",
         "24.0",
         "15.0",
         "23.0",
         "18.0",
         "5.0",
         "10.0",
         "9.0",
         "14.0",
         "23.0",
         "12.0",
         "26.0",
         "7.0",
         "9.0",
         "9.0",
         "16.0",
         "6.0",
         "47.0",
         "28.0",
         "34.0",
         "68.0",
         "20.0",
         "22.0",
         "34.0",
         "25.0",
         "27.0",
         "38.0",
         "40.0",
         "5.0",
         "11.0",
         "7.0",
         "11.0",
         "330.0",
         "4.0",
         "4.0",
         "3.0",
         "4.0",
         "2.0",
         "5.0",
         "4.0",
         "5.0",
         "5.0",
         "3.0",
         "4.0",
         "5.0",
         "1.0",
         "1.0",
         "5.0",
         "2.0",
         "5.0",
         "5.0",
         "2.0",
         "2.0",
         "0.0",
         "0.0",
         "2.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "3.0",
         "0.0",
         "0.0",
         "3.0",
         "0.0",
         "2.0",
         "4.0",
         "2.0",
         "4.0",
         "5.0",
         "4.0",
         "5.0",
         "5.0",
         "1.0",
         "4.0",
         "4.0",
         "3.0",
         "2.0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "5.0",
         "3.0",
         "0.0",
         "3.0"
        ],
        [
         "4",
         "18.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "44.0",
         "48.0",
         "10.0",
         "6.0",
         "6.0",
         "8.0",
         "8.0",
         "10.0",
         "5.0",
         "4.0",
         "5.0",
         "6.0",
         "7.0",
         "4.0",
         "6.0",
         "6.0",
         "6.0",
         "5.0",
         "5.0",
         "7.0",
         "7.0",
         "5.0",
         "8.0",
         "7.0",
         "10.0",
         "8.0",
         "8.0",
         "15.0",
         "13.0",
         "44.0",
         "28.0",
         "20.0",
         "34.0",
         "20.0",
         "9.0",
         "3.0",
         "13.0",
         "26.0",
         "7.0",
         "18.0",
         "15.0",
         "19.0",
         "19.0",
         "35.0",
         "12.0",
         "15.0",
         "23.0",
         "22.0",
         "6.0",
         "17.0",
         "4.0",
         "21.0",
         "23.0",
         "22.0",
         "17.0",
         "13.0",
         "12.0",
         "10.0",
         "12.0",
         "9.0",
         "44.0",
         "26.0",
         "42.0",
         "60.0",
         "26.0",
         "25.0",
         "31.0",
         "22.0",
         "25.0",
         "29.0",
         "28.0",
         "0.0",
         "5.0",
         "9.0",
         "5.0",
         "0.0",
         "2.0",
         "3.0",
         "4.0",
         "3.0",
         "4.0",
         "3.0",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "4.0",
         "3.0",
         "4.0",
         "1.0",
         "2.0",
         "3.0",
         "2.0",
         "4.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "2.0",
         "2.0",
         "2.0",
         "2.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "4.0",
         "4.0",
         "3.0",
         "1.0",
         "4.0",
         "1.0",
         "3.0",
         "4.0",
         "2.0",
         "3.0",
         "4.0",
         "2.0",
         "5.0",
         "0.0",
         "7.0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "2",
         "1",
         "0.0",
         "2.0",
         "1.0",
         "2.0"
        ]
       ],
       "shape": {
        "columns": 202,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_age_baseline</th>\n",
       "      <th>x_gender_baseline</th>\n",
       "      <th>x_previous_psychiatric_diagnostic_baseline</th>\n",
       "      <th>x_previous_psychiatric_treatment_baseline</th>\n",
       "      <th>x_previous_psychologist_baseline</th>\n",
       "      <th>x_curent_psychiatric_treatment_baseline</th>\n",
       "      <th>x_current_psychologist_baseline</th>\n",
       "      <th>x_ABS_irrational_TOTAL_baseline</th>\n",
       "      <th>x_ABS_rational_TOTAL_baseline</th>\n",
       "      <th>x_ATS_Generalization_TOTAL_baseline</th>\n",
       "      <th>...</th>\n",
       "      <th>x_mental_health_last_month_baseline_value_5</th>\n",
       "      <th>x_stress_management_first_day_value_0</th>\n",
       "      <th>x_stress_management_first_day_value_1</th>\n",
       "      <th>x_stress_management_first_day_value_2</th>\n",
       "      <th>x_participant_id</th>\n",
       "      <th>x_GHQ_TOTAL_score_category_first_day</th>\n",
       "      <th>x_time_copy</th>\n",
       "      <th>x_time_difference_first_day</th>\n",
       "      <th>y_GHQ_TOTAL_score_category_next</th>\n",
       "      <th>x_time_difference_first_day_copy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_age_baseline  x_gender_baseline  \\\n",
       "0            19.0                0.0   \n",
       "1            19.0                0.0   \n",
       "2            19.0                0.0   \n",
       "3            19.0                0.0   \n",
       "4            18.0                0.0   \n",
       "\n",
       "   x_previous_psychiatric_diagnostic_baseline  \\\n",
       "0                                         0.0   \n",
       "1                                         0.0   \n",
       "2                                         0.0   \n",
       "3                                         0.0   \n",
       "4                                         0.0   \n",
       "\n",
       "   x_previous_psychiatric_treatment_baseline  \\\n",
       "0                                        0.0   \n",
       "1                                        0.0   \n",
       "2                                        0.0   \n",
       "3                                        0.0   \n",
       "4                                        0.0   \n",
       "\n",
       "   x_previous_psychologist_baseline  x_curent_psychiatric_treatment_baseline  \\\n",
       "0                               1.0                                      0.0   \n",
       "1                               1.0                                      0.0   \n",
       "2                               1.0                                      0.0   \n",
       "3                               1.0                                      0.0   \n",
       "4                               0.0                                      0.0   \n",
       "\n",
       "   x_current_psychologist_baseline  x_ABS_irrational_TOTAL_baseline  \\\n",
       "0                              0.0                             45.0   \n",
       "1                              0.0                             45.0   \n",
       "2                              0.0                             45.0   \n",
       "3                              0.0                             45.0   \n",
       "4                              0.0                             44.0   \n",
       "\n",
       "   x_ABS_rational_TOTAL_baseline  x_ATS_Generalization_TOTAL_baseline  ...  \\\n",
       "0                           26.0                                  8.0  ...   \n",
       "1                           26.0                                  8.0  ...   \n",
       "2                           26.0                                  8.0  ...   \n",
       "3                           26.0                                  8.0  ...   \n",
       "4                           48.0                                 10.0  ...   \n",
       "\n",
       "   x_mental_health_last_month_baseline_value_5  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "\n",
       "   x_stress_management_first_day_value_0  \\\n",
       "0                                      0   \n",
       "1                                      0   \n",
       "2                                      0   \n",
       "3                                      0   \n",
       "4                                      0   \n",
       "\n",
       "   x_stress_management_first_day_value_1  \\\n",
       "0                                      0   \n",
       "1                                      1   \n",
       "2                                      1   \n",
       "3                                      1   \n",
       "4                                      0   \n",
       "\n",
       "   x_stress_management_first_day_value_2  x_participant_id  \\\n",
       "0                                      1                 0   \n",
       "1                                      0                 0   \n",
       "2                                      0                 0   \n",
       "3                                      0                 0   \n",
       "4                                      1                 2   \n",
       "\n",
       "   x_GHQ_TOTAL_score_category_first_day  x_time_copy  \\\n",
       "0                                     1          0.0   \n",
       "1                                     0          1.0   \n",
       "2                                     1          4.0   \n",
       "3                                     0          5.0   \n",
       "4                                     1          0.0   \n",
       "\n",
       "   x_time_difference_first_day  y_GHQ_TOTAL_score_category_next  \\\n",
       "0                          1.0                              0.0   \n",
       "1                          3.0                              1.0   \n",
       "2                          1.0                              0.0   \n",
       "3                          3.0                              0.0   \n",
       "4                          2.0                              1.0   \n",
       "\n",
       "   x_time_difference_first_day_copy  \n",
       "0                               1.0  \n",
       "1                               3.0  \n",
       "2                               1.0  \n",
       "3                               3.0  \n",
       "4                               2.0  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GHQ_cat_df = pd.read_csv(os.path.join(data_dir, \"PED_GHQ_categorical_data_forecast.csv\"))\n",
    "columns_GHQ_cat_df = pd.read_csv(os.path.join(data_dir, \"columns_PED_GHQ_categorical_data_forecast_S.csv\"))\n",
    "\n",
    "GHQ_cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c174fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the outcome column(s) marked with 1 in the \"outcomes\" column of columns_GHQ_cat_df\n",
    "GHQ_cat_outcome_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['outcomes'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_y = GHQ_cat_df[GHQ_cat_outcome_cols]\n",
    "GHQ_cat_y.head()\n",
    "\n",
    "# Same for outcomes lags column(s)\n",
    "GHQ_cat_outcomes_lags_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['outcomes_lags'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_outcomes_lags = GHQ_cat_df[GHQ_cat_outcomes_lags_cols]\n",
    "\n",
    "# Same for participant column(s)\n",
    "GHQ_cat_participant_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['participant_id'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_participant_id = GHQ_cat_df[GHQ_cat_participant_cols]\n",
    "\n",
    "# Same for time column(s)\n",
    "GHQ_cat_time_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['time'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_time = GHQ_cat_df[GHQ_cat_time_cols]\n",
    "\n",
    "# Same for forecast horizons column(s)\n",
    "GHQ_cat_forecast_horizons_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['forecast_horizons'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_forecast_horizons = GHQ_cat_df[GHQ_cat_forecast_horizons_cols]\n",
    "\n",
    "# Same for fixed effects column(s)\n",
    "GHQ_cat_only_fixed_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['only_fixed'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_only_fixed = GHQ_cat_df[GHQ_cat_only_fixed_cols]\n",
    "\n",
    "# Same for random effects column(s)\n",
    "GHQ_cat_fixed_and_random_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['fixed_and_random'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_fixed_and_random = GHQ_cat_df[GHQ_cat_fixed_and_random_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b382d0b",
   "metadata": {},
   "source": [
    "# Custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8845d70a",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e00bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "\n",
    "try:\n",
    "    from efficient_kan import KANLinear  # noqa: F401\n",
    "except Exception:\n",
    "    class KANLinear(nn.Module):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__()\n",
    "        def forward(self, x):\n",
    "            return x\n",
    "        def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "            return torch.tensor(0.0, device=torch.device(\"cpu\"))\n",
    "\n",
    "def _make_kan(in_dim, out_dim,\n",
    "              grid_size=8, spline_order=3,\n",
    "              scale_noise=0.1, scale_base=1.0, scale_spline=1.0,\n",
    "              enable_standalone_scale_spline=True,\n",
    "              base_activation=torch.nn.SiLU,\n",
    "              grid_eps=0.02, grid_range=(-1.0, 1.0)):\n",
    "    return nn.Linear(in_dim, out_dim)\n",
    "\n",
    "class KANBlock(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims=(128, 64), out_dim=None,\n",
    "                 dropout=0.0,\n",
    "                 grid_size=8, spline_order=3,\n",
    "                 scale_noise=0.1, scale_base=1.0, scale_spline=1.0,\n",
    "                 enable_standalone_scale_spline=True,\n",
    "                 base_activation=torch.nn.SiLU,\n",
    "                 grid_eps=0.02, grid_range=(-1.0, 1.0)):\n",
    "        super().__init__()\n",
    "        dims = [in_dim] + list(hidden_dims)\n",
    "        act = base_activation()\n",
    "        layers = []\n",
    "        for d0, d1 in zip(dims[:-1], dims[1:]):\n",
    "            layers.append(nn.Linear(d0, d1))\n",
    "            layers.append(act)\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "        if out_dim is not None:\n",
    "            layers.append(nn.Linear(dims[-1], out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        reg = 0.0\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, KANLinear):\n",
    "                reg = reg + m.regularization_loss(\n",
    "                    regularize_activation=regularize_activation,\n",
    "                    regularize_entropy=regularize_entropy\n",
    "                )\n",
    "        return reg\n",
    "\n",
    "class TemporalKernelAttentionKAN(nn.Module):\n",
    "    def __init__(self, n_kernels: int = 4, d_att: int = 32,\n",
    "                 grid_size=8, spline_order=3, dropout=0.0,\n",
    "                 normalize_weights: bool = False):\n",
    "        super().__init__()\n",
    "        self.n_k = n_kernels\n",
    "        self.normalize = normalize_weights\n",
    "        self.pi_logits = nn.Parameter(torch.zeros(n_kernels))\n",
    "        self.lam_raw = nn.Parameter(torch.zeros(n_kernels))\n",
    "        self.summarize = KANBlock(1, hidden_dims=(d_att,), out_dim=d_att,\n",
    "                                  dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "        self.out_head = KANBlock(d_att, hidden_dims=(d_att,), out_dim=1,\n",
    "                                 dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, y_lags, dt_lags):\n",
    "        B, L = y_lags.shape\n",
    "        pi = F.softmax(self.pi_logits, dim=-1)\n",
    "        lam = F.softplus(self.lam_raw) + 1e-6\n",
    "        pi_exp = pi.view(1, 1, -1).expand(B, 1, -1)\n",
    "        lam_exp = lam.view(1, 1, -1).expand(B, 1, -1)\n",
    "        kernel = torch.exp(-lam_exp * dt_lags.unsqueeze(-1))\n",
    "        w_lags = torch.sum(pi_exp * kernel, dim=-1)\n",
    "        if self.normalize:\n",
    "            w_sum = w_lags.sum(dim=1, keepdim=True) + 1e-8\n",
    "            w_lags = w_lags / w_sum\n",
    "        s = torch.sum(w_lags * y_lags, dim=1, keepdim=True)\n",
    "        z_att = self.summarize(s)\n",
    "        e_att = self.out_head(z_att)\n",
    "        return e_att, w_lags, z_att\n",
    "\n",
    "class FixedBranchKAN(nn.Module):\n",
    "    def __init__(self, d_fix, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = KANBlock(d_fix, hidden_dims=(256, 128), out_dim=d_latent,\n",
    "                            dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "        self.head = KANBlock(d_latent, hidden_dims=(64,), out_dim=1,\n",
    "                             dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, X_fix):\n",
    "        z = self.enc(X_fix)\n",
    "        e = self.head(z)\n",
    "        return e, z\n",
    "\n",
    "class RandEncoderKAN(nn.Module):\n",
    "    def __init__(self, d_zrand, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = KANBlock(d_zrand, hidden_dims=(256, 128), out_dim=d_latent,\n",
    "                            dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, Zrand):\n",
    "        return self.enc(Zrand)\n",
    "\n",
    "class TCEncoderKAN(nn.Module):\n",
    "    def __init__(self, d_tc, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = KANBlock(d_tc, hidden_dims=(256, 128), out_dim=d_latent,\n",
    "                            dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, TC):\n",
    "        return self.enc(TC)\n",
    "\n",
    "class RandomHeadKAN(nn.Module):\n",
    "    def __init__(self, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.head = KANBlock(d_latent, hidden_dims=(64,), out_dim=1,\n",
    "                             dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.head(z)\n",
    "\n",
    "class FiLMFromTC(nn.Module):\n",
    "    def __init__(self, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.gamma = KANBlock(d_latent, hidden_dims=(64,), out_dim=d_latent,\n",
    "                              dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "        self.beta = KANBlock(d_latent, hidden_dims=(64,), out_dim=d_latent,\n",
    "                             dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, e_tc):\n",
    "        return self.gamma(e_tc), self.beta(e_tc)\n",
    "\n",
    "class KANAdditiveMixedEffects(nn.Module):\n",
    "    def __init__(self,\n",
    "                 y_dim: int,\n",
    "                 d_fix: int,\n",
    "                 d_tc: int,\n",
    "                 d_zrand: int,\n",
    "                 n_ids: int = None,\n",
    "                 use_id_intercept: bool = False,\n",
    "                 n_kernels: int = 4,\n",
    "                 d_att: int = 128,\n",
    "                 d_fix_latent: int = 128,\n",
    "                 d_rand_latent: int = 128,\n",
    "                 grid_size: int = 8,\n",
    "                 spline_order: int = 3,\n",
    "                 dropout: float = 0.0,\n",
    "                 normalize_att_weights: bool = True,\n",
    "                 use_attention: bool = True,\n",
    "                 use_random: bool = True):\n",
    "        super().__init__()\n",
    "        self.y_dim = y_dim\n",
    "        self.d_fix = d_fix\n",
    "        self.d_tc  = d_tc\n",
    "        self.d_zr  = d_zrand\n",
    "        self.d_rand_latent = d_rand_latent\n",
    "        self.d_att = d_att\n",
    "        self.use_attention = use_attention\n",
    "        self.use_random = use_random\n",
    "\n",
    "        self.att_branches = nn.ModuleList([\n",
    "            TemporalKernelAttentionKAN(n_kernels=n_kernels, d_att=d_att,\n",
    "                                       grid_size=grid_size, spline_order=spline_order,\n",
    "                                       dropout=dropout, normalize_weights=normalize_att_weights)\n",
    "            for _ in range(y_dim)\n",
    "        ]) if use_attention else None\n",
    "\n",
    "        self.fix_branches = nn.ModuleList([\n",
    "            FixedBranchKAN(d_fix=d_fix, d_latent=d_fix_latent,\n",
    "                           grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "            for _ in range(y_dim)\n",
    "        ])\n",
    "\n",
    "        if use_random:\n",
    "            self.rand_encoders = nn.ModuleList([\n",
    "                RandEncoderKAN(d_zrand=d_zrand, d_latent=d_rand_latent,\n",
    "                               grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "                for _ in range(y_dim)\n",
    "            ]) if d_zrand > 0 else None\n",
    "            self.tc_encoders = nn.ModuleList([\n",
    "                TCEncoderKAN(d_tc=d_tc, d_latent=d_rand_latent,\n",
    "                             grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "                for _ in range(y_dim)\n",
    "            ]) if d_tc > 0 else None\n",
    "            self.film_from_tc = nn.ModuleList([\n",
    "                FiLMFromTC(d_latent=d_rand_latent, grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "                for _ in range(y_dim)\n",
    "            ]) if d_tc > 0 else None\n",
    "            self.rand_heads = nn.ModuleList([\n",
    "                RandomHeadKAN(d_latent=d_rand_latent, grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "                for _ in range(y_dim)\n",
    "            ])\n",
    "        else:\n",
    "            self.rand_encoders = None\n",
    "            self.tc_encoders = None\n",
    "            self.film_from_tc = None\n",
    "            self.rand_heads = None\n",
    "\n",
    "    def forward(self, X_fix, TC, Zrand, y_lags, dt_lags,\n",
    "                pid_idx: Optional[torch.Tensor] = None,\n",
    "                pid_seen_mask: Optional[torch.Tensor] = None):\n",
    "        B = X_fix.size(0)\n",
    "\n",
    "        if y_lags.dim() == 2:\n",
    "            y_lags_list = [y_lags for _ in range(self.y_dim)]\n",
    "        elif y_lags.dim() == 3:\n",
    "            assert y_lags.size(2) == self.y_dim\n",
    "            y_lags_list = [y_lags[:, :, j] for j in range(self.y_dim)]\n",
    "        else:\n",
    "            raise ValueError(\"y_lags must be [B, L] or [B, L, y_dim].\")\n",
    "\n",
    "        e_att_all, e_fix_all, e_rand_all = [], [], []\n",
    "        z_att_list, z_fix_list = [], []\n",
    "        z_rand_list, z_rand_film_list, z_tc_list = [], [], []\n",
    "        w_lags_list, e_rand_mod_list, gamma_list, beta_list = [], [], [], []\n",
    "\n",
    "        L = dt_lags.size(1) if dt_lags is not None and dt_lags.dim() == 2 else 1\n",
    "\n",
    "        for j in range(self.y_dim):\n",
    "            if self.use_attention and (self.att_branches is not None):\n",
    "                e_att_j, w_lags_j, z_att_j = self.att_branches[j](y_lags_list[j], dt_lags)\n",
    "            else:\n",
    "                e_att_j = torch.zeros(B, 1, device=X_fix.device, dtype=X_fix.dtype)\n",
    "                w_lags_j = torch.zeros(B, L, device=X_fix.device, dtype=X_fix.dtype)\n",
    "                z_att_j = torch.zeros(B, self.d_att, device=X_fix.device, dtype=X_fix.dtype)\n",
    "\n",
    "            e_fix_j, z_fix_j = self.fix_branches[j](X_fix)\n",
    "\n",
    "            if self.use_random and (self.rand_heads is not None):\n",
    "                if (Zrand is not None) and (Zrand.size(1) > 0) and (self.rand_encoders is not None):\n",
    "                    z_rand_j = self.rand_encoders[j](Zrand)\n",
    "                else:\n",
    "                    z_rand_j = torch.zeros(B, self.d_rand_latent, device=X_fix.device, dtype=X_fix.dtype)\n",
    "\n",
    "                if self.d_tc > 0 and (TC is not None) and (TC.size(1) > 0) and (self.tc_encoders is not None):\n",
    "                    z_tc_j = self.tc_encoders[j](TC)\n",
    "                    gamma_j, beta_j = self.film_from_tc[j](z_tc_j) if self.film_from_tc is not None else (torch.ones_like(z_rand_j), torch.zeros_like(z_rand_j))\n",
    "                else:\n",
    "                    z_tc_j = torch.zeros_like(z_rand_j)\n",
    "                    gamma_j = torch.ones_like(z_rand_j)\n",
    "                    beta_j  = torch.zeros_like(z_rand_j)\n",
    "\n",
    "                z_tilde_j = gamma_j * z_rand_j + beta_j\n",
    "                e_rand_j  = self.rand_heads[j](z_tilde_j)\n",
    "            else:\n",
    "                z_rand_j = torch.zeros(B, self.d_rand_latent, device=X_fix.device, dtype=X_fix.dtype)\n",
    "                z_tc_j   = torch.zeros_like(z_rand_j)\n",
    "                gamma_j  = torch.ones_like(z_rand_j)\n",
    "                beta_j   = torch.zeros_like(z_rand_j)\n",
    "                z_tilde_j= z_rand_j\n",
    "                e_rand_j = torch.zeros(B, 1, device=X_fix.device, dtype=X_fix.dtype)\n",
    "\n",
    "            e_att_all.append(e_att_j)\n",
    "            e_fix_all.append(e_fix_j)\n",
    "            e_rand_all.append(e_rand_j)\n",
    "\n",
    "            z_att_list.append(z_att_j)\n",
    "            z_fix_list.append(z_fix_j)\n",
    "            z_rand_list.append(z_rand_j)\n",
    "            z_rand_film_list.append(z_tilde_j)\n",
    "            z_tc_list.append(z_tc_j)\n",
    "\n",
    "            w_lags_list.append(w_lags_j)\n",
    "            e_rand_mod_list.append(e_rand_j)\n",
    "            gamma_list.append(gamma_j)\n",
    "            beta_list.append(beta_j)\n",
    "\n",
    "        e_att = torch.cat(e_att_all, dim=1) if e_att_all else torch.zeros(B, self.y_dim, device=X_fix.device, dtype=X_fix.dtype)\n",
    "        e_fix = torch.cat(e_fix_all, dim=1) if e_fix_all else torch.zeros(B, self.y_dim, device=X_fix.device, dtype=X_fix.dtype)\n",
    "        e_rand= torch.cat(e_rand_all,dim=1) if e_rand_all else torch.zeros(B, self.y_dim, device=X_fix.device, dtype=X_fix.dtype)\n",
    "        logits = e_att + e_fix + e_rand\n",
    "\n",
    "        parts = {\n",
    "            \"e_att\": e_att,\n",
    "            \"e_fix\": e_fix,\n",
    "            \"e_rand\": e_rand,\n",
    "            \"z_att_list\": z_att_list,\n",
    "            \"z_fix_list\": z_fix_list,\n",
    "            \"z_rand_list\": z_rand_list,\n",
    "            \"z_rand_film_list\": z_rand_film_list,\n",
    "            \"z_tc_list\": z_tc_list,\n",
    "            \"w_lags_list\": w_lags_list,\n",
    "            \"e_rand_mod_list\": e_rand_mod_list,\n",
    "            \"film_gamma_list\": gamma_list,\n",
    "            \"film_beta_list\": beta_list,\n",
    "        }\n",
    "        return logits, parts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba99af",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b19c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Optional, Tuple, Any\n",
    "\n",
    "def _bce_logits(\n",
    "    y_hat_logits: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    pos_weight: Optional[torch.Tensor] = None,\n",
    "    sample_weight: Optional[torch.Tensor] = None,\n",
    ") -> torch.Tensor:\n",
    "    if y_true.shape != y_hat_logits.shape:\n",
    "        if y_true.dim() == 1 and y_hat_logits.dim() == 2 and y_hat_logits.size(1) == 1:\n",
    "            y_true = y_true.unsqueeze(1)\n",
    "        else:\n",
    "            raise ValueError(f\"y_true shape {y_true.shape} must equal logits shape {y_hat_logits.shape}\")\n",
    "    pw = None\n",
    "    if pos_weight is not None:\n",
    "        pw = pos_weight.to(y_hat_logits.device, dtype=y_hat_logits.dtype)\n",
    "        if pw.numel() == 1:\n",
    "            pw = pw.view(1)\n",
    "        elif y_hat_logits.dim() == 2 and pw.numel() == y_hat_logits.size(1):\n",
    "            pw = pw.view(-1)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"pos_weight has {pw.numel()} elements but needs 1 or {y_hat_logits.size(-1)}\"\n",
    "            )\n",
    "    sw = None\n",
    "    if sample_weight is not None:\n",
    "        sw = sample_weight.to(y_hat_logits.device, dtype=y_hat_logits.dtype)\n",
    "    return F.binary_cross_entropy_with_logits(\n",
    "        y_hat_logits, y_true.float(), weight=sw, pos_weight=pw\n",
    "    )\n",
    "\n",
    "def _orthogonality_penalty_latents(z_fix: torch.Tensor, z_rand_film: torch.Tensor) -> torch.Tensor:\n",
    "    B = z_fix.size(0)\n",
    "    if B <= 1:\n",
    "        return z_fix.new_zeros(())\n",
    "    zf = z_fix - z_fix.mean(dim=0, keepdim=True)\n",
    "    zr = z_rand_film - z_rand_film.mean(dim=0, keepdim=True)\n",
    "    M = (zf.T @ zr) / float(B)\n",
    "    return (M ** 2).mean()\n",
    "\n",
    "def _optimal_threshold_exact_np(y_true: np.ndarray, y_prob: np.ndarray, beta: float = 1.0) -> float:\n",
    "    y = np.asarray(y_true, dtype=int).ravel()\n",
    "    p = np.asarray(y_prob, dtype=float).ravel()\n",
    "    if y.size == 0:\n",
    "        return 0.5\n",
    "    P = int(y.sum()); N = y.size - P\n",
    "    if P == 0:\n",
    "        return 1.0\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "    o = np.argsort(-p)\n",
    "    p = p[o]; y = y[o]\n",
    "    tp = np.cumsum(y)\n",
    "    fp = np.cumsum(1 - y)\n",
    "    prec = tp / np.maximum(1, tp + fp)\n",
    "    rec  = tp / max(1, P)\n",
    "    f = (1 + beta**2) * prec * rec / np.maximum(1e-12, beta**2 * prec + rec)\n",
    "    idx = int(np.nanargmax(f))\n",
    "    next_p = p[idx+1] if idx + 1 < len(p) else -np.inf\n",
    "    thr = (p[idx] + next_p) / 2.0 if np.isfinite(next_p) and next_p < p[idx] else max(0.0, p[idx] - np.finfo(p.dtype).eps)\n",
    "    return float(thr)\n",
    "\n",
    "def _combine_logits_from_parts(parts: dict, combine: str = \"all\"):\n",
    "    alias = {\n",
    "        \"time_constant\": \"only_fixed\",\n",
    "        \"time_varying\": \"all\",\n",
    "        \"fixed\": \"only_fixed\",\n",
    "    }\n",
    "    mode = alias.get(combine, combine)\n",
    "    e_fix  = parts[\"e_fix\"]\n",
    "    e_rand = parts.get(\"e_rand\", None)\n",
    "    e_att  = parts.get(\"e_att\", None)\n",
    "    if mode == \"only_fixed\":\n",
    "        return e_fix\n",
    "    elif mode == \"fixed_and_random\":\n",
    "        if e_rand is None:\n",
    "            e_rand = torch.zeros_like(e_fix)\n",
    "        return e_fix + e_rand\n",
    "    elif mode == \"all\":\n",
    "        if e_rand is None:\n",
    "            e_rand = torch.zeros_like(e_fix)\n",
    "        if e_att is None:\n",
    "            e_att = torch.zeros_like(e_fix)\n",
    "        return e_fix + e_rand + e_att\n",
    "    else:\n",
    "        raise ValueError(\"combine must be one of {'only_fixed','fixed_and_random','all','time_constant','time_varying','fixed'}\")\n",
    "\n",
    "class KANMixedEffectsWrapper:\n",
    "    def __init__(self, model: nn.Module, cfg: Optional[Dict[str, Any]] = None, device: Optional[torch.device] = None):\n",
    "        self.model = model\n",
    "        self.cfg = {\n",
    "            \"lambda_mean0\": 1e-4,\n",
    "            \"lambda_ridge\": 1e-4,\n",
    "            \"lambda_orth_latent\": 1e-3,\n",
    "            \"lambda_film_identity\": 1e-4,\n",
    "            \"lambda_kan\": 0.0,\n",
    "            \"kan_reg_activation\": 1.0,\n",
    "            \"kan_reg_entropy\": 1.0,\n",
    "            \"clip_grad\": 5.0,\n",
    "            \"lr\": 1e-3,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"max_epochs\": 100,\n",
    "            \"patience\": 10,\n",
    "            \"batch_size\": 256,\n",
    "            \"amp\": True,\n",
    "            \"threshold\": 0.5,\n",
    "            \"auto_pos_weight\": True,\n",
    "            \"pos_weight_eps\": 1e-6,\n",
    "            \"sample_weight_index\": None,\n",
    "        }\n",
    "        if cfg:\n",
    "            self.cfg.update(cfg)\n",
    "        self.device = (\n",
    "            device\n",
    "            or (torch.device(\"mps\") if torch.backends.mps.is_available() else None)\n",
    "            or (torch.device(\"cuda\") if torch.cuda.is_available() else None)\n",
    "            or torch.device(\"cpu\")\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.thresholds_: Optional[np.ndarray] = None\n",
    "        self.history_: Dict[str, list] = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X_fix: torch.Tensor,\n",
    "        TC: Optional[torch.Tensor],\n",
    "        Zrand: Optional[torch.Tensor],\n",
    "        y_lags: torch.Tensor,\n",
    "        dt_lags: torch.Tensor,\n",
    "        pid_idx: Optional[torch.Tensor] = None,\n",
    "        pid_seen_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        return self.model(\n",
    "            X_fix=X_fix,\n",
    "            TC=TC,\n",
    "            Zrand=Zrand,\n",
    "            y_lags=y_lags,\n",
    "            dt_lags=dt_lags,\n",
    "            pid_idx=pid_idx,\n",
    "            pid_seen_mask=pid_seen_mask\n",
    "        )\n",
    "\n",
    "    def _sum_kan_regularization(self) -> torch.Tensor:\n",
    "        if hasattr(self.model, \"regularization_loss\"):\n",
    "            return self.model.regularization_loss(\n",
    "                regularize_activation=self.cfg[\"kan_reg_activation\"],\n",
    "                regularize_entropy=self.cfg[\"kan_reg_entropy\"]\n",
    "            )\n",
    "        device = self.device\n",
    "        reg = torch.tensor(0.0, device=device)\n",
    "        use_att = bool(getattr(self.model, \"use_attention\", True))\n",
    "        use_rand = bool(getattr(self.model, \"use_random\", True))\n",
    "        if hasattr(self.model, \"fix_branches\") and self.model.fix_branches is not None:\n",
    "            for fb in self.model.fix_branches:\n",
    "                if hasattr(fb, \"enc\"):  reg = reg + fb.enc.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "                if hasattr(fb, \"head\"): reg = reg + fb.head.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "        if use_att and hasattr(self.model, \"att_branches\") and self.model.att_branches is not None:\n",
    "            for ab in self.model.att_branches:\n",
    "                if hasattr(ab, \"summarize\"): reg = reg + ab.summarize.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "                if hasattr(ab, \"out_head\"):  reg = reg + ab.out_head.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "        if use_rand:\n",
    "            if hasattr(self.model, \"rand_encoders\") and self.model.rand_encoders is not None:\n",
    "                for re in self.model.rand_encoders:\n",
    "                    if hasattr(re, \"enc\"): reg = reg + re.enc.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "            if hasattr(self.model, \"tc_encoders\") and self.model.tc_encoders is not None:\n",
    "                for te in self.model.tc_encoders:\n",
    "                    if hasattr(te, \"enc\"): reg = reg + te.enc.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "            if hasattr(self.model, \"film_from_tc\") and self.model.film_from_tc is not None:\n",
    "                for film in self.model.film_from_tc:\n",
    "                    if hasattr(film, \"gamma\"): reg = reg + film.gamma.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "                    if hasattr(film, \"beta\"):  reg = reg + film.beta.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "            if hasattr(self.model, \"rand_heads\") and self.model.rand_heads is not None:\n",
    "                for rh in self.model.rand_heads:\n",
    "                    if hasattr(rh, \"head\"): reg = reg + rh.head.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "        return reg\n",
    "\n",
    "    def _compute_pos_weight_from_loader(self, train_loader) -> torch.Tensor:\n",
    "        device = self.device\n",
    "        pos_sum = None\n",
    "        total_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in train_loader:\n",
    "                y_b = batch[3].to(device)\n",
    "                if y_b.dim() == 1:\n",
    "                    y_b = y_b.unsqueeze(1)\n",
    "                bs, y_dim = y_b.shape\n",
    "                if pos_sum is None:\n",
    "                    pos_sum = torch.zeros(y_dim, device=device, dtype=torch.float32)\n",
    "                pos_sum += y_b.float().sum(dim=0)\n",
    "                total_sum += bs\n",
    "        if pos_sum is None:\n",
    "            return torch.ones(1, device=device, dtype=torch.float32)\n",
    "        P = pos_sum\n",
    "        T = torch.tensor(float(total_sum), device=device, dtype=torch.float32)\n",
    "        N = T - P\n",
    "        eps = float(self.cfg.get(\"pos_weight_eps\", 1e-6))\n",
    "        pos_weight = N / torch.clamp(P, min=eps)\n",
    "        pos_weight = torch.where(torch.isfinite(pos_weight), pos_weight, torch.ones_like(pos_weight))\n",
    "        pos_weight = torch.clamp(pos_weight, min=eps)\n",
    "        return pos_weight\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        y_true: torch.Tensor,\n",
    "        logits: torch.Tensor,\n",
    "        parts: Dict[str, Any],\n",
    "        *,\n",
    "        X_fix: torch.Tensor,\n",
    "        pid_idx: Optional[torch.Tensor] = None,\n",
    "        TC: Optional[torch.Tensor] = None,\n",
    "        pos_weight: Optional[torch.Tensor] = None,\n",
    "        sample_weight: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "        cfg = self.cfg\n",
    "        use_rand = bool(getattr(self.model, \"use_random\", True))\n",
    "\n",
    "        loss_pred = _bce_logits(logits, y_true, pos_weight=pos_weight, sample_weight=sample_weight)\n",
    "\n",
    "        if use_rand:\n",
    "            e_rand = parts.get(\"e_rand\", None)\n",
    "            if e_rand is None:\n",
    "                e_rand = torch.zeros_like(logits)\n",
    "            loss_mean0 = (e_rand.mean(dim=0) ** 2).sum()\n",
    "            loss_ridge = (e_rand ** 2).mean()\n",
    "            z_fix_list = parts.get(\"z_fix_list\", [])\n",
    "            z_rand_film_list = parts.get(\"z_rand_film_list\", [])\n",
    "            loss_orth = torch.tensor(0.0, device=logits.device)\n",
    "            if len(z_fix_list) and len(z_rand_film_list) and len(z_fix_list) == len(z_rand_film_list):\n",
    "                acc = 0.0\n",
    "                for zf, zr in zip(z_fix_list, z_rand_film_list):\n",
    "                    acc = acc + _orthogonality_penalty_latents(zf, zr)\n",
    "                loss_orth = acc / float(len(z_fix_list))\n",
    "            gamma_list = parts.get(\"film_gamma_list\", [])\n",
    "            beta_list  = parts.get(\"film_beta_list\", [])\n",
    "            loss_film = torch.tensor(0.0, device=logits.device)\n",
    "            if len(gamma_list) and len(beta_list):\n",
    "                acc = 0.0\n",
    "                one = torch.tensor(1.0, device=logits.device, dtype=logits.dtype)\n",
    "                for g, b in zip(gamma_list, beta_list):\n",
    "                    acc = acc + ((g - one) ** 2).mean() + (b ** 2).mean()\n",
    "                loss_film = acc / float(len(gamma_list))\n",
    "        else:\n",
    "            loss_mean0 = torch.tensor(0.0, device=logits.device)\n",
    "            loss_ridge = torch.tensor(0.0, device=logits.device)\n",
    "            loss_orth  = torch.tensor(0.0, device=logits.device)\n",
    "            loss_film  = torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "        kan_reg = torch.tensor(0.0, device=logits.device)\n",
    "        if cfg[\"lambda_kan\"] > 0:\n",
    "            kan_reg = self._sum_kan_regularization()\n",
    "\n",
    "        total_loss = (\n",
    "            loss_pred\n",
    "            + cfg[\"lambda_mean0\"] * loss_mean0\n",
    "            + cfg[\"lambda_ridge\"] * loss_ridge\n",
    "            + cfg[\"lambda_orth_latent\"] * loss_orth\n",
    "            + cfg[\"lambda_film_identity\"] * loss_film\n",
    "            + cfg[\"lambda_kan\"] * kan_reg\n",
    "        )\n",
    "\n",
    "        parts_out = {\n",
    "            \"loss_total\": float(total_loss.detach().cpu()),\n",
    "            \"loss_pred\":  float(loss_pred.detach().cpu()),\n",
    "            \"loss_mean0\": float(loss_mean0.detach().cpu()),\n",
    "            \"loss_ridge\": float(loss_ridge.detach().cpu()),\n",
    "            \"loss_orth\":  float(loss_orth.detach().cpu()),\n",
    "            \"loss_fi\":    float(loss_film.detach().cpu()),\n",
    "            \"loss_kan\":   float(kan_reg.detach().cpu()),\n",
    "        }\n",
    "        return total_loss, parts_out\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_loader,\n",
    "        val_loader=None,\n",
    "        *,\n",
    "        verbose: bool = True,\n",
    "        pos_weight: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        cfg = self.cfg\n",
    "        model = self.model\n",
    "        device = self.device\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "        use_amp = bool(cfg.get(\"amp\", True) and torch.cuda.is_available())\n",
    "        scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "        if pos_weight is None and cfg.get(\"auto_pos_weight\", True):\n",
    "            pos_weight = self._compute_pos_weight_from_loader(train_loader)\n",
    "        if pos_weight is not None:\n",
    "            pos_weight = pos_weight.to(device, dtype=next(model.parameters()).dtype)\n",
    "        best_val = float(\"inf\")\n",
    "        best_state = None\n",
    "        no_improve = 0\n",
    "        for epoch in range(1, cfg[\"max_epochs\"] + 1):\n",
    "            model.train()\n",
    "            total_tr, n_tr = 0.0, 0\n",
    "            for batch in train_loader:\n",
    "                tensors = [b.to(device) for b in batch]\n",
    "                Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b = tensors[:8]\n",
    "                sample_w_b = None\n",
    "                if cfg.get(\"sample_weight_index\") is not None:\n",
    "                    idx_w = int(cfg[\"sample_weight_index\"])\n",
    "                    if idx_w < len(tensors):\n",
    "                        sample_w_b = tensors[idx_w]\n",
    "                TC_in = TC_b if TC_b.size(1) > 0 else None\n",
    "                Zr_in = Zr_b if Zr_b.size(1) > 0 else None\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "                    logits, parts = self.forward(\n",
    "                        X_fix=Xf_b, TC=TC_in, Zrand=Zr_in,\n",
    "                        y_lags=yl_b, dt_lags=dt_b,\n",
    "                        pid_idx=pid_b, pid_seen_mask=seen_b\n",
    "                    )\n",
    "                    loss, _ = self.compute_loss(\n",
    "                        y_true=y_b, logits=logits, parts=parts,\n",
    "                        X_fix=Xf_b, pid_idx=pid_b, TC=TC_in,\n",
    "                        pos_weight=pos_weight,\n",
    "                        sample_weight=sample_w_b,\n",
    "                    )\n",
    "                scaler.scale(loss).backward()\n",
    "                if cfg.get(\"clip_grad\", None):\n",
    "                    scaler.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"clip_grad\"])\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                bs = Xf_b.size(0)\n",
    "                total_tr += float(loss.detach().cpu()) * bs\n",
    "                n_tr += bs\n",
    "            train_loss = total_tr / max(1, n_tr)\n",
    "            self.history_[\"train_loss\"].append(train_loss)\n",
    "            if val_loader is not None:\n",
    "                model.eval()\n",
    "                total_va, n_va = 0.0, 0\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_loader:\n",
    "                        tensors = [b.to(device) for b in batch]\n",
    "                        Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b = tensors[:8]\n",
    "                        sample_w_b = None\n",
    "                        TC_in = TC_b if TC_b.size(1) > 0 else None\n",
    "                        Zr_in = Zr_b if Zr_b.size(1) > 0 else None\n",
    "                        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "                            logits, parts = self.forward(\n",
    "                                X_fix=Xf_b, TC=TC_in, Zrand=Zr_in,\n",
    "                                y_lags=yl_b, dt_lags=dt_b,\n",
    "                                pid_idx=pid_b, pid_seen_mask=seen_b\n",
    "                            )\n",
    "                            l, _ = self.compute_loss(\n",
    "                                y_true=y_b, logits=logits, parts=parts,\n",
    "                                X_fix=Xf_b, pid_idx=pid_b, TC=TC_in,\n",
    "                                pos_weight=pos_weight,\n",
    "                                sample_weight=sample_w_b,\n",
    "                            )\n",
    "                        bs = Xf_b.size(0)\n",
    "                        total_va += float(l.detach().cpu()) * bs\n",
    "                        n_va += bs\n",
    "                val_loss = total_va / max(1, n_va)\n",
    "                self.history_[\"val_loss\"].append(val_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:03d} | train {train_loss:.6f} | val {val_loss:.6f}\")\n",
    "                if val_loss < best_val - 1e-6:\n",
    "                    best_val = val_loss\n",
    "                    best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= cfg[\"patience\"]:\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping at epoch {epoch:03d} (best val {best_val:.6f})\")\n",
    "                        break\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:03d} | train {train_loss:.6f}\")\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        self.thresholds_ = None\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            all_prob, all_true = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    tensors = [b.to(device) for b in batch]\n",
    "                    Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b = tensors[:8]\n",
    "                    TC_in = TC_b if TC_b.size(1) > 0 else None\n",
    "                    Zr_in = Zr_b if Zr_b.size(1) > 0 else None\n",
    "                    logits, _ = self.forward(\n",
    "                        X_fix=Xf_b, TC=TC_in, Zrand=Zr_in,\n",
    "                        y_lags=yl_b, dt_lags=dt_b,\n",
    "                        pid_idx=pid_b, pid_seen_mask=seen_b\n",
    "                    )\n",
    "                    all_prob.append(torch.sigmoid(logits).cpu().numpy())\n",
    "                    all_true.append(y_b.cpu().numpy())\n",
    "            y_prob = np.vstack(all_prob)\n",
    "            y_true = np.vstack(all_true)\n",
    "            y_dim = y_prob.shape[1] if y_prob.ndim == 2 else 1\n",
    "            thr_vec = np.zeros((y_dim,), dtype=float)\n",
    "            for j in range(y_dim):\n",
    "                thr_vec[j] = _optimal_threshold_exact_np(y_true[:, j], y_prob[:, j], beta=1.0)\n",
    "            self.thresholds_ = thr_vec\n",
    "        return {\"best_val_loss\": (best_val if val_loader is not None else self.history_[\"train_loss\"][-1])}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_logits(\n",
    "        self,\n",
    "        X_fix: torch.Tensor,\n",
    "        TC: Optional[torch.Tensor],\n",
    "        Zrand: Optional[torch.Tensor],\n",
    "        y_lags: torch.Tensor,\n",
    "        dt_lags: torch.Tensor,\n",
    "        pid_idx: Optional[torch.Tensor] = None,\n",
    "        pid_seen_mask: Optional[torch.Tensor] = None,\n",
    "        *,\n",
    "        combine: str = \"all\",\n",
    "    ) -> torch.Tensor:\n",
    "        self.model.eval()\n",
    "        logits_full, parts = self.forward(X_fix, TC, Zrand, y_lags, dt_lags, pid_idx, pid_seen_mask)\n",
    "        return _combine_logits_from_parts(parts, combine=combine)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self, *args, combine: str = \"all\", **kwargs) -> torch.Tensor:\n",
    "        logits = self.predict_logits(*args, combine=combine, **kwargs)\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(\n",
    "        self,\n",
    "        *args,\n",
    "        threshold: Optional[float] = None,\n",
    "        thresholds: Optional[np.ndarray] = None,\n",
    "        use_fitted_thresholds: bool = False,\n",
    "        combine: str = \"all\",\n",
    "        **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        probs = self.predict_proba(*args, combine=combine, **kwargs)\n",
    "        if probs.ndim == 2:\n",
    "            B, y_dim = probs.shape\n",
    "        else:\n",
    "            B, y_dim = probs.numel(), 1\n",
    "        if thresholds is not None:\n",
    "            thr = torch.as_tensor(thresholds, device=probs.device, dtype=probs.dtype).view(1, -1)\n",
    "        elif use_fitted_thresholds and (self.thresholds_ is not None):\n",
    "            thr = torch.as_tensor(self.thresholds_, device=probs.device, dtype=probs.dtype).view(1, -1)\n",
    "        else:\n",
    "            thr_scalar = self.cfg.get(\"threshold\", 0.5) if threshold is None else threshold\n",
    "            thr = torch.tensor([thr_scalar], device=probs.device, dtype=probs.dtype).view(1, 1)\n",
    "            if probs.ndim == 2 and probs.size(1) > 1:\n",
    "                thr = thr.expand(1, probs.size(1))\n",
    "        return (probs >= thr).to(torch.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf6f7b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29fa7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import t as student_t\n",
    "from dataclasses import dataclass\n",
    "\n",
    "_VAR_EPS = 1e-8\n",
    "_STD_EPS = 1e-6\n",
    "_CLIP_Z  = 8.0\n",
    "\n",
    "@dataclass\n",
    "class PCAPipeline:\n",
    "    keep_mask: np.ndarray\n",
    "    mean_: np.ndarray\n",
    "    scale_: np.ndarray\n",
    "    pca: PCA\n",
    "\n",
    "def _fit_pca_pipeline(X_train: np.ndarray, var_ratio: float = 0.95, random_state: int | None = None) -> PCAPipeline:\n",
    "    X = np.asarray(X_train, dtype=np.float64)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    var = X.var(axis=0)\n",
    "    keep = var > _VAR_EPS\n",
    "    if not np.any(keep):\n",
    "        pca = PCA(n_components=0, svd_solver='full', random_state=random_state)\n",
    "        return PCAPipeline(keep_mask=keep, mean_=np.array([], dtype=np.float64),\n",
    "                           scale_=np.array([], dtype=np.float64), pca=pca)\n",
    "    Xk = X[:, keep]\n",
    "    mean = Xk.mean(axis=0)\n",
    "    std  = Xk.std(axis=0)\n",
    "    std  = np.maximum(std, _STD_EPS)\n",
    "    Z = (Xk - mean) / std\n",
    "    Z = np.nan_to_num(Z, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    np.clip(Z, -_CLIP_Z, _CLIP_Z, out=Z)\n",
    "    pca = PCA(n_components=var_ratio, svd_solver='full', random_state=random_state)\n",
    "    pca.fit(Z)\n",
    "    if not np.isfinite(pca.components_).all():\n",
    "        raise RuntimeError(\"PCA components contain non-finite values after fit.\")\n",
    "    return PCAPipeline(keep_mask=keep, mean_=mean, scale_=std, pca=pca)\n",
    "\n",
    "def _transform_pca_pipeline(pipe: PCAPipeline | None, X: np.ndarray | None) -> np.ndarray | None:\n",
    "    if pipe is None or X is None:\n",
    "        return None\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if pipe.keep_mask.size == 0 or not np.any(pipe.keep_mask):\n",
    "        return np.zeros((X.shape[0], 0), dtype=np.float32)\n",
    "    Xk = X[:, pipe.keep_mask]\n",
    "    Z = (Xk - pipe.mean_) / pipe.scale_\n",
    "    Z = np.nan_to_num(Z, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    np.clip(Z, -_CLIP_Z, _CLIP_Z, out=Z)\n",
    "    if not np.isfinite(Z).all():\n",
    "        bad = np.argwhere(~np.isfinite(Z))[0]\n",
    "        raise RuntimeError(f\"[our PCA] Z non-finite at {tuple(bad)}: {Z[tuple(bad)]}\")\n",
    "    if np.abs(Z).max() > 1e6:\n",
    "        raise RuntimeError(f\"[our PCA] Z max |z| too large: {np.abs(Z).max()}\")\n",
    "    if not np.isfinite(pipe.pca.components_).all():\n",
    "        raise RuntimeError(\"[our PCA] components_ non-finite\")\n",
    "    if hasattr(pipe.pca, \"mean_\") and not np.isfinite(pipe.pca.mean_).all():\n",
    "        raise RuntimeError(\"[our PCA] mean_ non-finite\")\n",
    "    Z64 = np.ascontiguousarray(Z, dtype=np.float64)\n",
    "    CT  = np.ascontiguousarray(pipe.pca.components_.T, dtype=np.float64)\n",
    "    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n",
    "        Xt = Z64 @ CT\n",
    "    Xt = np.nan_to_num(Xt, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return Xt\n",
    "\n",
    "def _metrics_binary_full(y_true, y_prob, thr=0.5) -> dict:\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob).astype(float)\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    try: auc = float(roc_auc_score(y_true, y_prob))\n",
    "    except: auc = float(\"nan\")\n",
    "    try: auprc = float(average_precision_score(y_true, y_prob))\n",
    "    except: auprc = float(\"nan\")\n",
    "    brier = float(np.mean((y_prob - y_true) ** 2))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    acc = float((y_pred == y_true).mean())\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "        sens = float(tp / (tp + fn)) if (tp + fn) > 0 else float(\"nan\")\n",
    "        spec = float(tn / (tn + fp)) if (tn + fp) > 0 else float(\"nan\")\n",
    "    except:\n",
    "        sens, spec = float(\"nan\"), float(\"nan\")\n",
    "    return {\"AUC\": auc, \"AUPRC\": auprc, \"Brier\": brier, \"ACC\": acc, \"F1\": float(f1),\n",
    "            \"Precision\": float(prec), \"Recall\": float(rec),\n",
    "            \"Sensitivity\": sens, \"Specificity\": spec}\n",
    "\n",
    "def evaluate_multitask(y_true: np.ndarray, y_prob: np.ndarray, thr=0.5) -> dict:\n",
    "    y_true = np.asarray(y_true); y_prob = np.asarray(y_prob)\n",
    "    if y_true.ndim == 1: y_true = y_true[:, None]\n",
    "    if y_prob.ndim == 1: y_prob = y_prob[:, None]\n",
    "    assert y_true.shape == y_prob.shape\n",
    "    y_dim = y_true.shape[1]\n",
    "    out, macro = {}, {}\n",
    "    for j in range(y_dim):\n",
    "        m = _metrics_binary_full(y_true[:, j], y_prob[:, j], thr)\n",
    "        for k, v in m.items():\n",
    "            out[f\"task_{j+1}_{k}\"] = float(v)\n",
    "            macro.setdefault(k, []).append(float(v))\n",
    "    for k, vals in macro.items():\n",
    "        out[f\"macro_{k}\"] = float(np.nanmean(np.array(vals, dtype=float)))\n",
    "    return out\n",
    "\n",
    "def _evaluate_with_thresholds(y_true: np.ndarray, y_prob: np.ndarray, thresholds: np.ndarray) -> dict:\n",
    "    if y_true.ndim == 1: y_true = y_true[:, None]\n",
    "    if y_prob.ndim == 1: y_prob = y_prob[:, None]\n",
    "    y_dim = y_true.shape[1]\n",
    "    out, macro = {}, {}\n",
    "    for j in range(y_dim):\n",
    "        m = _metrics_binary_full(y_true[:, j], y_prob[:, j], thr=float(thresholds[j]))\n",
    "        for k, v in m.items():\n",
    "            out[f\"task_{j+1}_{k}_optthr\"] = float(v)\n",
    "            macro.setdefault(k, []).append(float(v))\n",
    "    for k, vals in macro.items():\n",
    "        out[f\"macro_{k}_optthr\"] = float(np.nanmean(np.array(vals, dtype=float)))\n",
    "    return out\n",
    "\n",
    "def compute_calibration_curves(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10, strategy: str = \"quantile\") -> dict:\n",
    "    y_true = np.asarray(y_true); y_prob = np.asarray(y_prob)\n",
    "    if y_true.ndim == 1: y_true = y_true[:, None]\n",
    "    if y_prob.ndim == 1: y_prob = y_prob[:, None]\n",
    "    y_dim = y_true.shape[1]\n",
    "    per_task, eces, mces = {}, [], []\n",
    "    for j in range(y_dim):\n",
    "        t = y_true[:, j].astype(int)\n",
    "        p = np.clip(y_prob[:, j].astype(float), 1e-6, 1-1e-6)\n",
    "        try:\n",
    "            frac_pos, mean_pred = calibration_curve(t, p, n_bins=n_bins, strategy=strategy)\n",
    "        except Exception:\n",
    "            frac_pos, mean_pred = np.array([]), np.array([])\n",
    "        if strategy == \"uniform\":\n",
    "            edges = np.linspace(0, 1, n_bins+1)\n",
    "        else:\n",
    "            qs = np.linspace(0, 1, n_bins+1)\n",
    "            edges = np.quantile(p, qs); edges[0], edges[-1] = 0.0, 1.0\n",
    "        bin_ids = np.digitize(p, edges[1:-1], right=True)\n",
    "        counts = np.bincount(bin_ids, minlength=n_bins).astype(float)\n",
    "        N = max(1, len(p))\n",
    "        if len(mean_pred) == n_bins:\n",
    "            weights = counts / N\n",
    "        else:\n",
    "            if len(mean_pred) > 0:\n",
    "                idxs = np.digitize(mean_pred, edges[1:-1], right=True)\n",
    "                weights = counts[idxs] / N\n",
    "            else:\n",
    "                weights = np.array([])\n",
    "        gaps = np.abs(frac_pos - mean_pred) if len(mean_pred) else np.array([np.nan])\n",
    "        ece = float(np.nansum(weights * gaps)) if len(mean_pred) else float(\"nan\")\n",
    "        mce = float(np.nanmax(gaps)) if len(mean_pred) else float(\"nan\")\n",
    "        brier = float(np.mean((p - t) ** 2)) if N > 0 else float(\"nan\")\n",
    "        per_task[j] = {\"mean_pred\": mean_pred.tolist(), \"frac_pos\": frac_pos.tolist(),\n",
    "                       \"counts\": counts.tolist(), \"ece\": ece, \"mce\": mce, \"brier\": brier}\n",
    "        eces.append(ece); mces.append(mce)\n",
    "    return {\"per_task\": per_task, \"macro_ECE\": float(np.nanmean(eces)), \"macro_MCE\": float(np.nanmean(mces))}\n",
    "\n",
    "def _split_cases(pid_array, test_fraction=0.2, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    u = np.unique(pid_array)\n",
    "    te_ids = rng.choice(u, size=max(1, int(len(u)*test_fraction)), replace=False)\n",
    "    te_mask = np.isin(pid_array, te_ids)\n",
    "    return np.where(~te_mask)[0], np.where(te_mask)[0]\n",
    "\n",
    "def _split_time_basic(time_index, test_fraction=0.2):\n",
    "    order = np.argsort(time_index)\n",
    "    n = len(order)\n",
    "    split = int(np.floor(n*(1.0 - test_fraction)))\n",
    "    return order[:split], order[split:]\n",
    "\n",
    "def _filter_time_test_min_measurements(pid_idx: np.ndarray, test_idx: np.ndarray, min_meas: int = 2):\n",
    "    pid = np.asarray(pid_idx)\n",
    "    counts = {pid_val: np.sum(pid == pid_val) for pid_val in np.unique(pid)}\n",
    "    keep = [i for i in test_idx if counts.get(pid[i], 0) >= min_meas]\n",
    "    return np.array(keep, dtype=int)\n",
    "\n",
    "def _concat_safe(*arrays: Optional[np.ndarray]) -> np.ndarray:\n",
    "    parts = [a for a in arrays if a is not None and a.size > 0]\n",
    "    if not parts:\n",
    "        return np.zeros((0, 0), dtype=np.float32)\n",
    "    return np.concatenate(parts, axis=1).astype(np.float32)\n",
    "\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import GroupKFold, TimeSeriesSplit, ParameterGrid\n",
    "from scipy.stats import t as student_t\n",
    "\n",
    "class _KANDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_fix: np.ndarray,\n",
    "        TC: Optional[np.ndarray],\n",
    "        Zrand: Optional[np.ndarray],\n",
    "        y: np.ndarray,\n",
    "        y_lags: np.ndarray,\n",
    "        dt_lags: np.ndarray,\n",
    "        pid_idx: np.ndarray,\n",
    "        pid_seen_mask: np.ndarray,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        N = X_fix.shape[0]\n",
    "        assert y.shape[0] == N and y_lags.shape[0] == N and dt_lags.shape[0] == N and pid_idx.shape[0] == N\n",
    "        def _to_tensor(a, dtype=torch.float32):\n",
    "            if a is None:\n",
    "                return torch.zeros((N, 0), dtype=dtype, device=device)\n",
    "            return torch.as_tensor(a, dtype=dtype, device=device)\n",
    "        self.X_fix  = _to_tensor(X_fix, torch.float32)\n",
    "        self.TC     = _to_tensor(TC,    torch.float32)\n",
    "        self.Zrand  = _to_tensor(Zrand, torch.float32)\n",
    "        self.y      = _to_tensor(y,     torch.float32)\n",
    "        self.y_lags = _to_tensor(y_lags, torch.float32)\n",
    "        self.dt     = _to_tensor(dt_lags, torch.float32)\n",
    "        self.pid    = torch.as_tensor(pid_idx, dtype=torch.long, device=device)\n",
    "        self.seen   = torch.as_tensor(pid_seen_mask.astype(bool), dtype=torch.bool, device=device)\n",
    "    def __len__(self):\n",
    "        return self.X_fix.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            self.X_fix[i], self.TC[i], self.Zrand[i],\n",
    "            self.y[i], self.y_lags[i], self.dt[i],\n",
    "            self.pid[i], self.seen[i]\n",
    "        )\n",
    "\n",
    "def _make_loader_kan(\n",
    "    X_fix, TC, Zrand, y, y_lags, dt_lags,\n",
    "    pid_idx, pid_seen_mask,\n",
    "    batch_size: int,\n",
    "    shuffle: bool,\n",
    "    device: torch.device\n",
    ") -> DataLoader:\n",
    "    ds = _KANDataset(\n",
    "        X_fix, TC, Zrand, y, y_lags, dt_lags, pid_idx, pid_seen_mask, device\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, drop_last=False, num_workers=0)\n",
    "\n",
    "def _optimal_threshold_exact_np(y_true: np.ndarray, y_prob: np.ndarray, beta: float = 1.0) -> float:\n",
    "    y = y_true.astype(int).ravel()\n",
    "    p = y_prob.astype(float).ravel()\n",
    "    if y.size == 0:\n",
    "        return 0.5\n",
    "    P = int(y.sum()); N = y.size - P\n",
    "    if P == 0: return 1.0\n",
    "    if N == 0: return 0.0\n",
    "    o = np.argsort(-p); p = p[o]; y = y[o]\n",
    "    tp = np.cumsum(y); fp = np.cumsum(1 - y)\n",
    "    prec = tp / np.maximum(1, tp + fp)\n",
    "    rec  = tp / max(1, P)\n",
    "    f = (1 + beta**2) * prec * rec / np.maximum(1e-12, beta**2 * prec + rec)\n",
    "    idx = int(np.nanargmax(f))\n",
    "    next_p = p[idx+1] if idx+1 < len(p) else -np.inf\n",
    "    thr = (p[idx] + next_p)/2.0 if np.isfinite(next_p) and next_p < p[idx] else max(0.0, p[idx] - np.finfo(p.dtype).eps)\n",
    "    return float(thr)\n",
    "\n",
    "def _find_best_thresholds_from_loader(wrapper, loader: DataLoader) -> Tuple[np.ndarray, Dict[str, float]]:\n",
    "    wrapper.model.eval()\n",
    "    probs_all, y_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b in loader:\n",
    "            logits = wrapper.predict_logits(\n",
    "                X_fix=Xf_b,\n",
    "                TC=TC_b if TC_b.size(1) > 0 else None,\n",
    "                Zrand=Zr_b if Zr_b.size(1) > 0 else None,\n",
    "                y_lags=yl_b,\n",
    "                dt_lags=dt_b,\n",
    "                pid_idx=pid_b,\n",
    "                pid_seen_mask=seen_b,\n",
    "            )\n",
    "            probs_all.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            y_all.append(y_b.cpu().numpy())\n",
    "    y_prob = np.vstack(probs_all); y_true = np.vstack(y_all)\n",
    "    y_dim = y_prob.shape[1]\n",
    "    thr = np.zeros((y_dim,), dtype=float)\n",
    "    for j in range(y_dim):\n",
    "        thr[j] = _optimal_threshold_exact_np(y_true[:, j], y_prob[:, j], beta=1.0)\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    preds = (y_prob >= thr[None, :]).astype(int)\n",
    "    f1s = []\n",
    "    for j in range(y_dim):\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_true[:, j], preds[:, j], average='binary', zero_division=0)\n",
    "        f1s.append(float(f1))\n",
    "    return thr, {\"macro_F1_trainthr\": float(np.nanmean(f1s))}\n",
    "\n",
    "def _print_split_info(name, idxs, pid_idx_full):\n",
    "    n = int(len(idxs))\n",
    "    u = int(len(np.unique(pid_idx_full[idxs]))) if n > 0 else 0\n",
    "    print(f\"[split] {name:<5} | rows={n:5d} | unique_ids={u:5d}\")\n",
    "\n",
    "def _prepare_split_and_loaders_kan(\n",
    "    X_only_fixed: np.ndarray,\n",
    "    X_fixed_and_random: Optional[np.ndarray],\n",
    "    y: np.ndarray,\n",
    "    y_lags: np.ndarray,\n",
    "    dt_lags: np.ndarray,\n",
    "    pid_idx_full: np.ndarray,\n",
    "    indices_train: np.ndarray,\n",
    "    indices_val: Optional[np.ndarray],\n",
    "    indices_test: np.ndarray,\n",
    "    scenario: str,\n",
    "    batch_size: int,\n",
    "    device: torch.device,\n",
    "    random_state: int = 42,\n",
    "    pca_var_ratio: float = 0.95,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    of_pipe = _fit_pca_pipeline(X_only_fixed[indices_train], var_ratio=pca_var_ratio, random_state=random_state)\n",
    "    fr_pipe = None\n",
    "    if X_fixed_and_random is not None and X_fixed_and_random.shape[1] > 0:\n",
    "        fr_pipe = _fit_pca_pipeline(X_fixed_and_random[indices_train], var_ratio=pca_var_ratio, random_state=random_state)\n",
    "    def transform_block(idxs):\n",
    "        of = _transform_pca_pipeline(of_pipe, X_only_fixed[idxs])\n",
    "        fr = _transform_pca_pipeline(fr_pipe, None if X_fixed_and_random is None else X_fixed_and_random[idxs])\n",
    "        X_fix = _concat_safe(of, fr)\n",
    "        TC    = of\n",
    "        Zr    = fr\n",
    "        return X_fix, TC, Zr\n",
    "    if verbose:\n",
    "        _print_split_info(\"train\", indices_train, pid_idx_full)\n",
    "        if indices_val is not None:\n",
    "            _print_split_info(\"val\", indices_val, pid_idx_full)\n",
    "        _print_split_info(\"test\", indices_test, pid_idx_full)\n",
    "    Xf_tr, TC_tr, Zr_tr = transform_block(indices_train)\n",
    "    Xf_te, TC_te, Zr_te = transform_block(indices_test)\n",
    "    if indices_val is not None:\n",
    "        Xf_va, TC_va, Zr_va = transform_block(indices_val)\n",
    "    else:\n",
    "        Xf_va = TC_va = Zr_va = None\n",
    "    if scenario == \"cases\":\n",
    "        seen_pids = set(pid_idx_full[indices_train].tolist())\n",
    "        seen_tr = np.ones(indices_train.shape[0], dtype=bool)\n",
    "        seen_va = np.ones(indices_val.shape[0], dtype=bool) if indices_val is not None else None\n",
    "        seen_te = np.array([p in seen_pids for p in pid_idx_full[indices_test]], dtype=bool)\n",
    "    else:\n",
    "        seen_tr = np.ones(indices_train.shape[0], dtype=bool)\n",
    "        seen_va = np.ones(indices_val.shape[0], dtype=bool) if indices_val is not None else None\n",
    "        seen_te = np.ones(indices_test.shape[0], dtype=bool)\n",
    "    tr_loader = _make_loader_kan(\n",
    "        Xf_tr, TC_tr, Zr_tr, y[indices_train], y_lags[indices_train], dt_lags[indices_train],\n",
    "        pid_idx_full[indices_train], seen_tr,\n",
    "        batch_size=batch_size, shuffle=True, device=device\n",
    "    )\n",
    "    va_loader = None\n",
    "    if indices_val is not None:\n",
    "        va_loader = _make_loader_kan(\n",
    "            Xf_va, TC_va, Zr_va, y[indices_val], y_lags[indices_val], dt_lags[indices_val],\n",
    "            pid_idx_full[indices_val], seen_va,\n",
    "            batch_size=batch_size, shuffle=False, device=device\n",
    "        )\n",
    "    te_loader = _make_loader_kan(\n",
    "        Xf_te, TC_te, Zr_te, y[indices_test], y_lags[indices_test], dt_lags[indices_test],\n",
    "        pid_idx_full[indices_test], seen_te,\n",
    "        batch_size=batch_size, shuffle=False, device=device\n",
    "    )\n",
    "    preprocessors = {\n",
    "        \"of_pipe\": of_pipe,\n",
    "        \"fr_pipe\": fr_pipe,\n",
    "        \"d_fix\":   Xf_tr.shape[1],\n",
    "        \"d_tc\":    (Xf_tr.shape[1] - (0 if Zr_tr is None else Zr_tr.shape[1])),\n",
    "        \"d_zrand\": 0 if Zr_tr is None else Zr_tr.shape[1],\n",
    "        \"n_ids\":   int(len(np.unique(pid_idx_full))),\n",
    "    }\n",
    "    loaders = {\"train\": tr_loader, \"val\": va_loader, \"test\": te_loader}\n",
    "    return preprocessors, loaders\n",
    "\n",
    "def _model_param_count(model: torch.nn.Module) -> int:\n",
    "    return int(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "def _fit_eval_once_kan(\n",
    "    build_model_fn, wrapper_cls,\n",
    "    arch_params: Dict[str, Any],\n",
    "    train_params: Dict[str, Any],\n",
    "    X_only_fixed: np.ndarray,\n",
    "    X_fixed_and_random: Optional[np.ndarray],\n",
    "    y: np.ndarray,\n",
    "    y_lags: np.ndarray,\n",
    "    dt_lags: np.ndarray,\n",
    "    pid_idx_full: np.ndarray,\n",
    "    tr_idx: np.ndarray,\n",
    "    va_idx: Optional[np.ndarray],\n",
    "    te_idx: np.ndarray,\n",
    "    device: torch.device,\n",
    "    scenario: str,\n",
    "    threshold_selection_source: str = \"train\",\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    preprocessors, loaders = _prepare_split_and_loaders_kan(\n",
    "        X_only_fixed, X_fixed_and_random, y, y_lags, dt_lags, pid_idx_full,\n",
    "        tr_idx, va_idx, te_idx, scenario,\n",
    "        batch_size=train_params.get(\"batch_size\", 256),\n",
    "        device=device,\n",
    "        random_state=train_params.get(\"random_state\", 42),\n",
    "        pca_var_ratio=train_params.get(\"pca_var_ratio\", 0.95),\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    y_dim = y.shape[1] if y.ndim == 2 else 1\n",
    "    model = build_model_fn(\n",
    "        y_dim=y_dim,\n",
    "        d_fix=preprocessors[\"d_fix\"],\n",
    "        d_tc=preprocessors[\"d_tc\"],\n",
    "        d_zrand=preprocessors[\"d_zrand\"],\n",
    "        n_ids=preprocessors[\"n_ids\"],\n",
    "        **arch_params\n",
    "    ).to(device)\n",
    "    wrapper = wrapper_cls(model, cfg=train_params, device=device)\n",
    "    wrapper.fit(\n",
    "        loaders[\"train\"], loaders[\"val\"],\n",
    "        verbose=verbose,\n",
    "        pos_weight=train_params.get(\"pos_weight\", None)\n",
    "    )\n",
    "    wrapper.model.eval()\n",
    "    probs_all, y_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b in loaders[\"test\"]:\n",
    "            logits = wrapper.predict_logits(\n",
    "                X_fix=Xf_b,\n",
    "                TC=TC_b if TC_b.size(1) > 0 else None,\n",
    "                Zrand=Zr_b if Zr_b.size(1) > 0 else None,\n",
    "                y_lags=yl_b,\n",
    "                dt_lags=dt_b,\n",
    "                pid_idx=pid_b,\n",
    "                pid_seen_mask=seen_b,\n",
    "            )\n",
    "            probs_all.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            y_all.append(y_b.cpu().numpy())\n",
    "    y_prob_te = np.vstack(probs_all); y_true_te = np.vstack(y_all)\n",
    "    metrics_050 = evaluate_multitask(y_true_te, y_prob_te, thr=train_params.get(\"threshold\", 0.5))\n",
    "    if threshold_selection_source == \"train\":\n",
    "        thr_vec, thr_summary = _find_best_thresholds_from_loader(wrapper, loaders[\"train\"])\n",
    "    elif threshold_selection_source == \"val\" and loaders[\"val\"] is not None:\n",
    "        thr_vec, thr_summary = _find_best_thresholds_from_loader(wrapper, loaders[\"val\"])\n",
    "    else:\n",
    "        thr_vec, thr_summary = _find_best_thresholds_from_loader(wrapper, loaders[\"test\"])\n",
    "    metrics_opt = _evaluate_with_thresholds(y_true_te, y_prob_te, thr_vec)\n",
    "    calib = compute_calibration_curves(\n",
    "        y_true_te, y_prob_te,\n",
    "        n_bins=train_params.get(\"calibration_bins\", 10),\n",
    "        strategy=train_params.get(\"calibration_strategy\", \"quantile\"),\n",
    "    )\n",
    "    y_pred_050 = (y_prob_te >= float(train_params.get(\"threshold\", 0.5))).astype(int)\n",
    "    y_pred_opt = (y_prob_te >= thr_vec[None, :]).astype(int)\n",
    "    return {\n",
    "        \"metrics@0.5\": metrics_050,\n",
    "        \"metrics@optthr\": metrics_opt,\n",
    "        \"opt_thresholds\": thr_vec,\n",
    "        \"opt_thresholds_summary\": thr_summary,\n",
    "        \"preprocessors\": preprocessors,\n",
    "        \"wrapper\": wrapper,\n",
    "        \"model\": wrapper.model,\n",
    "        \"model_class\": wrapper.model.__class__.__name__,\n",
    "        \"model_param_count\": _model_param_count(wrapper.model),\n",
    "        \"arch_params_final\": dict(arch_params),\n",
    "        \"train_params_final\": dict(train_params),\n",
    "        \"macro_ECE\": calib[\"macro_ECE\"],\n",
    "        \"macro_MCE\": calib[\"macro_MCE\"],\n",
    "        \"y_true_test\": y_true_te,\n",
    "        \"y_prob_test\": y_prob_te,\n",
    "        \"y_pred_test@0.5\": y_pred_050,\n",
    "        \"y_pred_test@optthr\": y_pred_opt,\n",
    "    }\n",
    "\n",
    "def run_training_and_eval_kan(\n",
    "    X_only_fixed: np.ndarray,\n",
    "    X_fixed_and_random: Optional[np.ndarray],\n",
    "    y: np.ndarray,\n",
    "    y_lags: np.ndarray,\n",
    "    dt_lags: np.ndarray,\n",
    "    pid_idx: np.ndarray,\n",
    "    time_index: np.ndarray,\n",
    "    build_model_fn,\n",
    "    wrapper_cls,\n",
    "    *,\n",
    "    mode: str = \"single\",\n",
    "    scenario: str = \"cases\",\n",
    "    outer_folds: int = 5,\n",
    "    inner_folds: int = 3,\n",
    "    param_grid: Optional[Dict[str, List]] = None,\n",
    "    arch_defaults: Optional[Dict[str, Any]] = None,\n",
    "    train_defaults: Optional[Dict[str, Any]] = None,\n",
    "    device: Optional[torch.device] = None,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    device = (\n",
    "        device\n",
    "        or (torch.device(\"mps\") if torch.backends.mps.is_available() else None)\n",
    "        or (torch.device(\"cuda\") if torch.cuda.is_available() else None)\n",
    "        or torch.device(\"cpu\")\n",
    "    )\n",
    "    if scenario == \"both\":\n",
    "        out_cases = run_training_and_eval_kan(\n",
    "            X_only_fixed, X_fixed_and_random, y, y_lags, dt_lags, pid_idx, time_index,\n",
    "            build_model_fn, wrapper_cls,\n",
    "            mode=mode, scenario=\"cases\", outer_folds=outer_folds, inner_folds=inner_folds,\n",
    "            param_grid=param_grid, arch_defaults=arch_defaults, train_defaults=train_defaults,\n",
    "            device=device, verbose=verbose\n",
    "        )\n",
    "        out_time = run_training_and_eval_kan(\n",
    "            X_only_fixed, X_fixed_and_random, y, y_lags, dt_lags, pid_idx, time_index,\n",
    "            build_model_fn, wrapper_cls,\n",
    "            mode=mode, scenario=\"time\", outer_folds=outer_folds, inner_folds=inner_folds,\n",
    "            param_grid=param_grid, arch_defaults=arch_defaults, train_defaults=train_defaults,\n",
    "            device=device, verbose=verbose\n",
    "        )\n",
    "        return {\"scenario_cases\": out_cases, \"scenario_time\": out_time}\n",
    "    X_of = np.asarray(X_only_fixed, dtype=np.float32)\n",
    "    X_fr = None if X_fixed_and_random is None else np.asarray(X_fixed_and_random, dtype=np.float32)\n",
    "    y    = np.asarray(y, dtype=np.float32)\n",
    "    if y.ndim == 1: y = y[:, None]\n",
    "    y_lags = np.asarray(y_lags, dtype=np.float32)\n",
    "    dt_lags = np.asarray(dt_lags, dtype=np.float32)\n",
    "    pid_idx = np.asarray(pid_idx, dtype=np.int64)\n",
    "    time_ix = np.asarray(time_index)\n",
    "    arch_defaults = arch_defaults or {}\n",
    "    train_defaults = train_defaults or {}\n",
    "    rnd = int(train_defaults.get(\"random_state\", 42))\n",
    "    val_frac = float(train_defaults.get(\"val_fraction\", 0.10))\n",
    "    thr_source = train_defaults.get(\"threshold_selection_source\", \"train\")\n",
    "    def _make_train_val_split(idx_array: np.ndarray, seed: int) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        idx_array = np.asarray(idx_array)\n",
    "        if len(idx_array) <= 10 or val_frac <= 0.0:\n",
    "            return idx_array, None\n",
    "        rng = np.random.default_rng(seed)\n",
    "        perm = rng.permutation(len(idx_array))\n",
    "        cut = max(1, int(val_frac * len(idx_array)))\n",
    "        va_sel, tr_sel = perm[:cut], perm[cut:]\n",
    "        return idx_array[tr_sel], idx_array[va_sel]\n",
    "    if mode == \"single\":\n",
    "        if scenario == \"cases\":\n",
    "            tr_idx_all, te_idx = _split_cases(pid_idx, test_fraction=0.2, seed=rnd)\n",
    "        elif scenario == \"time\":\n",
    "            tr_idx_all, te_idx_raw = _split_time_basic(time_ix, test_fraction=0.2)\n",
    "            te_idx = _filter_time_test_min_measurements(\n",
    "                pid_idx, te_idx_raw, min_meas=train_defaults.get(\"min_meas_test\", 3)\n",
    "            )\n",
    "            if len(te_idx) == 0:\n",
    "                raise RuntimeError(\"Time split produced empty test after â‰¥3-measurements filter.\")\n",
    "        else:\n",
    "            raise ValueError(\"scenario must be 'cases' or 'time'\")\n",
    "        tr_idx, va_idx = _make_train_val_split(tr_idx_all, seed=rnd)\n",
    "        res = _fit_eval_once_kan(\n",
    "            build_model_fn, wrapper_cls,\n",
    "            arch_defaults, train_defaults,\n",
    "            X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "            tr_idx, va_idx, te_idx,\n",
    "            device=device, scenario=scenario,\n",
    "            threshold_selection_source=thr_source,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"\\nSingle-fit test metrics @0.5:\")\n",
    "            for k, v in res[\"metrics@0.5\"].items():\n",
    "                print(f\"{k:>18}: {v:.4f}\")\n",
    "            print(\"\\nSingle-fit test metrics @F1-opt per task:\")\n",
    "            for k, v in res[\"metrics@optthr\"].items():\n",
    "                print(f\"{k:>18}: {v:.4f}\")\n",
    "            print(f\"\\nModel: {res['model_class']} | params={res['model_param_count']}\")\n",
    "            print(\"Final arch params:\", res[\"arch_params_final\"])\n",
    "            print(\"Final train params:\", res[\"train_params_final\"])\n",
    "        return res\n",
    "    if mode == \"cv_only\":\n",
    "        fold_metrics_050, fold_metrics_opt = [], []\n",
    "        if scenario == \"cases\":\n",
    "            outer = GroupKFold(n_splits=outer_folds)\n",
    "            outer_iter = outer.split(X_of, y[:, 0], groups=pid_idx)\n",
    "        else:\n",
    "            tss = TimeSeriesSplit(n_splits=outer_folds)\n",
    "            order = np.argsort(time_ix)\n",
    "            X_order = X_of[order]; y_order = y[order]\n",
    "            outer_iter = ((order[tr], order[te]) for tr, te in tss.split(X_order, y_order[:, 0]))\n",
    "        for fold_id, (tr_idx_all, te_idx) in enumerate(outer_iter, start=1):\n",
    "            tr_idx, va_idx = _make_train_val_split(np.asarray(tr_idx_all), seed=rnd + fold_id)\n",
    "            if verbose:\n",
    "                print(f\"\\n[CV fold {fold_id}/{outer_folds}] scenario={scenario}\")\n",
    "            res = _fit_eval_once_kan(\n",
    "                build_model_fn, wrapper_cls,\n",
    "                arch_defaults, train_defaults,\n",
    "                X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                tr_idx, va_idx, te_idx,\n",
    "                device=device, scenario=scenario,\n",
    "                threshold_selection_source=thr_source,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            fold_metrics_050.append(res[\"metrics@0.5\"])\n",
    "            fold_metrics_opt.append(res[\"metrics@optthr\"])\n",
    "            if verbose:\n",
    "                macro_keys = [k for k in res[\"metrics@optthr\"].keys() if k.startswith(\"macro_\")]\n",
    "                print(\"Fold macro (optthr): \" + \", \".join(f\"{k}={res['metrics@optthr'][k]:.4f}\" for k in macro_keys))\n",
    "        def _summarize_cv_folds(results_folds: List[Dict[str, float]]) -> Dict[str, float]:\n",
    "            if not results_folds:\n",
    "                return {}\n",
    "            all_keys = set().union(*results_folds)\n",
    "            summary = {}\n",
    "            for k in sorted(all_keys):\n",
    "                vals = np.array([fold.get(k, np.nan) for fold in results_folds], dtype=float)\n",
    "                mask = np.isfinite(vals); n = int(mask.sum())\n",
    "                if n == 0:\n",
    "                    m = low = high = np.nan\n",
    "                elif n == 1:\n",
    "                    m = float(vals[mask][0]); low = high = np.nan\n",
    "                else:\n",
    "                    m = float(np.nanmean(vals)); s = float(np.nanstd(vals, ddof=1)); se = s/np.sqrt(n)\n",
    "                    tcrit = float(student_t.ppf(0.975, df=n-1))\n",
    "                    low, high = m - tcrit*se, m + tcrit*se\n",
    "                summary[f\"{k}_mean\"] = m\n",
    "                summary[f\"{k}_95ci_low\"] = low\n",
    "                summary[f\"{k}_95ci_high\"] = high\n",
    "            return summary\n",
    "        cv_summary_050 = _summarize_cv_folds(fold_metrics_050)\n",
    "        cv_summary_opt = _summarize_cv_folds(fold_metrics_opt)\n",
    "        if verbose:\n",
    "            print(\"\\nCV averages (Â±95% CI) @0.5:\")\n",
    "            for key in sorted(cv_summary_050.keys()):\n",
    "                if key.endswith(\"_mean\"):\n",
    "                    base = key[:-5]\n",
    "                    low = cv_summary_050.get(f\"{base}_95ci_low\", np.nan)\n",
    "                    high = cv_summary_050.get(f\"{base}_95ci_high\", np.nan)\n",
    "                    print(f\"{base:>20}: {cv_summary_050[key]:.4f}  (95% CI {low:.4f}, {high:.4f})\")\n",
    "            print(\"\\nCV averages (Â±95% CI) @opt thresholds:\")\n",
    "            for key in sorted(cv_summary_opt.keys()):\n",
    "                if key.endswith(\"_mean\"):\n",
    "                    base = key[:-5]\n",
    "                    low = cv_summary_opt.get(f\"{base}_95ci_low\", np.nan)\n",
    "                    high = cv_summary_opt.get(f\"{base}_95ci_high\", np.nan)\n",
    "                    print(f\"{base:>20}: {cv_summary_opt[key]:.4f}  (95% CI {low:.4f}, {high:.4f})\")\n",
    "            print(\"Arch params (used in all folds):\", arch_defaults)\n",
    "            print(\"Train params (used in all folds):\", train_defaults)\n",
    "        return {\n",
    "            \"cv_folds_metrics@0.5\": fold_metrics_050,\n",
    "            \"cv_folds_metrics@optthr\": fold_metrics_opt,\n",
    "            \"cv_summary@0.5\": cv_summary_050,\n",
    "            \"cv_summary@optthr\": cv_summary_opt,\n",
    "            \"arch_params_final\": dict(arch_defaults),\n",
    "            \"train_params_final\": dict(train_defaults),\n",
    "        }\n",
    "    if mode == \"nested_cv\":\n",
    "        if not param_grid:\n",
    "            param_grid = {\n",
    "                \"d_fix_latent\": [128, 256],\n",
    "                \"d_rand_latent\": [128],\n",
    "                \"n_kernels\": [4],\n",
    "                \"dropout\": [0.0, 0.1],\n",
    "                \"lr\": [1e-3, 3e-4],\n",
    "                \"weight_decay\": [0.0, 1e-4],\n",
    "                \"batch_size\": [256],\n",
    "                \"max_epochs\": [100],\n",
    "                \"patience\": [10],\n",
    "            }\n",
    "        results_folds = []\n",
    "        best_score_global, best_params_global = -np.inf, None\n",
    "        if scenario == \"cases\":\n",
    "            outer = GroupKFold(n_splits=outer_folds)\n",
    "            outer_iter = outer.split(X_of, y[:, 0], groups=pid_idx)\n",
    "        else:\n",
    "            tss = TimeSeriesSplit(n_splits=outer_folds)\n",
    "            order = np.argsort(time_ix)\n",
    "            X_order = X_of[order]; y_order = y[order]\n",
    "            outer_iter = ((order[tr], order[te]) for tr, te in tss.split(X_order, y_order[:, 0]))\n",
    "        for fold_id, (tr_idx_all, te_idx) in enumerate(outer_iter, start=1):\n",
    "            if verbose:\n",
    "                print(f\"\\nOuter fold {fold_id}/{outer_folds} scenario={scenario}\")\n",
    "            def inner_iter():\n",
    "                if scenario == \"cases\":\n",
    "                    inner = GroupKFold(n_splits=inner_folds)\n",
    "                    return inner.split(X_of[tr_idx_all], y[tr_idx_all, 0], groups=pid_idx[tr_idx_all])\n",
    "                else:\n",
    "                    tr_order = np.argsort(time_ix[tr_idx_all])\n",
    "                    X_tr_order = X_of[tr_idx_all][tr_order]\n",
    "                    y_tr_order = y[tr_idx_all][tr_order]\n",
    "                    inner_tss = TimeSeriesSplit(n_splits=inner_folds)\n",
    "                    return ((tr_idx_all[tr_order][itr], tr_idx_all[tr_order][iva])\n",
    "                            for itr, iva in inner_tss.split(X_tr_order, y_tr_order[:, 0]))\n",
    "            best_inner_score, best_inner_params = -np.inf, None\n",
    "            for params in ParameterGrid(param_grid):\n",
    "                arch_params = dict(arch_defaults)\n",
    "                train_params = dict(train_defaults)\n",
    "                for k, v in params.items():\n",
    "                    if k in (\"d_fix_latent\", \"d_rand_latent\", \"n_kernels\", \"dropout\", \"use_attention\", \"use_random\"):\n",
    "                        arch_params[k] = v\n",
    "                    else:\n",
    "                        train_params[k] = v\n",
    "                inner_scores = []\n",
    "                for in_tr, in_va in inner_iter():\n",
    "                    tr_idx_inner, va_idx_inner = _make_train_val_split(np.asarray(in_tr), seed=rnd + fold_id)\n",
    "                    res_inner = _fit_eval_once_kan(\n",
    "                        build_model_fn, wrapper_cls,\n",
    "                        arch_params, train_params,\n",
    "                        X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                        tr_idx_inner, va_idx_inner, in_va,\n",
    "                        device=device, scenario=scenario,\n",
    "                        threshold_selection_source=\"train\",\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    score = res_inner[\"metrics@optthr\"].get(\"macro_F1_optthr\", np.nan)\n",
    "                    inner_scores.append(score)\n",
    "                avg_score = float(np.nanmean(inner_scores)) if len(inner_scores) else -np.inf\n",
    "                if avg_score > best_inner_score:\n",
    "                    best_inner_score = avg_score\n",
    "                    best_inner_params = (arch_params, train_params)\n",
    "            if best_inner_params is None:\n",
    "                if verbose: print(\"No viable inner config; skipping outer fold.\")\n",
    "                continue\n",
    "            arch_params, train_params = best_inner_params\n",
    "            tr_idx_outer, va_idx_outer = _make_train_val_split(np.asarray(tr_idx_all), seed=rnd + fold_id * 17)\n",
    "            res_outer = _fit_eval_once_kan(\n",
    "                build_model_fn, wrapper_cls,\n",
    "                arch_params, train_params,\n",
    "                X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                tr_idx_outer, va_idx_outer, te_idx,\n",
    "                device=device, scenario=scenario,\n",
    "                threshold_selection_source=\"train\",\n",
    "                verbose=False\n",
    "            )\n",
    "            results_folds.append(res_outer)\n",
    "            score_outer = res_outer[\"metrics@optthr\"].get(\"macro_F1_optthr\", -np.inf)\n",
    "            if score_outer > best_score_global:\n",
    "                best_score_global = score_outer\n",
    "                best_params_global = (arch_params, train_params)\n",
    "            if verbose:\n",
    "                macro_keys = [k for k in res_outer[\"metrics@optthr\"].keys() if k.startswith(\"macro_\")]\n",
    "                print(\"Outer fold macro (optthr): \" + \", \".join(f\"{k}={res_outer['metrics@optthr'][k]:.4f}\" for k in macro_keys))\n",
    "        def _summarize_block(results_list: List[Dict[str, Any]], which: str) -> Dict[str, float]:\n",
    "            keys = list(results_list[0][which].keys())\n",
    "            out = {}\n",
    "            for k in keys:\n",
    "                arr = np.array([res[which][k] for res in results_list], dtype=float)\n",
    "                m = float(np.nanmean(arr)); s = float(np.nanstd(arr, ddof=1)); n = len(arr)\n",
    "                se = s / np.sqrt(n) if n > 1 else np.nan\n",
    "                if n > 1:\n",
    "                    tcrit = float(student_t.ppf(0.975, df=n-1))\n",
    "                    ci = (m - tcrit * se, m + tcrit * se)\n",
    "                else:\n",
    "                    ci = (np.nan, np.nan)\n",
    "                out[k + \"_mean\"] = m\n",
    "                out[k + \"_95ci_low\"] = ci[0]\n",
    "                out[k + \"_95ci_high\"] = ci[1]\n",
    "            return out\n",
    "        cv_summary_050    = _summarize_block(results_folds, \"metrics@0.5\")\n",
    "        cv_summary_optthr = _summarize_block(results_folds, \"metrics@optthr\")\n",
    "        if verbose and best_params_global is not None:\n",
    "            print(\"\\nBest params (by outer macro_F1 at opt thresholds):\")\n",
    "            arch_p, train_p = best_params_global\n",
    "            print(\"[ARCH]:\");   [print(f\"  {k}: {v}\") for k, v in arch_p.items()]\n",
    "            print(\"[TRAIN]:\");  [print(f\"  {k}: {v}\") for k, v in train_p.items()]\n",
    "        print(\"\\nCross-validation results:\")\n",
    "        print(\"CV Summary @ 0.5:\")\n",
    "        print(cv_summary_050)\n",
    "        print(\"\\nCV Summary @ Optimal Threshold:\")\n",
    "        print(cv_summary_optthr)\n",
    "        return {\n",
    "            \"outer_folds\": results_folds,\n",
    "            \"cv_summary@0.5\": cv_summary_050,\n",
    "            \"cv_summary@optthr\": cv_summary_optthr,\n",
    "            \"best_params\": (best_params_global[0], best_params_global[1]) if best_params_global else None,\n",
    "        }\n",
    "    raise ValueError(\"mode must be one of {'single','cv_only','nested_cv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ef43a",
   "metadata": {},
   "source": [
    "## Model test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac406d",
   "metadata": {},
   "source": [
    "### Define variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0232db3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1031, 1) (1031, 130) (1031, 66) (1031,) (1031,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ---------- targets & aux ----------\n",
    "y_raw      = GHQ_cat_y.to_numpy(np.float32)\n",
    "y_np       = y_raw if y_raw.ndim == 2 else y_raw.reshape(-1, 1)\n",
    "\n",
    "y_lags_np  = GHQ_cat_outcomes_lags.to_numpy(np.float32)\n",
    "dt_lags_np = GHQ_cat_forecast_horizons.to_numpy(np.float32)\n",
    "\n",
    "# ---------- inputs ----------\n",
    "# keep the two blocks SEPARATE; evaluator will:\n",
    "#   - TC  <- only_fixed (after PCA)\n",
    "#   - X_fix <- concat(only_fixed_PCA, fixed_and_random_PCA)\n",
    "#   - Zrand <- fixed_and_random_PCA\n",
    "X_only_fixed_np        = GHQ_cat_only_fixed.to_numpy(np.float32)\n",
    "X_fixed_and_random_np  = GHQ_cat_fixed_and_random.to_numpy(np.float32)\n",
    "\n",
    "# ---------- ids & time ----------\n",
    "pid_raw    = GHQ_cat_participant_id.to_numpy().ravel()\n",
    "pid_uniqs, pid_encoded = np.unique(pid_raw, return_inverse=True)\n",
    "pid_np     = pid_encoded.astype(np.int64)\n",
    "n_ids      = int(len(pid_uniqs))\n",
    "\n",
    "time_ix_np = GHQ_cat_time.to_numpy().ravel()\n",
    "\n",
    "# ---------- dynamic builder ----------\n",
    "def build_model_fn(\n",
    "    *,\n",
    "    y_dim: int,\n",
    "    d_fix: int,\n",
    "    d_tc: int,        # this will be the PCAâ€™d only_fixed dim (activates FiLM)\n",
    "    d_zrand: int,     # this will be the PCAâ€™d fixed_and_random dim\n",
    "    n_ids: int,\n",
    "    **arch\n",
    "):\n",
    "    return KANAdditiveMixedEffects(\n",
    "        y_dim=y_dim,\n",
    "        d_fix=d_fix,       # = dim(concat(only_fixed_PCA, fixed_and_random_PCA))\n",
    "        d_tc=d_tc,         # = dim(only_fixed_PCA)  -> used by FiLM\n",
    "        d_zrand=d_zrand,   # = dim(fixed_and_random_PCA)\n",
    "        n_ids=n_ids,\n",
    "        **arch\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "arch_defaults = dict(d_fix_latent=256, d_rand_latent=256, n_kernels=8, dropout=0.0)\n",
    "train_defaults = dict(\n",
    "    lr=3e-4, weight_decay=3e-6, batch_size=256,\n",
    "    max_epochs=100, patience=20, threshold=0.5,\n",
    "    lambda_mean0=1e-3, lambda_ridge=3e-3,\n",
    "    lambda_orth_latent=3e-3,\n",
    "    lambda_film_identity=3e-3,\n",
    "    lambda_kan=0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(y_np.shape, X_only_fixed_np.shape, X_fixed_and_random_np.shape, pid_np.shape, time_ix_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ec280",
   "metadata": {},
   "source": [
    "### Simple cases split test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c8348913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[split] train | rows=  742 | unique_ids=  247\n",
      "[split] val   | rows=   82 | unique_ids=   72\n",
      "[split] test  | rows=  204 | unique_ids=   93\n",
      "Epoch 001 | train 0.655723 | val 0.654983\n",
      "Epoch 002 | train 0.647988 | val 0.648192\n",
      "Epoch 003 | train 0.638431 | val 0.637774\n",
      "Epoch 004 | train 0.627023 | val 0.624220\n",
      "Epoch 005 | train 0.611346 | val 0.606442\n",
      "Epoch 006 | train 0.590734 | val 0.584293\n",
      "Epoch 007 | train 0.566892 | val 0.558976\n",
      "Epoch 008 | train 0.540184 | val 0.534596\n",
      "Epoch 009 | train 0.518674 | val 0.517156\n",
      "Epoch 010 | train 0.506928 | val 0.510262\n",
      "Epoch 011 | train 0.498021 | val 0.510095\n",
      "Epoch 012 | train 0.489918 | val 0.507949\n",
      "Epoch 013 | train 0.479653 | val 0.504469\n",
      "Epoch 014 | train 0.471373 | val 0.501960\n",
      "Epoch 015 | train 0.463004 | val 0.503813\n",
      "Epoch 016 | train 0.455794 | val 0.506980\n",
      "Epoch 017 | train 0.447606 | val 0.514464\n",
      "Epoch 018 | train 0.437956 | val 0.524415\n",
      "Epoch 019 | train 0.429094 | val 0.538501\n",
      "Epoch 020 | train 0.417618 | val 0.554193\n",
      "Epoch 021 | train 0.406892 | val 0.573836\n",
      "Epoch 022 | train 0.394014 | val 0.596951\n",
      "Epoch 023 | train 0.380319 | val 0.618403\n",
      "Epoch 024 | train 0.365530 | val 0.639565\n",
      "Epoch 025 | train 0.348421 | val 0.653722\n",
      "Epoch 026 | train 0.330630 | val 0.674640\n",
      "Epoch 027 | train 0.309483 | val 0.693340\n",
      "Epoch 028 | train 0.286562 | val 0.717135\n",
      "Epoch 029 | train 0.264821 | val 0.744173\n",
      "Epoch 030 | train 0.239128 | val 0.781006\n",
      "Epoch 031 | train 0.212740 | val 0.829108\n",
      "Epoch 032 | train 0.185141 | val 0.879474\n",
      "Epoch 033 | train 0.158231 | val 0.925981\n",
      "Epoch 034 | train 0.134986 | val 0.971247\n",
      "Early stopping at epoch 034 (best val 0.501960)\n",
      "\n",
      "Single-fit test metrics @0.5:\n",
      "        task_1_AUC: 0.8500\n",
      "      task_1_AUPRC: 0.9127\n",
      "      task_1_Brier: 0.1589\n",
      "        task_1_ACC: 0.7647\n",
      "         task_1_F1: 0.7949\n",
      "  task_1_Precision: 0.8611\n",
      "     task_1_Recall: 0.7381\n",
      "task_1_Sensitivity: 0.7381\n",
      "task_1_Specificity: 0.8077\n",
      "         macro_AUC: 0.8500\n",
      "       macro_AUPRC: 0.9127\n",
      "       macro_Brier: 0.1589\n",
      "         macro_ACC: 0.7647\n",
      "          macro_F1: 0.7949\n",
      "   macro_Precision: 0.8611\n",
      "      macro_Recall: 0.7381\n",
      " macro_Sensitivity: 0.7381\n",
      " macro_Specificity: 0.8077\n",
      "\n",
      "Single-fit test metrics @F1-opt per task:\n",
      " task_1_AUC_optthr: 0.8500\n",
      "task_1_AUPRC_optthr: 0.9127\n",
      "task_1_Brier_optthr: 0.1589\n",
      " task_1_ACC_optthr: 0.7843\n",
      "  task_1_F1_optthr: 0.8321\n",
      "task_1_Precision_optthr: 0.8015\n",
      "task_1_Recall_optthr: 0.8651\n",
      "task_1_Sensitivity_optthr: 0.8651\n",
      "task_1_Specificity_optthr: 0.6538\n",
      "  macro_AUC_optthr: 0.8500\n",
      "macro_AUPRC_optthr: 0.9127\n",
      "macro_Brier_optthr: 0.1589\n",
      "  macro_ACC_optthr: 0.7843\n",
      "   macro_F1_optthr: 0.8321\n",
      "macro_Precision_optthr: 0.8015\n",
      "macro_Recall_optthr: 0.8651\n",
      "macro_Sensitivity_optthr: 0.8651\n",
      "macro_Specificity_optthr: 0.6538\n",
      "\n",
      "Model: KANAdditiveMixedEffects | params=5076880\n",
      "Final arch params: {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0}\n",
      "Final train params: {'lr': 0.0003, 'weight_decay': 3e-06, 'batch_size': 256, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 0.001, 'lambda_ridge': 0.003, 'lambda_orth_latent': 0.003, 'lambda_film_identity': 0.003, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"single\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"time\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd54bc3",
   "metadata": {},
   "source": [
    "### Simple time split test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9edf5e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[split] train | rows=  751 | unique_ids=  200\n",
      "[split] val   | rows=   83 | unique_ids=   67\n",
      "[split] test  | rows=  197 | unique_ids=   51\n",
      "Epoch 001 | train 0.625355 | val 0.621029\n",
      "Epoch 002 | train 0.614971 | val 0.610853\n",
      "Epoch 003 | train 0.603476 | val 0.598269\n",
      "Epoch 004 | train 0.589149 | val 0.582723\n",
      "Epoch 005 | train 0.570891 | val 0.563352\n",
      "Epoch 006 | train 0.548452 | val 0.540165\n",
      "Epoch 007 | train 0.521367 | val 0.515338\n",
      "Epoch 008 | train 0.492249 | val 0.491707\n",
      "Epoch 009 | train 0.463986 | val 0.474024\n",
      "Epoch 010 | train 0.442665 | val 0.463530\n",
      "Epoch 011 | train 0.430051 | val 0.460556\n",
      "Epoch 012 | train 0.424118 | val 0.459908\n",
      "Epoch 013 | train 0.416297 | val 0.462933\n",
      "Epoch 014 | train 0.407116 | val 0.464862\n",
      "Epoch 015 | train 0.398769 | val 0.459882\n",
      "Epoch 016 | train 0.390784 | val 0.455816\n",
      "Epoch 017 | train 0.382113 | val 0.451618\n",
      "Epoch 018 | train 0.373503 | val 0.449699\n",
      "Epoch 019 | train 0.363996 | val 0.450802\n",
      "Epoch 020 | train 0.354566 | val 0.451563\n",
      "Epoch 021 | train 0.343139 | val 0.460571\n",
      "Epoch 022 | train 0.332454 | val 0.467252\n",
      "Epoch 023 | train 0.320529 | val 0.476251\n",
      "Epoch 024 | train 0.306535 | val 0.480613\n",
      "Epoch 025 | train 0.294322 | val 0.494796\n",
      "Epoch 026 | train 0.280133 | val 0.513810\n",
      "Epoch 027 | train 0.264699 | val 0.515476\n",
      "Epoch 028 | train 0.250129 | val 0.530635\n",
      "Epoch 029 | train 0.233563 | val 0.550991\n",
      "Epoch 030 | train 0.215253 | val 0.559962\n",
      "Epoch 031 | train 0.196828 | val 0.578784\n",
      "Epoch 032 | train 0.179354 | val 0.595066\n",
      "Epoch 033 | train 0.159430 | val 0.594434\n",
      "Epoch 034 | train 0.138786 | val 0.649785\n",
      "Epoch 035 | train 0.119968 | val 0.661103\n",
      "Epoch 036 | train 0.098067 | val 0.686728\n",
      "Epoch 037 | train 0.076364 | val 0.760768\n",
      "Epoch 038 | train 0.058247 | val 0.784306\n",
      "Early stopping at epoch 038 (best val 0.449699)\n",
      "\n",
      "Single-fit test metrics @0.5:\n",
      "        task_1_AUC: 0.6991\n",
      "      task_1_AUPRC: 0.7031\n",
      "      task_1_Brier: 0.2358\n",
      "        task_1_ACC: 0.6701\n",
      "         task_1_F1: 0.6597\n",
      "  task_1_Precision: 0.7000\n",
      "     task_1_Recall: 0.6238\n",
      "task_1_Sensitivity: 0.6238\n",
      "task_1_Specificity: 0.7188\n",
      "         macro_AUC: 0.6991\n",
      "       macro_AUPRC: 0.7031\n",
      "       macro_Brier: 0.2358\n",
      "         macro_ACC: 0.6701\n",
      "          macro_F1: 0.6597\n",
      "   macro_Precision: 0.7000\n",
      "      macro_Recall: 0.6238\n",
      " macro_Sensitivity: 0.6238\n",
      " macro_Specificity: 0.7188\n",
      "\n",
      "Single-fit test metrics @F1-opt per task:\n",
      " task_1_AUC_optthr: 0.6991\n",
      "task_1_AUPRC_optthr: 0.7031\n",
      "task_1_Brier_optthr: 0.2358\n",
      " task_1_ACC_optthr: 0.6396\n",
      "  task_1_F1_optthr: 0.7029\n",
      "task_1_Precision_optthr: 0.6087\n",
      "task_1_Recall_optthr: 0.8317\n",
      "task_1_Sensitivity_optthr: 0.8317\n",
      "task_1_Specificity_optthr: 0.4375\n",
      "  macro_AUC_optthr: 0.6991\n",
      "macro_AUPRC_optthr: 0.7031\n",
      "macro_Brier_optthr: 0.2358\n",
      "  macro_ACC_optthr: 0.6396\n",
      "   macro_F1_optthr: 0.7029\n",
      "macro_Precision_optthr: 0.6087\n",
      "macro_Recall_optthr: 0.8317\n",
      "macro_Sensitivity_optthr: 0.8317\n",
      "macro_Specificity_optthr: 0.4375\n",
      "\n",
      "Model: KANAdditiveMixedEffects | params=5043600\n",
      "Final arch params: {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0}\n",
      "Final train params: {'lr': 0.0003, 'weight_decay': 3e-06, 'batch_size': 256, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 0.001, 'lambda_ridge': 0.003, 'lambda_orth_latent': 0.003, 'lambda_film_identity': 0.003, 'lambda_kan': 0, 'random_state': 42}\n",
      "[split] train | rows=  742 | unique_ids=  247\n",
      "[split] val   | rows=   82 | unique_ids=   72\n",
      "[split] test  | rows=  204 | unique_ids=   93\n",
      "Epoch 001 | train 0.657116 | val 0.657494\n",
      "Epoch 002 | train 0.649883 | val 0.651166\n",
      "Epoch 003 | train 0.641735 | val 0.642880\n",
      "Epoch 004 | train 0.632020 | val 0.631968\n",
      "Epoch 005 | train 0.618429 | val 0.617257\n",
      "Epoch 006 | train 0.600828 | val 0.598346\n",
      "Epoch 007 | train 0.579702 | val 0.575280\n",
      "Epoch 008 | train 0.554695 | val 0.550492\n",
      "Epoch 009 | train 0.530789 | val 0.528016\n",
      "Epoch 010 | train 0.510624 | val 0.513923\n",
      "Epoch 011 | train 0.497143 | val 0.510657\n",
      "Epoch 012 | train 0.490448 | val 0.511531\n",
      "Epoch 013 | train 0.481281 | val 0.508758\n",
      "Epoch 014 | train 0.471863 | val 0.507050\n",
      "Epoch 015 | train 0.463903 | val 0.507156\n",
      "Epoch 016 | train 0.456461 | val 0.509998\n",
      "Epoch 017 | train 0.447704 | val 0.516334\n",
      "Epoch 018 | train 0.439707 | val 0.525692\n",
      "Epoch 019 | train 0.430581 | val 0.537909\n",
      "Epoch 020 | train 0.420554 | val 0.553693\n",
      "Epoch 021 | train 0.409759 | val 0.575139\n",
      "Epoch 022 | train 0.397190 | val 0.597479\n",
      "Epoch 023 | train 0.384558 | val 0.622251\n",
      "Epoch 024 | train 0.369839 | val 0.640152\n",
      "Epoch 025 | train 0.353688 | val 0.663808\n",
      "Epoch 026 | train 0.336576 | val 0.691498\n",
      "Epoch 027 | train 0.317744 | val 0.707044\n",
      "Epoch 028 | train 0.294326 | val 0.731867\n",
      "Epoch 029 | train 0.269612 | val 0.756269\n",
      "Epoch 030 | train 0.242997 | val 0.783706\n",
      "Epoch 031 | train 0.217158 | val 0.816894\n",
      "Epoch 032 | train 0.187991 | val 0.852090\n",
      "Epoch 033 | train 0.158344 | val 0.908233\n",
      "Epoch 034 | train 0.132864 | val 0.941960\n",
      "Early stopping at epoch 034 (best val 0.507050)\n",
      "\n",
      "Single-fit test metrics @0.5:\n",
      "        task_1_AUC: 0.8485\n",
      "      task_1_AUPRC: 0.9114\n",
      "      task_1_Brier: 0.1587\n",
      "        task_1_ACC: 0.7696\n",
      "         task_1_F1: 0.7983\n",
      "  task_1_Precision: 0.8692\n",
      "     task_1_Recall: 0.7381\n",
      "task_1_Sensitivity: 0.7381\n",
      "task_1_Specificity: 0.8205\n",
      "         macro_AUC: 0.8485\n",
      "       macro_AUPRC: 0.9114\n",
      "       macro_Brier: 0.1587\n",
      "         macro_ACC: 0.7696\n",
      "          macro_F1: 0.7983\n",
      "   macro_Precision: 0.8692\n",
      "      macro_Recall: 0.7381\n",
      " macro_Sensitivity: 0.7381\n",
      " macro_Specificity: 0.8205\n",
      "\n",
      "Single-fit test metrics @F1-opt per task:\n",
      " task_1_AUC_optthr: 0.8485\n",
      "task_1_AUPRC_optthr: 0.9114\n",
      "task_1_Brier_optthr: 0.1587\n",
      " task_1_ACC_optthr: 0.7696\n",
      "  task_1_F1_optthr: 0.8185\n",
      "task_1_Precision_optthr: 0.7970\n",
      "task_1_Recall_optthr: 0.8413\n",
      "task_1_Sensitivity_optthr: 0.8413\n",
      "task_1_Specificity_optthr: 0.6538\n",
      "  macro_AUC_optthr: 0.8485\n",
      "macro_AUPRC_optthr: 0.9114\n",
      "macro_Brier_optthr: 0.1587\n",
      "  macro_ACC_optthr: 0.7696\n",
      "   macro_F1_optthr: 0.8185\n",
      "macro_Precision_optthr: 0.7970\n",
      "macro_Recall_optthr: 0.8413\n",
      "macro_Sensitivity_optthr: 0.8413\n",
      "macro_Specificity_optthr: 0.6538\n",
      "\n",
      "Model: KANAdditiveMixedEffects | params=5076880\n",
      "Final arch params: {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0}\n",
      "Final train params: {'lr': 0.0003, 'weight_decay': 3e-06, 'batch_size': 256, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 0.001, 'lambda_ridge': 0.003, 'lambda_orth_latent': 0.003, 'lambda_film_identity': 0.003, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"single\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"both\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54510d6a",
   "metadata": {},
   "source": [
    "### Cases split test CV without parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "11b8a291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV fold 1/3] scenario=cases\n",
      "[split] train | rows=  619 | unique_ids=  162\n",
      "[split] val   | rows=   68 | unique_ids=   51\n",
      "[split] test  | rows=  344 | unique_ids=   85\n",
      "Epoch 001 | train 0.626386 | val 0.631180\n",
      "Epoch 002 | train 0.618664 | val 0.626191\n",
      "Epoch 003 | train 0.609833 | val 0.619980\n",
      "Epoch 004 | train 0.598692 | val 0.613087\n",
      "Epoch 005 | train 0.584390 | val 0.604130\n",
      "Epoch 006 | train 0.565252 | val 0.593680\n",
      "Epoch 007 | train 0.543033 | val 0.582972\n",
      "Epoch 008 | train 0.516012 | val 0.574479\n",
      "Epoch 009 | train 0.490675 | val 0.571622\n",
      "Epoch 010 | train 0.468066 | val 0.571354\n",
      "Epoch 011 | train 0.451320 | val 0.577060\n",
      "Epoch 012 | train 0.438476 | val 0.582741\n",
      "Epoch 013 | train 0.429051 | val 0.587713\n",
      "Epoch 014 | train 0.419968 | val 0.590652\n",
      "Epoch 015 | train 0.412148 | val 0.592757\n",
      "Epoch 016 | train 0.403862 | val 0.595539\n",
      "Epoch 017 | train 0.395921 | val 0.599732\n",
      "Epoch 018 | train 0.388455 | val 0.602804\n",
      "Epoch 019 | train 0.380055 | val 0.607732\n",
      "Epoch 020 | train 0.370436 | val 0.615369\n",
      "Epoch 021 | train 0.359895 | val 0.622548\n",
      "Epoch 022 | train 0.350625 | val 0.627206\n",
      "Epoch 023 | train 0.339694 | val 0.636672\n",
      "Epoch 024 | train 0.329189 | val 0.644003\n",
      "Epoch 025 | train 0.319108 | val 0.627174\n",
      "Epoch 026 | train 0.308044 | val 0.618544\n",
      "Epoch 027 | train 0.296591 | val 0.622041\n",
      "Epoch 028 | train 0.284025 | val 0.630265\n",
      "Epoch 029 | train 0.271330 | val 0.634711\n",
      "Epoch 030 | train 0.257403 | val 0.630807\n",
      "Early stopping at epoch 030 (best val 0.571354)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7966, macro_AUPRC_optthr=0.8132, macro_Brier_optthr=0.1827, macro_ACC_optthr=0.6657, macro_F1_optthr=0.7368, macro_Precision_optthr=0.6364, macro_Recall_optthr=0.8750, macro_Sensitivity_optthr=0.8750, macro_Specificity_optthr=0.4250\n",
      "\n",
      "[CV fold 2/3] scenario=cases\n",
      "[split] train | rows=  619 | unique_ids=  169\n",
      "[split] val   | rows=   68 | unique_ids=   55\n",
      "[split] test  | rows=  344 | unique_ids=   86\n",
      "Epoch 001 | train 0.660857 | val 0.645835\n",
      "Epoch 002 | train 0.651860 | val 0.630871\n",
      "Epoch 003 | train 0.642154 | val 0.613207\n",
      "Epoch 004 | train 0.629538 | val 0.591743\n",
      "Epoch 005 | train 0.613475 | val 0.564961\n",
      "Epoch 006 | train 0.592906 | val 0.531286\n",
      "Epoch 007 | train 0.568658 | val 0.492928\n",
      "Epoch 008 | train 0.543594 | val 0.455310\n",
      "Epoch 009 | train 0.522298 | val 0.423343\n",
      "Epoch 010 | train 0.507463 | val 0.401743\n",
      "Epoch 011 | train 0.496953 | val 0.389232\n",
      "Epoch 012 | train 0.490109 | val 0.380080\n",
      "Epoch 013 | train 0.482716 | val 0.373467\n",
      "Epoch 014 | train 0.474933 | val 0.370124\n",
      "Epoch 015 | train 0.467713 | val 0.370000\n",
      "Epoch 016 | train 0.458655 | val 0.376109\n",
      "Epoch 017 | train 0.449775 | val 0.382554\n",
      "Epoch 018 | train 0.441603 | val 0.387444\n",
      "Epoch 019 | train 0.430897 | val 0.386763\n",
      "Epoch 020 | train 0.422209 | val 0.386939\n",
      "Epoch 021 | train 0.409102 | val 0.394077\n",
      "Epoch 022 | train 0.396864 | val 0.410044\n",
      "Epoch 023 | train 0.384572 | val 0.415266\n",
      "Epoch 024 | train 0.369792 | val 0.416957\n",
      "Epoch 025 | train 0.356364 | val 0.418756\n",
      "Epoch 026 | train 0.341720 | val 0.427604\n",
      "Epoch 027 | train 0.327231 | val 0.445949\n",
      "Epoch 028 | train 0.313240 | val 0.431434\n",
      "Epoch 029 | train 0.293698 | val 0.427276\n",
      "Epoch 030 | train 0.276846 | val 0.431350\n",
      "Epoch 031 | train 0.258019 | val 0.438973\n",
      "Epoch 032 | train 0.238006 | val 0.431821\n",
      "Epoch 033 | train 0.214922 | val 0.445688\n",
      "Epoch 034 | train 0.190645 | val 0.467798\n",
      "Epoch 035 | train 0.165884 | val 0.486423\n",
      "Early stopping at epoch 035 (best val 0.370000)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7916, macro_AUPRC_optthr=0.8024, macro_Brier_optthr=0.1850, macro_ACC_optthr=0.7616, macro_F1_optthr=0.7940, macro_Precision_optthr=0.7633, macro_Recall_optthr=0.8272, macro_Sensitivity_optthr=0.8272, macro_Specificity_optthr=0.6797\n",
      "\n",
      "[CV fold 3/3] scenario=cases\n",
      "[split] train | rows=  620 | unique_ids=  166\n",
      "[split] val   | rows=   68 | unique_ids=   54\n",
      "[split] test  | rows=  343 | unique_ids=   86\n",
      "Epoch 001 | train 0.621835 | val 0.626838\n",
      "Epoch 002 | train 0.613957 | val 0.620770\n",
      "Epoch 003 | train 0.605711 | val 0.612877\n",
      "Epoch 004 | train 0.595394 | val 0.601050\n",
      "Epoch 005 | train 0.581667 | val 0.585049\n",
      "Epoch 006 | train 0.564011 | val 0.565899\n",
      "Epoch 007 | train 0.541871 | val 0.542764\n",
      "Epoch 008 | train 0.517190 | val 0.520884\n",
      "Epoch 009 | train 0.490770 | val 0.500758\n",
      "Epoch 010 | train 0.465837 | val 0.485471\n",
      "Epoch 011 | train 0.450527 | val 0.478388\n",
      "Epoch 012 | train 0.441242 | val 0.478941\n",
      "Epoch 013 | train 0.432069 | val 0.485404\n",
      "Epoch 014 | train 0.421846 | val 0.501641\n",
      "Epoch 015 | train 0.411243 | val 0.516952\n",
      "Epoch 016 | train 0.402908 | val 0.526471\n",
      "Epoch 017 | train 0.392534 | val 0.518227\n",
      "Epoch 018 | train 0.382516 | val 0.520372\n",
      "Epoch 019 | train 0.373168 | val 0.538032\n",
      "Epoch 020 | train 0.362327 | val 0.557099\n",
      "Epoch 021 | train 0.351675 | val 0.577997\n",
      "Epoch 022 | train 0.339772 | val 0.593107\n",
      "Epoch 023 | train 0.327895 | val 0.597686\n",
      "Epoch 024 | train 0.315149 | val 0.612466\n",
      "Epoch 025 | train 0.303492 | val 0.634842\n",
      "Epoch 026 | train 0.288981 | val 0.659809\n",
      "Epoch 027 | train 0.274306 | val 0.665679\n",
      "Epoch 028 | train 0.260040 | val 0.654784\n",
      "Epoch 029 | train 0.244529 | val 0.670838\n",
      "Epoch 030 | train 0.227161 | val 0.710495\n",
      "Epoch 031 | train 0.214613 | val 0.756623\n",
      "Early stopping at epoch 031 (best val 0.478388)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7711, macro_AUPRC_optthr=0.7792, macro_Brier_optthr=0.1953, macro_ACC_optthr=0.7085, macro_F1_optthr=0.7549, macro_Precision_optthr=0.6784, macro_Recall_optthr=0.8508, macro_Sensitivity_optthr=0.8508, macro_Specificity_optthr=0.5494\n",
      "\n",
      "CV averages (Â±95% CI) @0.5:\n",
      "           macro_ACC: 0.7313  (95% CI 0.6479, 0.8146)\n",
      "           macro_AUC: 0.7864  (95% CI 0.7529, 0.8199)\n",
      "         macro_AUPRC: 0.7983  (95% CI 0.7552, 0.8414)\n",
      "         macro_Brier: 0.1877  (95% CI 0.1710, 0.2043)\n",
      "            macro_F1: 0.7476  (95% CI 0.6738, 0.8214)\n",
      "     macro_Precision: 0.7587  (95% CI 0.6416, 0.8758)\n",
      "        macro_Recall: 0.7374  (95% CI 0.6969, 0.7779)\n",
      "   macro_Sensitivity: 0.7374  (95% CI 0.6969, 0.7779)\n",
      "   macro_Specificity: 0.7249  (95% CI 0.5857, 0.8640)\n",
      "          task_1_ACC: 0.7313  (95% CI 0.6479, 0.8146)\n",
      "          task_1_AUC: 0.7864  (95% CI 0.7529, 0.8199)\n",
      "        task_1_AUPRC: 0.7983  (95% CI 0.7552, 0.8414)\n",
      "        task_1_Brier: 0.1877  (95% CI 0.1710, 0.2043)\n",
      "           task_1_F1: 0.7476  (95% CI 0.6738, 0.8214)\n",
      "    task_1_Precision: 0.7587  (95% CI 0.6416, 0.8758)\n",
      "       task_1_Recall: 0.7374  (95% CI 0.6969, 0.7779)\n",
      "  task_1_Sensitivity: 0.7374  (95% CI 0.6969, 0.7779)\n",
      "  task_1_Specificity: 0.7249  (95% CI 0.5857, 0.8640)\n",
      "\n",
      "CV averages (Â±95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.7119  (95% CI 0.5925, 0.8313)\n",
      "    macro_AUC_optthr: 0.7864  (95% CI 0.7529, 0.8199)\n",
      "  macro_AUPRC_optthr: 0.7983  (95% CI 0.7552, 0.8414)\n",
      "  macro_Brier_optthr: 0.1877  (95% CI 0.1710, 0.2043)\n",
      "     macro_F1_optthr: 0.7619  (95% CI 0.6894, 0.8344)\n",
      "macro_Precision_optthr: 0.6927  (95% CI 0.5321, 0.8533)\n",
      " macro_Recall_optthr: 0.8510  (95% CI 0.7917, 0.9104)\n",
      "macro_Sensitivity_optthr: 0.8510  (95% CI 0.7917, 0.9104)\n",
      "macro_Specificity_optthr: 0.5514  (95% CI 0.2349, 0.8678)\n",
      "   task_1_ACC_optthr: 0.7119  (95% CI 0.5925, 0.8313)\n",
      "   task_1_AUC_optthr: 0.7864  (95% CI 0.7529, 0.8199)\n",
      " task_1_AUPRC_optthr: 0.7983  (95% CI 0.7552, 0.8414)\n",
      " task_1_Brier_optthr: 0.1877  (95% CI 0.1710, 0.2043)\n",
      "    task_1_F1_optthr: 0.7619  (95% CI 0.6894, 0.8344)\n",
      "task_1_Precision_optthr: 0.6927  (95% CI 0.5321, 0.8533)\n",
      "task_1_Recall_optthr: 0.8510  (95% CI 0.7917, 0.9104)\n",
      "task_1_Sensitivity_optthr: 0.8510  (95% CI 0.7917, 0.9104)\n",
      "task_1_Specificity_optthr: 0.5514  (95% CI 0.2349, 0.8678)\n",
      "Arch params (used in all folds): {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0}\n",
      "Train params (used in all folds): {'lr': 0.0003, 'weight_decay': 3e-06, 'batch_size': 256, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 0.001, 'lambda_ridge': 0.003, 'lambda_orth_latent': 0.003, 'lambda_film_identity': 0.003, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"cv_only\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"cases\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5c06e",
   "metadata": {},
   "source": [
    "### Time split test CV without parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c1e00e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV fold 1/3] scenario=cases\n",
      "[split] train | rows=  619 | unique_ids=  162\n",
      "[split] val   | rows=   68 | unique_ids=   51\n",
      "[split] test  | rows=  344 | unique_ids=   85\n",
      "Epoch 001 | train 0.629266 | val 0.639404\n",
      "Epoch 002 | train 0.608112 | val 0.623038\n",
      "Epoch 003 | train 0.585267 | val 0.607057\n",
      "Epoch 004 | train 0.558784 | val 0.592006\n",
      "Epoch 005 | train 0.529264 | val 0.579286\n",
      "Epoch 006 | train 0.494826 | val 0.573656\n",
      "Epoch 007 | train 0.465392 | val 0.582767\n",
      "Epoch 008 | train 0.444371 | val 0.610108\n",
      "Epoch 009 | train 0.436558 | val 0.640593\n",
      "Epoch 010 | train 0.431692 | val 0.650805\n",
      "Epoch 011 | train 0.425172 | val 0.645518\n",
      "Epoch 012 | train 0.415187 | val 0.629519\n",
      "Epoch 013 | train 0.405254 | val 0.616713\n",
      "Epoch 014 | train 0.397368 | val 0.607551\n",
      "Epoch 015 | train 0.391718 | val 0.602223\n",
      "Epoch 016 | train 0.385515 | val 0.603144\n",
      "Epoch 017 | train 0.377531 | val 0.608961\n",
      "Epoch 018 | train 0.368462 | val 0.620888\n",
      "Epoch 019 | train 0.359260 | val 0.636419\n",
      "Epoch 020 | train 0.350693 | val 0.648991\n",
      "Epoch 021 | train 0.341962 | val 0.657624\n",
      "Epoch 022 | train 0.332525 | val 0.663248\n",
      "Epoch 023 | train 0.322942 | val 0.668234\n",
      "Epoch 024 | train 0.315125 | val 0.672895\n",
      "Epoch 025 | train 0.305985 | val 0.683012\n",
      "Epoch 026 | train 0.296562 | val 0.689263\n",
      "Early stopping at epoch 026 (best val 0.573656)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8297, macro_AUPRC_optthr=0.8329, macro_Brier_optthr=0.1747, macro_ACC_optthr=0.7587, macro_F1_optthr=0.7726, macro_Precision_optthr=0.7790, macro_Recall_optthr=0.7663, macro_Sensitivity_optthr=0.7663, macro_Specificity_optthr=0.7500\n",
      "\n",
      "[CV fold 2/3] scenario=cases\n",
      "[split] train | rows=  619 | unique_ids=  169\n",
      "[split] val   | rows=   68 | unique_ids=   55\n",
      "[split] test  | rows=  344 | unique_ids=   86\n",
      "Epoch 001 | train 0.661950 | val 0.638247\n",
      "Epoch 002 | train 0.640112 | val 0.607569\n",
      "Epoch 003 | train 0.620336 | val 0.576583\n",
      "Epoch 004 | train 0.597045 | val 0.545660\n",
      "Epoch 005 | train 0.569708 | val 0.511596\n",
      "Epoch 006 | train 0.540006 | val 0.476387\n",
      "Epoch 007 | train 0.508311 | val 0.447980\n",
      "Epoch 008 | train 0.489064 | val 0.432794\n",
      "Epoch 009 | train 0.481543 | val 0.424623\n",
      "Epoch 010 | train 0.475109 | val 0.411251\n",
      "Epoch 011 | train 0.465542 | val 0.398457\n",
      "Epoch 012 | train 0.455387 | val 0.390385\n",
      "Epoch 013 | train 0.447695 | val 0.386421\n",
      "Epoch 014 | train 0.441492 | val 0.387109\n",
      "Epoch 015 | train 0.435298 | val 0.391557\n",
      "Epoch 016 | train 0.427472 | val 0.393982\n",
      "Epoch 017 | train 0.420104 | val 0.396403\n",
      "Epoch 018 | train 0.410986 | val 0.399636\n",
      "Epoch 019 | train 0.401779 | val 0.403215\n",
      "Epoch 020 | train 0.393335 | val 0.408296\n",
      "Epoch 021 | train 0.383763 | val 0.417071\n",
      "Epoch 022 | train 0.373732 | val 0.426318\n",
      "Epoch 023 | train 0.363756 | val 0.433074\n",
      "Epoch 024 | train 0.354114 | val 0.438441\n",
      "Epoch 025 | train 0.344645 | val 0.443456\n",
      "Epoch 026 | train 0.334009 | val 0.453865\n",
      "Epoch 027 | train 0.323849 | val 0.466164\n",
      "Epoch 028 | train 0.312806 | val 0.473148\n",
      "Epoch 029 | train 0.302161 | val 0.481345\n",
      "Epoch 030 | train 0.291348 | val 0.494104\n",
      "Epoch 031 | train 0.281204 | val 0.505397\n",
      "Epoch 032 | train 0.267674 | val 0.520132\n",
      "Epoch 033 | train 0.256453 | val 0.527511\n",
      "Early stopping at epoch 033 (best val 0.386421)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8140, macro_AUPRC_optthr=0.8158, macro_Brier_optthr=0.1767, macro_ACC_optthr=0.7297, macro_F1_optthr=0.7780, macro_Precision_optthr=0.7149, macro_Recall_optthr=0.8534, macro_Sensitivity_optthr=0.8534, macro_Specificity_optthr=0.5752\n",
      "\n",
      "[CV fold 3/3] scenario=cases\n",
      "[split] train | rows=  620 | unique_ids=  166\n",
      "[split] val   | rows=   68 | unique_ids=   54\n",
      "[split] test  | rows=  343 | unique_ids=   86\n",
      "Epoch 001 | train 0.618392 | val 0.613791\n",
      "Epoch 002 | train 0.597408 | val 0.599589\n",
      "Epoch 003 | train 0.576068 | val 0.582007\n",
      "Epoch 004 | train 0.550950 | val 0.559580\n",
      "Epoch 005 | train 0.522457 | val 0.535255\n",
      "Epoch 006 | train 0.489254 | val 0.510096\n",
      "Epoch 007 | train 0.456401 | val 0.493723\n",
      "Epoch 008 | train 0.432668 | val 0.494423\n",
      "Epoch 009 | train 0.418215 | val 0.501252\n",
      "Epoch 010 | train 0.411844 | val 0.506449\n",
      "Epoch 011 | train 0.402376 | val 0.512100\n",
      "Epoch 012 | train 0.391173 | val 0.517437\n",
      "Epoch 013 | train 0.379964 | val 0.524893\n",
      "Epoch 014 | train 0.370242 | val 0.537179\n",
      "Epoch 015 | train 0.361695 | val 0.544619\n",
      "Epoch 016 | train 0.352347 | val 0.553491\n",
      "Epoch 017 | train 0.342807 | val 0.562643\n",
      "Epoch 018 | train 0.332972 | val 0.572417\n",
      "Epoch 019 | train 0.323787 | val 0.586492\n",
      "Epoch 020 | train 0.314027 | val 0.601535\n",
      "Epoch 021 | train 0.303734 | val 0.616690\n",
      "Epoch 022 | train 0.294264 | val 0.633916\n",
      "Epoch 023 | train 0.284250 | val 0.652031\n",
      "Epoch 024 | train 0.274132 | val 0.673450\n",
      "Epoch 025 | train 0.263852 | val 0.685947\n",
      "Epoch 026 | train 0.253521 | val 0.701810\n",
      "Epoch 027 | train 0.243511 | val 0.721799\n",
      "Early stopping at epoch 027 (best val 0.493723)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8013, macro_AUPRC_optthr=0.8030, macro_Brier_optthr=0.1835, macro_ACC_optthr=0.6997, macro_F1_optthr=0.7588, macro_Precision_optthr=0.6585, macro_Recall_optthr=0.8950, macro_Sensitivity_optthr=0.8950, macro_Specificity_optthr=0.4815\n",
      "\n",
      "CV averages (Â±95% CI) @0.5:\n",
      "           macro_ACC: 0.7488  (95% CI 0.7080, 0.7895)\n",
      "           macro_AUC: 0.8150  (95% CI 0.7796, 0.8504)\n",
      "         macro_AUPRC: 0.8172  (95% CI 0.7799, 0.8545)\n",
      "         macro_Brier: 0.1783  (95% CI 0.1668, 0.1898)\n",
      "            macro_F1: 0.7666  (95% CI 0.7448, 0.7884)\n",
      "     macro_Precision: 0.7702  (95% CI 0.6740, 0.8663)\n",
      "        macro_Recall: 0.7647  (95% CI 0.7098, 0.8195)\n",
      "   macro_Sensitivity: 0.7647  (95% CI 0.7098, 0.8195)\n",
      "   macro_Specificity: 0.7310  (95% CI 0.5855, 0.8765)\n",
      "          task_1_ACC: 0.7488  (95% CI 0.7080, 0.7895)\n",
      "          task_1_AUC: 0.8150  (95% CI 0.7796, 0.8504)\n",
      "        task_1_AUPRC: 0.8172  (95% CI 0.7799, 0.8545)\n",
      "        task_1_Brier: 0.1783  (95% CI 0.1668, 0.1898)\n",
      "           task_1_F1: 0.7666  (95% CI 0.7448, 0.7884)\n",
      "    task_1_Precision: 0.7702  (95% CI 0.6740, 0.8663)\n",
      "       task_1_Recall: 0.7647  (95% CI 0.7098, 0.8195)\n",
      "  task_1_Sensitivity: 0.7647  (95% CI 0.7098, 0.8195)\n",
      "  task_1_Specificity: 0.7310  (95% CI 0.5855, 0.8765)\n",
      "\n",
      "CV averages (Â±95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.7294  (95% CI 0.6561, 0.8027)\n",
      "    macro_AUC_optthr: 0.8150  (95% CI 0.7796, 0.8504)\n",
      "  macro_AUPRC_optthr: 0.8172  (95% CI 0.7799, 0.8545)\n",
      "  macro_Brier_optthr: 0.1783  (95% CI 0.1668, 0.1898)\n",
      "     macro_F1_optthr: 0.7698  (95% CI 0.7451, 0.7945)\n",
      "macro_Precision_optthr: 0.7175  (95% CI 0.5678, 0.8672)\n",
      " macro_Recall_optthr: 0.8382  (95% CI 0.6751, 1.0014)\n",
      "macro_Sensitivity_optthr: 0.8382  (95% CI 0.6751, 1.0014)\n",
      "macro_Specificity_optthr: 0.6022  (95% CI 0.2637, 0.9408)\n",
      "   task_1_ACC_optthr: 0.7294  (95% CI 0.6561, 0.8027)\n",
      "   task_1_AUC_optthr: 0.8150  (95% CI 0.7796, 0.8504)\n",
      " task_1_AUPRC_optthr: 0.8172  (95% CI 0.7799, 0.8545)\n",
      " task_1_Brier_optthr: 0.1783  (95% CI 0.1668, 0.1898)\n",
      "    task_1_F1_optthr: 0.7698  (95% CI 0.7451, 0.7945)\n",
      "task_1_Precision_optthr: 0.7175  (95% CI 0.5678, 0.8672)\n",
      "task_1_Recall_optthr: 0.8382  (95% CI 0.6751, 1.0014)\n",
      "task_1_Sensitivity_optthr: 0.8382  (95% CI 0.6751, 1.0014)\n",
      "task_1_Specificity_optthr: 0.6022  (95% CI 0.2637, 0.9408)\n",
      "Arch params (used in all folds): {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0}\n",
      "Train params (used in all folds): {'lr': 0.0003, 'weight_decay': 3e-06, 'batch_size': 256, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 0.001, 'lambda_ridge': 0.003, 'lambda_orth_latent': 0.003, 'lambda_film_identity': 0.003, 'lambda_kan': 0, 'random_state': 42}\n",
      "\n",
      "[CV fold 1/3] scenario=time\n",
      "[split] train | rows=  234 | unique_ids=  231\n",
      "[split] val   | rows=   26 | unique_ids=   26\n",
      "[split] test  | rows=  257 | unique_ids=  165\n",
      "Epoch 001 | train 0.631863 | val 0.624517\n",
      "Epoch 002 | train 0.624977 | val 0.615401\n",
      "Epoch 003 | train 0.618513 | val 0.606763\n",
      "Epoch 004 | train 0.612288 | val 0.598500\n",
      "Epoch 005 | train 0.606114 | val 0.590516\n",
      "Epoch 006 | train 0.599814 | val 0.582712\n",
      "Epoch 007 | train 0.593222 | val 0.575004\n",
      "Epoch 008 | train 0.586199 | val 0.567333\n",
      "Epoch 009 | train 0.578636 | val 0.559670\n",
      "Epoch 010 | train 0.570459 | val 0.552025\n",
      "Epoch 011 | train 0.561628 | val 0.544451\n",
      "Epoch 012 | train 0.552139 | val 0.537049\n",
      "Epoch 013 | train 0.542024 | val 0.529977\n",
      "Epoch 014 | train 0.531356 | val 0.523459\n",
      "Epoch 015 | train 0.520256 | val 0.517788\n",
      "Epoch 016 | train 0.508896 | val 0.513329\n",
      "Epoch 017 | train 0.497501 | val 0.510500\n",
      "Epoch 018 | train 0.486339 | val 0.509733\n",
      "Epoch 019 | train 0.475700 | val 0.511401\n",
      "Epoch 020 | train 0.465852 | val 0.515699\n",
      "Epoch 021 | train 0.456983 | val 0.522512\n",
      "Epoch 022 | train 0.449124 | val 0.531282\n",
      "Epoch 023 | train 0.442082 | val 0.540992\n",
      "Epoch 024 | train 0.435442 | val 0.550344\n",
      "Epoch 025 | train 0.428654 | val 0.558118\n",
      "Epoch 026 | train 0.421196 | val 0.563521\n",
      "Epoch 027 | train 0.412728 | val 0.566334\n",
      "Epoch 028 | train 0.403143 | val 0.566834\n",
      "Epoch 029 | train 0.392535 | val 0.565614\n",
      "Epoch 030 | train 0.381101 | val 0.563398\n",
      "Epoch 031 | train 0.369068 | val 0.560900\n",
      "Epoch 032 | train 0.356625 | val 0.558747\n",
      "Epoch 033 | train 0.343896 | val 0.557434\n",
      "Epoch 034 | train 0.330940 | val 0.557324\n",
      "Epoch 035 | train 0.317768 | val 0.558664\n",
      "Epoch 036 | train 0.304363 | val 0.561619\n",
      "Epoch 037 | train 0.290712 | val 0.566305\n",
      "Epoch 038 | train 0.276826 | val 0.572816\n",
      "Early stopping at epoch 038 (best val 0.509733)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7998, macro_AUPRC_optthr=0.7540, macro_Brier_optthr=0.1856, macro_ACC_optthr=0.6615, macro_F1_optthr=0.7256, macro_Precision_optthr=0.6021, macro_Recall_optthr=0.9127, macro_Sensitivity_optthr=0.9127, macro_Specificity_optthr=0.4198\n",
      "\n",
      "[CV fold 2/3] scenario=time\n",
      "[split] train | rows=  466 | unique_ids=  246\n",
      "[split] val   | rows=   51 | unique_ids=   48\n",
      "[split] test  | rows=  257 | unique_ids=  138\n",
      "Epoch 001 | train 0.638595 | val 0.657244\n",
      "Epoch 002 | train 0.627326 | val 0.651375\n",
      "Epoch 003 | train 0.614428 | val 0.642906\n",
      "Epoch 004 | train 0.600651 | val 0.633547\n",
      "Epoch 005 | train 0.584704 | val 0.623709\n",
      "Epoch 006 | train 0.567379 | val 0.613307\n",
      "Epoch 007 | train 0.549102 | val 0.602961\n",
      "Epoch 008 | train 0.528497 | val 0.594072\n",
      "Epoch 009 | train 0.508864 | val 0.588206\n",
      "Epoch 010 | train 0.490697 | val 0.586039\n",
      "Epoch 011 | train 0.480037 | val 0.590760\n",
      "Epoch 012 | train 0.473234 | val 0.595952\n",
      "Epoch 013 | train 0.468668 | val 0.603587\n",
      "Epoch 014 | train 0.462031 | val 0.604847\n",
      "Epoch 015 | train 0.453643 | val 0.605569\n",
      "Epoch 016 | train 0.444927 | val 0.604608\n",
      "Epoch 017 | train 0.435648 | val 0.602583\n",
      "Epoch 018 | train 0.427019 | val 0.606282\n",
      "Epoch 019 | train 0.418845 | val 0.609472\n",
      "Epoch 020 | train 0.410899 | val 0.613925\n",
      "Epoch 021 | train 0.402508 | val 0.621545\n",
      "Epoch 022 | train 0.393099 | val 0.630211\n",
      "Epoch 023 | train 0.382974 | val 0.639066\n",
      "Epoch 024 | train 0.372407 | val 0.649180\n",
      "Epoch 025 | train 0.361313 | val 0.661218\n",
      "Epoch 026 | train 0.349831 | val 0.677990\n",
      "Epoch 027 | train 0.338425 | val 0.699251\n",
      "Epoch 028 | train 0.325478 | val 0.723033\n",
      "Epoch 029 | train 0.311896 | val 0.749126\n",
      "Epoch 030 | train 0.298904 | val 0.783814\n",
      "Early stopping at epoch 030 (best val 0.586039)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8350, macro_AUPRC_optthr=0.8216, macro_Brier_optthr=0.1707, macro_ACC_optthr=0.7471, macro_F1_optthr=0.7826, macro_Precision_optthr=0.6882, macro_Recall_optthr=0.9070, macro_Sensitivity_optthr=0.9070, macro_Specificity_optthr=0.5859\n",
      "\n",
      "[CV fold 3/3] scenario=time\n",
      "[split] train | rows=  697 | unique_ids=  250\n",
      "[split] val   | rows=   77 | unique_ids=   67\n",
      "[split] test  | rows=  257 | unique_ids=  110\n",
      "Epoch 001 | train 0.674449 | val 0.658654\n",
      "Epoch 002 | train 0.653742 | val 0.640368\n",
      "Epoch 003 | train 0.632437 | val 0.619763\n",
      "Epoch 004 | train 0.607109 | val 0.597410\n",
      "Epoch 005 | train 0.577121 | val 0.575178\n",
      "Epoch 006 | train 0.546502 | val 0.558289\n",
      "Epoch 007 | train 0.514054 | val 0.557616\n",
      "Epoch 008 | train 0.497925 | val 0.576771\n",
      "Epoch 009 | train 0.487958 | val 0.595068\n",
      "Epoch 010 | train 0.478150 | val 0.598197\n",
      "Epoch 011 | train 0.466886 | val 0.598119\n",
      "Epoch 012 | train 0.456261 | val 0.598075\n",
      "Epoch 013 | train 0.446485 | val 0.601059\n",
      "Epoch 014 | train 0.437657 | val 0.608806\n",
      "Epoch 015 | train 0.428986 | val 0.620137\n",
      "Epoch 016 | train 0.417513 | val 0.635012\n",
      "Epoch 017 | train 0.407253 | val 0.655809\n",
      "Epoch 018 | train 0.394511 | val 0.686149\n",
      "Epoch 019 | train 0.382959 | val 0.720576\n",
      "Epoch 020 | train 0.370925 | val 0.751312\n",
      "Epoch 021 | train 0.360155 | val 0.775254\n",
      "Epoch 022 | train 0.348238 | val 0.800121\n",
      "Epoch 023 | train 0.336690 | val 0.822712\n",
      "Epoch 024 | train 0.325785 | val 0.850375\n",
      "Epoch 025 | train 0.313436 | val 0.879227\n",
      "Epoch 026 | train 0.300538 | val 0.902866\n",
      "Epoch 027 | train 0.287108 | val 0.931978\n",
      "Early stopping at epoch 027 (best val 0.557616)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8615, macro_AUPRC_optthr=0.9149, macro_Brier_optthr=0.1629, macro_ACC_optthr=0.7860, macro_F1_optthr=0.8232, macro_Precision_optthr=0.8312, macro_Recall_optthr=0.8153, macro_Sensitivity_optthr=0.8153, macro_Specificity_optthr=0.7400\n",
      "\n",
      "CV averages (Â±95% CI) @0.5:\n",
      "           macro_ACC: 0.7536  (95% CI 0.7170, 0.7902)\n",
      "           macro_AUC: 0.8321  (95% CI 0.7551, 0.9090)\n",
      "         macro_AUPRC: 0.8302  (95% CI 0.6295, 1.0309)\n",
      "         macro_Brier: 0.1731  (95% CI 0.1445, 0.2016)\n",
      "            macro_F1: 0.7577  (95% CI 0.6754, 0.8400)\n",
      "     macro_Precision: 0.7947  (95% CI 0.5928, 0.9966)\n",
      "        macro_Recall: 0.7292  (95% CI 0.5990, 0.8594)\n",
      "   macro_Sensitivity: 0.7292  (95% CI 0.5990, 0.8594)\n",
      "   macro_Specificity: 0.7882  (95% CI 0.6106, 0.9658)\n",
      "          task_1_ACC: 0.7536  (95% CI 0.7170, 0.7902)\n",
      "          task_1_AUC: 0.8321  (95% CI 0.7551, 0.9090)\n",
      "        task_1_AUPRC: 0.8302  (95% CI 0.6295, 1.0309)\n",
      "        task_1_Brier: 0.1731  (95% CI 0.1445, 0.2016)\n",
      "           task_1_F1: 0.7577  (95% CI 0.6754, 0.8400)\n",
      "    task_1_Precision: 0.7947  (95% CI 0.5928, 0.9966)\n",
      "       task_1_Recall: 0.7292  (95% CI 0.5990, 0.8594)\n",
      "  task_1_Sensitivity: 0.7292  (95% CI 0.5990, 0.8594)\n",
      "  task_1_Specificity: 0.7882  (95% CI 0.6106, 0.9658)\n",
      "\n",
      "CV averages (Â±95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.7315  (95% CI 0.5733, 0.8898)\n",
      "    macro_AUC_optthr: 0.8321  (95% CI 0.7551, 0.9090)\n",
      "  macro_AUPRC_optthr: 0.8302  (95% CI 0.6295, 1.0309)\n",
      "  macro_Brier_optthr: 0.1731  (95% CI 0.1445, 0.2016)\n",
      "     macro_F1_optthr: 0.7771  (95% CI 0.6553, 0.8989)\n",
      "macro_Precision_optthr: 0.7072  (95% CI 0.4197, 0.9946)\n",
      " macro_Recall_optthr: 0.8783  (95% CI 0.7425, 1.0141)\n",
      "macro_Sensitivity_optthr: 0.8783  (95% CI 0.7425, 1.0141)\n",
      "macro_Specificity_optthr: 0.5819  (95% CI 0.1842, 0.9797)\n",
      "   task_1_ACC_optthr: 0.7315  (95% CI 0.5733, 0.8898)\n",
      "   task_1_AUC_optthr: 0.8321  (95% CI 0.7551, 0.9090)\n",
      " task_1_AUPRC_optthr: 0.8302  (95% CI 0.6295, 1.0309)\n",
      " task_1_Brier_optthr: 0.1731  (95% CI 0.1445, 0.2016)\n",
      "    task_1_F1_optthr: 0.7771  (95% CI 0.6553, 0.8989)\n",
      "task_1_Precision_optthr: 0.7072  (95% CI 0.4197, 0.9946)\n",
      "task_1_Recall_optthr: 0.8783  (95% CI 0.7425, 1.0141)\n",
      "task_1_Sensitivity_optthr: 0.8783  (95% CI 0.7425, 1.0141)\n",
      "task_1_Specificity_optthr: 0.5819  (95% CI 0.1842, 0.9797)\n",
      "Arch params (used in all folds): {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0}\n",
      "Train params (used in all folds): {'lr': 0.0003, 'weight_decay': 3e-06, 'batch_size': 256, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 0.001, 'lambda_ridge': 0.003, 'lambda_orth_latent': 0.003, 'lambda_film_identity': 0.003, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"cv_only\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"both\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25babf89",
   "metadata": {},
   "source": [
    "### Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "051f906d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV fold 1/5] scenario=cases\n",
      "[split] train | rows=  742 | unique_ids=  201\n",
      "[split] val   | rows=   82 | unique_ids=   65\n",
      "[split] test  | rows=  207 | unique_ids=   51\n",
      "Epoch 001 | train 0.662666 | val 0.657978\n",
      "Epoch 002 | train 0.660247 | val 0.655375\n",
      "Epoch 003 | train 0.657050 | val 0.651790\n",
      "Epoch 004 | train 0.652186 | val 0.646179\n",
      "Epoch 005 | train 0.644552 | val 0.637915\n",
      "Epoch 006 | train 0.634448 | val 0.626129\n",
      "Epoch 007 | train 0.619174 | val 0.611002\n",
      "Epoch 008 | train 0.598446 | val 0.592786\n",
      "Epoch 009 | train 0.575270 | val 0.573702\n",
      "Epoch 010 | train 0.549078 | val 0.557326\n",
      "Epoch 011 | train 0.523538 | val 0.547020\n",
      "Epoch 012 | train 0.502395 | val 0.547514\n",
      "Epoch 013 | train 0.486301 | val 0.553952\n",
      "Epoch 014 | train 0.472944 | val 0.562537\n",
      "Epoch 015 | train 0.461520 | val 0.567270\n",
      "Epoch 016 | train 0.448859 | val 0.566741\n",
      "Epoch 017 | train 0.436466 | val 0.563500\n",
      "Epoch 018 | train 0.423948 | val 0.566173\n",
      "Epoch 019 | train 0.413287 | val 0.566165\n",
      "Epoch 020 | train 0.402715 | val 0.564648\n",
      "Epoch 021 | train 0.392523 | val 0.572284\n",
      "Epoch 022 | train 0.382623 | val 0.579868\n",
      "Epoch 023 | train 0.372968 | val 0.584773\n",
      "Epoch 024 | train 0.362388 | val 0.588883\n",
      "Epoch 025 | train 0.351706 | val 0.598656\n",
      "Epoch 026 | train 0.340408 | val 0.611052\n",
      "Epoch 027 | train 0.328888 | val 0.630906\n",
      "Epoch 028 | train 0.316234 | val 0.643488\n",
      "Epoch 029 | train 0.303152 | val 0.654654\n",
      "Epoch 030 | train 0.289429 | val 0.673298\n",
      "Epoch 031 | train 0.275942 | val 0.702073\n",
      "Early stopping at epoch 031 (best val 0.547020)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7058, macro_AUPRC_optthr=0.7412, macro_Brier_optthr=0.2178, macro_ACC_optthr=0.6377, macro_F1_optthr=0.6753, macro_Precision_optthr=0.7091, macro_Recall_optthr=0.6446, macro_Sensitivity_optthr=0.6446, macro_Specificity_optthr=0.6279\n",
      "\n",
      "[CV fold 2/5] scenario=cases\n",
      "[split] train | rows=  743 | unique_ids=  198\n",
      "[split] val   | rows=   82 | unique_ids=   63\n",
      "[split] test  | rows=  206 | unique_ids=   51\n",
      "Epoch 001 | train 0.652404 | val 0.642151\n",
      "Epoch 002 | train 0.649885 | val 0.637594\n",
      "Epoch 003 | train 0.646443 | val 0.631975\n",
      "Epoch 004 | train 0.641325 | val 0.623264\n",
      "Epoch 005 | train 0.633422 | val 0.610388\n",
      "Epoch 006 | train 0.622047 | val 0.592617\n",
      "Epoch 007 | train 0.606690 | val 0.570729\n",
      "Epoch 008 | train 0.587089 | val 0.546340\n",
      "Epoch 009 | train 0.563999 | val 0.523318\n",
      "Epoch 010 | train 0.539331 | val 0.505854\n",
      "Epoch 011 | train 0.516051 | val 0.498847\n",
      "Epoch 012 | train 0.497462 | val 0.502068\n",
      "Epoch 013 | train 0.484153 | val 0.511867\n",
      "Epoch 014 | train 0.475875 | val 0.517773\n",
      "Epoch 015 | train 0.467560 | val 0.516283\n",
      "Epoch 016 | train 0.456529 | val 0.507935\n",
      "Epoch 017 | train 0.444365 | val 0.497672\n",
      "Epoch 018 | train 0.433685 | val 0.491885\n",
      "Epoch 019 | train 0.424432 | val 0.494590\n",
      "Epoch 020 | train 0.414414 | val 0.499300\n",
      "Epoch 021 | train 0.405480 | val 0.499785\n",
      "Epoch 022 | train 0.396207 | val 0.505571\n",
      "Epoch 023 | train 0.386859 | val 0.510146\n",
      "Epoch 024 | train 0.376809 | val 0.508879\n",
      "Epoch 025 | train 0.367320 | val 0.515718\n",
      "Epoch 026 | train 0.356902 | val 0.530581\n",
      "Epoch 027 | train 0.345144 | val 0.537838\n",
      "Epoch 028 | train 0.332981 | val 0.546188\n",
      "Epoch 029 | train 0.320567 | val 0.556765\n",
      "Epoch 030 | train 0.306678 | val 0.563678\n",
      "Epoch 031 | train 0.292653 | val 0.573543\n",
      "Epoch 032 | train 0.276418 | val 0.582311\n",
      "Epoch 033 | train 0.260547 | val 0.608962\n",
      "Epoch 034 | train 0.242572 | val 0.625357\n",
      "Epoch 035 | train 0.225479 | val 0.631602\n",
      "Epoch 036 | train 0.206091 | val 0.681878\n",
      "Epoch 037 | train 0.186401 | val 0.702760\n",
      "Epoch 038 | train 0.167789 | val 0.744061\n",
      "Early stopping at epoch 038 (best val 0.491885)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.6502, macro_AUPRC_optthr=0.7091, macro_Brier_optthr=0.2546, macro_ACC_optthr=0.5680, macro_F1_optthr=0.6009, macro_Precision_optthr=0.5982, macro_Recall_optthr=0.6036, macro_Sensitivity_optthr=0.6036, macro_Specificity_optthr=0.5263\n",
      "\n",
      "[CV fold 3/5] scenario=cases\n",
      "[split] train | rows=  743 | unique_ids=  202\n",
      "[split] val   | rows=   82 | unique_ids=   62\n",
      "[split] test  | rows=  206 | unique_ids=   51\n",
      "Epoch 001 | train 0.630105 | val 0.625343\n",
      "Epoch 002 | train 0.627933 | val 0.623448\n",
      "Epoch 003 | train 0.625254 | val 0.620568\n",
      "Epoch 004 | train 0.620552 | val 0.616437\n",
      "Epoch 005 | train 0.613972 | val 0.610152\n",
      "Epoch 006 | train 0.604533 | val 0.601299\n",
      "Epoch 007 | train 0.591027 | val 0.589698\n",
      "Epoch 008 | train 0.573501 | val 0.576015\n",
      "Epoch 009 | train 0.554398 | val 0.561881\n",
      "Epoch 010 | train 0.531612 | val 0.550099\n",
      "Epoch 011 | train 0.509744 | val 0.542602\n",
      "Epoch 012 | train 0.489868 | val 0.540831\n",
      "Epoch 013 | train 0.475105 | val 0.541826\n",
      "Epoch 014 | train 0.462356 | val 0.540523\n",
      "Epoch 015 | train 0.449600 | val 0.537151\n",
      "Epoch 016 | train 0.436370 | val 0.529970\n",
      "Epoch 017 | train 0.423751 | val 0.525513\n",
      "Epoch 018 | train 0.412369 | val 0.522594\n",
      "Epoch 019 | train 0.401332 | val 0.517851\n",
      "Epoch 020 | train 0.391460 | val 0.513729\n",
      "Epoch 021 | train 0.381211 | val 0.509051\n",
      "Epoch 022 | train 0.371289 | val 0.508077\n",
      "Epoch 023 | train 0.361276 | val 0.502931\n",
      "Epoch 024 | train 0.350095 | val 0.500732\n",
      "Epoch 025 | train 0.339188 | val 0.495996\n",
      "Epoch 026 | train 0.327847 | val 0.493703\n",
      "Epoch 027 | train 0.314591 | val 0.492695\n",
      "Epoch 028 | train 0.302989 | val 0.491032\n",
      "Epoch 029 | train 0.288724 | val 0.488003\n",
      "Epoch 030 | train 0.273728 | val 0.488054\n",
      "Epoch 031 | train 0.259262 | val 0.487951\n",
      "Epoch 032 | train 0.241336 | val 0.487988\n",
      "Epoch 033 | train 0.223889 | val 0.487561\n",
      "Epoch 034 | train 0.203433 | val 0.492148\n",
      "Epoch 035 | train 0.184376 | val 0.499857\n",
      "Epoch 036 | train 0.163180 | val 0.509679\n",
      "Epoch 037 | train 0.141670 | val 0.527420\n",
      "Epoch 038 | train 0.120224 | val 0.552638\n",
      "Epoch 039 | train 0.096817 | val 0.575409\n",
      "Epoch 040 | train 0.076493 | val 0.603541\n",
      "Epoch 041 | train 0.059415 | val 0.630840\n",
      "Epoch 042 | train 0.046047 | val 0.661362\n",
      "Epoch 043 | train 0.034514 | val 0.690223\n",
      "Epoch 044 | train 0.027432 | val 0.716572\n",
      "Epoch 045 | train 0.023228 | val 0.746040\n",
      "Epoch 046 | train 0.018942 | val 0.776050\n",
      "Epoch 047 | train 0.014982 | val 0.804659\n",
      "Epoch 048 | train 0.012804 | val 0.832185\n",
      "Epoch 049 | train 0.010698 | val 0.854321\n",
      "Epoch 050 | train 0.008035 | val 0.871866\n",
      "Epoch 051 | train 0.006592 | val 0.877728\n",
      "Epoch 052 | train 0.003417 | val 0.888043\n",
      "Epoch 053 | train 0.002992 | val 0.901582\n",
      "Early stopping at epoch 053 (best val 0.487561)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.6976, macro_AUPRC_optthr=0.7368, macro_Brier_optthr=0.2803, macro_ACC_optthr=0.6262, macro_F1_optthr=0.6723, macro_Precision_optthr=0.6031, macro_Recall_optthr=0.7596, macro_Sensitivity_optthr=0.7596, macro_Specificity_optthr=0.4902\n",
      "\n",
      "[CV fold 4/5] scenario=cases\n",
      "[split] train | rows=  743 | unique_ids=  205\n",
      "[split] val   | rows=   82 | unique_ids=   66\n",
      "[split] test  | rows=  206 | unique_ids=   52\n",
      "Epoch 001 | train 0.661628 | val 0.656051\n",
      "Epoch 002 | train 0.659235 | val 0.653412\n",
      "Epoch 003 | train 0.656275 | val 0.649667\n",
      "Epoch 004 | train 0.651664 | val 0.644468\n",
      "Epoch 005 | train 0.645242 | val 0.637246\n",
      "Epoch 006 | train 0.635818 | val 0.627669\n",
      "Epoch 007 | train 0.622231 | val 0.616400\n",
      "Epoch 008 | train 0.605665 | val 0.603854\n",
      "Epoch 009 | train 0.586387 | val 0.592020\n",
      "Epoch 010 | train 0.564844 | val 0.583817\n",
      "Epoch 011 | train 0.540781 | val 0.581872\n",
      "Epoch 012 | train 0.521616 | val 0.588721\n",
      "Epoch 013 | train 0.504949 | val 0.601677\n",
      "Epoch 014 | train 0.491540 | val 0.610784\n",
      "Epoch 015 | train 0.479869 | val 0.612574\n",
      "Epoch 016 | train 0.466021 | val 0.606370\n",
      "Epoch 017 | train 0.452939 | val 0.599033\n",
      "Epoch 018 | train 0.439034 | val 0.596904\n",
      "Epoch 019 | train 0.426619 | val 0.600442\n",
      "Epoch 020 | train 0.414256 | val 0.604271\n",
      "Epoch 021 | train 0.405202 | val 0.597875\n",
      "Epoch 022 | train 0.393507 | val 0.598909\n",
      "Epoch 023 | train 0.383440 | val 0.605178\n",
      "Epoch 024 | train 0.372299 | val 0.611305\n",
      "Epoch 025 | train 0.361509 | val 0.613794\n",
      "Epoch 026 | train 0.351052 | val 0.611397\n",
      "Epoch 027 | train 0.338755 | val 0.631712\n",
      "Epoch 028 | train 0.326538 | val 0.633868\n",
      "Epoch 029 | train 0.313847 | val 0.635097\n",
      "Epoch 030 | train 0.302300 | val 0.625073\n",
      "Epoch 031 | train 0.288437 | val 0.650419\n",
      "Early stopping at epoch 031 (best val 0.581872)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8249, macro_AUPRC_optthr=0.8695, macro_Brier_optthr=0.1781, macro_ACC_optthr=0.6650, macro_F1_optthr=0.7612, macro_Precision_optthr=0.6433, macro_Recall_optthr=0.9322, macro_Sensitivity_optthr=0.9322, macro_Specificity_optthr=0.3068\n",
      "\n",
      "[CV fold 5/5] scenario=cases\n",
      "[split] train | rows=  743 | unique_ids=  201\n",
      "[split] val   | rows=   82 | unique_ids=   66\n",
      "[split] test  | rows=  206 | unique_ids=   52\n",
      "Epoch 001 | train 0.622797 | val 0.621966\n",
      "Epoch 002 | train 0.620647 | val 0.620052\n",
      "Epoch 003 | train 0.617810 | val 0.617265\n",
      "Epoch 004 | train 0.613572 | val 0.613080\n",
      "Epoch 005 | train 0.607029 | val 0.607007\n",
      "Epoch 006 | train 0.597412 | val 0.598723\n",
      "Epoch 007 | train 0.583976 | val 0.588294\n",
      "Epoch 008 | train 0.566306 | val 0.576868\n",
      "Epoch 009 | train 0.545697 | val 0.565714\n",
      "Epoch 010 | train 0.521852 | val 0.557116\n",
      "Epoch 011 | train 0.497600 | val 0.553688\n",
      "Epoch 012 | train 0.477999 | val 0.558814\n",
      "Epoch 013 | train 0.460035 | val 0.568987\n",
      "Epoch 014 | train 0.447296 | val 0.583842\n",
      "Epoch 015 | train 0.434059 | val 0.597883\n",
      "Epoch 016 | train 0.421347 | val 0.610829\n",
      "Epoch 017 | train 0.410114 | val 0.618806\n",
      "Epoch 018 | train 0.399269 | val 0.613164\n",
      "Epoch 019 | train 0.388991 | val 0.607360\n",
      "Epoch 020 | train 0.379592 | val 0.605932\n",
      "Epoch 021 | train 0.371988 | val 0.605588\n",
      "Epoch 022 | train 0.362309 | val 0.610868\n",
      "Epoch 023 | train 0.353223 | val 0.615332\n",
      "Epoch 024 | train 0.343790 | val 0.621492\n",
      "Epoch 025 | train 0.333709 | val 0.627661\n",
      "Epoch 026 | train 0.322056 | val 0.638110\n",
      "Epoch 027 | train 0.310418 | val 0.653319\n",
      "Epoch 028 | train 0.297807 | val 0.663768\n",
      "Epoch 029 | train 0.283792 | val 0.673915\n",
      "Epoch 030 | train 0.269433 | val 0.686931\n",
      "Epoch 031 | train 0.253989 | val 0.702484\n",
      "Early stopping at epoch 031 (best val 0.553688)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7202, macro_AUPRC_optthr=0.7352, macro_Brier_optthr=0.2107, macro_ACC_optthr=0.6408, macro_F1_optthr=0.6754, macro_Precision_optthr=0.6111, macro_Recall_optthr=0.7549, macro_Sensitivity_optthr=0.7549, macro_Specificity_optthr=0.5288\n",
      "\n",
      "CV averages (Â±95% CI) @0.5:\n",
      "           macro_ACC: 0.6596  (95% CI 0.5770, 0.7421)\n",
      "           macro_AUC: 0.7197  (95% CI 0.6398, 0.7997)\n",
      "         macro_AUPRC: 0.7584  (95% CI 0.6797, 0.8371)\n",
      "         macro_Brier: 0.2283  (95% CI 0.1789, 0.2777)\n",
      "            macro_F1: 0.6627  (95% CI 0.5666, 0.7588)\n",
      "     macro_Precision: 0.7109  (95% CI 0.6062, 0.8156)\n",
      "        macro_Recall: 0.6319  (95% CI 0.4835, 0.7803)\n",
      "   macro_Sensitivity: 0.6319  (95% CI 0.4835, 0.7803)\n",
      "   macro_Specificity: 0.6954  (95% CI 0.5535, 0.8373)\n",
      "          task_1_ACC: 0.6596  (95% CI 0.5770, 0.7421)\n",
      "          task_1_AUC: 0.7197  (95% CI 0.6398, 0.7997)\n",
      "        task_1_AUPRC: 0.7584  (95% CI 0.6797, 0.8371)\n",
      "        task_1_Brier: 0.2283  (95% CI 0.1789, 0.2777)\n",
      "           task_1_F1: 0.6627  (95% CI 0.5666, 0.7588)\n",
      "    task_1_Precision: 0.7109  (95% CI 0.6062, 0.8156)\n",
      "       task_1_Recall: 0.6319  (95% CI 0.4835, 0.7803)\n",
      "  task_1_Sensitivity: 0.6319  (95% CI 0.4835, 0.7803)\n",
      "  task_1_Specificity: 0.6954  (95% CI 0.5535, 0.8373)\n",
      "\n",
      "CV averages (Â±95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.6275  (95% CI 0.5826, 0.6725)\n",
      "    macro_AUC_optthr: 0.7197  (95% CI 0.6398, 0.7997)\n",
      "  macro_AUPRC_optthr: 0.7584  (95% CI 0.6797, 0.8371)\n",
      "  macro_Brier_optthr: 0.2283  (95% CI 0.1789, 0.2777)\n",
      "     macro_F1_optthr: 0.6770  (95% CI 0.6065, 0.7476)\n",
      "macro_Precision_optthr: 0.6329  (95% CI 0.5758, 0.6901)\n",
      " macro_Recall_optthr: 0.7390  (95% CI 0.5804, 0.8976)\n",
      "macro_Sensitivity_optthr: 0.7390  (95% CI 0.5804, 0.8976)\n",
      "macro_Specificity_optthr: 0.4960  (95% CI 0.3501, 0.6419)\n",
      "   task_1_ACC_optthr: 0.6275  (95% CI 0.5826, 0.6725)\n",
      "   task_1_AUC_optthr: 0.7197  (95% CI 0.6398, 0.7997)\n",
      " task_1_AUPRC_optthr: 0.7584  (95% CI 0.6797, 0.8371)\n",
      " task_1_Brier_optthr: 0.2283  (95% CI 0.1789, 0.2777)\n",
      "    task_1_F1_optthr: 0.6770  (95% CI 0.6065, 0.7476)\n",
      "task_1_Precision_optthr: 0.6329  (95% CI 0.5758, 0.6901)\n",
      "task_1_Recall_optthr: 0.7390  (95% CI 0.5804, 0.8976)\n",
      "task_1_Sensitivity_optthr: 0.7390  (95% CI 0.5804, 0.8976)\n",
      "task_1_Specificity_optthr: 0.4960  (95% CI 0.3501, 0.6419)\n",
      "Arch params (used in all folds): {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0, 'use_attention': False, 'use_random': False}\n",
      "Train params (used in all folds): {'lr': 0.0003, 'weight_decay': 3e-06, 'batch_size': 256, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 0.001, 'lambda_ridge': 0.003, 'lambda_orth_latent': 0.003, 'lambda_film_identity': 0.003, 'lambda_kan': 0, 'random_state': 42}\n",
      "\n",
      "[CV fold 1/5] scenario=time\n",
      "[split] train | rows=  159 | unique_ids=  159\n",
      "[split] val   | rows=   17 | unique_ids=   17\n",
      "[split] test  | rows=  171 | unique_ids=  142\n",
      "Epoch 001 | train 0.662580 | val 0.664853\n",
      "Epoch 002 | train 0.661693 | val 0.664018\n",
      "Epoch 003 | train 0.660766 | val 0.663157\n",
      "Epoch 004 | train 0.659754 | val 0.662245\n",
      "Epoch 005 | train 0.658613 | val 0.661253\n",
      "Epoch 006 | train 0.657296 | val 0.660160\n",
      "Epoch 007 | train 0.655762 | val 0.658943\n",
      "Epoch 008 | train 0.653972 | val 0.657583\n",
      "Epoch 009 | train 0.651886 | val 0.656064\n",
      "Epoch 010 | train 0.649464 | val 0.654372\n",
      "Epoch 011 | train 0.646663 | val 0.652495\n",
      "Epoch 012 | train 0.643440 | val 0.650421\n",
      "Epoch 013 | train 0.639750 | val 0.648143\n",
      "Epoch 014 | train 0.635549 | val 0.645658\n",
      "Epoch 015 | train 0.630793 | val 0.642965\n",
      "Epoch 016 | train 0.625439 | val 0.640070\n",
      "Epoch 017 | train 0.619450 | val 0.636987\n",
      "Epoch 018 | train 0.612796 | val 0.633737\n",
      "Epoch 019 | train 0.605453 | val 0.630352\n",
      "Epoch 020 | train 0.597411 | val 0.626877\n",
      "Epoch 021 | train 0.588667 | val 0.623363\n",
      "Epoch 022 | train 0.579233 | val 0.619876\n",
      "Epoch 023 | train 0.569129 | val 0.616491\n",
      "Epoch 024 | train 0.558388 | val 0.613299\n",
      "Epoch 025 | train 0.547054 | val 0.610409\n",
      "Epoch 026 | train 0.535184 | val 0.607958\n",
      "Epoch 027 | train 0.522847 | val 0.606098\n",
      "Epoch 028 | train 0.510116 | val 0.604996\n",
      "Epoch 029 | train 0.497062 | val 0.604822\n",
      "Epoch 030 | train 0.483751 | val 0.605744\n",
      "Epoch 031 | train 0.470233 | val 0.607920\n",
      "Epoch 032 | train 0.456538 | val 0.611491\n",
      "Epoch 033 | train 0.442662 | val 0.616585\n",
      "Epoch 034 | train 0.428575 | val 0.623311\n",
      "Epoch 035 | train 0.414231 | val 0.631764\n",
      "Epoch 036 | train 0.399579 | val 0.642040\n",
      "Epoch 037 | train 0.384578 | val 0.654246\n",
      "Epoch 038 | train 0.369203 | val 0.668521\n",
      "Epoch 039 | train 0.353444 | val 0.685028\n",
      "Epoch 040 | train 0.337293 | val 0.703944\n",
      "Epoch 041 | train 0.320737 | val 0.725433\n",
      "Epoch 042 | train 0.303751 | val 0.749622\n",
      "Epoch 043 | train 0.286289 | val 0.776588\n",
      "Epoch 044 | train 0.268300 | val 0.806369\n",
      "Epoch 045 | train 0.249776 | val 0.838982\n",
      "Epoch 046 | train 0.230832 | val 0.874432\n",
      "Epoch 047 | train 0.211705 | val 0.912676\n",
      "Epoch 048 | train 0.192639 | val 0.953575\n",
      "Epoch 049 | train 0.173969 | val 0.996885\n",
      "Early stopping at epoch 049 (best val 0.604822)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7301, macro_AUPRC_optthr=0.7496, macro_Brier_optthr=0.2079, macro_ACC_optthr=0.6550, macro_F1_optthr=0.7094, macro_Precision_optthr=0.6729, macro_Recall_optthr=0.7500, macro_Sensitivity_optthr=0.7500, macro_Specificity_optthr=0.5333\n",
      "\n",
      "[CV fold 2/5] scenario=time\n",
      "[split] train | rows=  313 | unique_ids=  242\n",
      "[split] val   | rows=   34 | unique_ids=   33\n",
      "[split] test  | rows=  171 | unique_ids=  132\n",
      "Epoch 001 | train 0.646299 | val 0.640797\n",
      "Epoch 002 | train 0.644736 | val 0.640539\n",
      "Epoch 003 | train 0.643276 | val 0.639890\n",
      "Epoch 004 | train 0.641452 | val 0.638893\n",
      "Epoch 005 | train 0.639109 | val 0.637544\n",
      "Epoch 006 | train 0.635983 | val 0.635560\n",
      "Epoch 007 | train 0.632155 | val 0.632856\n",
      "Epoch 008 | train 0.627043 | val 0.629434\n",
      "Epoch 009 | train 0.620848 | val 0.625106\n",
      "Epoch 010 | train 0.613277 | val 0.620028\n",
      "Epoch 011 | train 0.603725 | val 0.614102\n",
      "Epoch 012 | train 0.593191 | val 0.607185\n",
      "Epoch 013 | train 0.582007 | val 0.600553\n",
      "Epoch 014 | train 0.569565 | val 0.595773\n",
      "Epoch 015 | train 0.555037 | val 0.592917\n",
      "Epoch 016 | train 0.541830 | val 0.592154\n",
      "Epoch 017 | train 0.528200 | val 0.593548\n",
      "Epoch 018 | train 0.514374 | val 0.597175\n",
      "Epoch 019 | train 0.501282 | val 0.604034\n",
      "Epoch 020 | train 0.488601 | val 0.612401\n",
      "Epoch 021 | train 0.475486 | val 0.623822\n",
      "Epoch 022 | train 0.462714 | val 0.639257\n",
      "Epoch 023 | train 0.449349 | val 0.657003\n",
      "Epoch 024 | train 0.435474 | val 0.676807\n",
      "Epoch 025 | train 0.421947 | val 0.701003\n",
      "Epoch 026 | train 0.407398 | val 0.726629\n",
      "Epoch 027 | train 0.392395 | val 0.751553\n",
      "Epoch 028 | train 0.377466 | val 0.776437\n",
      "Epoch 029 | train 0.361593 | val 0.805542\n",
      "Epoch 030 | train 0.344936 | val 0.844305\n",
      "Epoch 031 | train 0.329294 | val 0.887300\n",
      "Epoch 032 | train 0.314948 | val 0.913354\n",
      "Epoch 033 | train 0.299406 | val 0.923053\n",
      "Epoch 034 | train 0.285190 | val 0.927678\n",
      "Epoch 035 | train 0.273051 | val 0.942375\n",
      "Epoch 036 | train 0.263520 | val 0.985589\n",
      "Early stopping at epoch 036 (best val 0.592154)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7318, macro_AUPRC_optthr=0.7244, macro_Brier_optthr=0.2082, macro_ACC_optthr=0.6784, macro_F1_optthr=0.6995, macro_Precision_optthr=0.6400, macro_Recall_optthr=0.7711, macro_Sensitivity_optthr=0.7711, macro_Specificity_optthr=0.5909\n",
      "\n",
      "[CV fold 3/5] scenario=time\n",
      "[split] train | rows=  467 | unique_ids=  252\n",
      "[split] val   | rows=   51 | unique_ids=   47\n",
      "[split] test  | rows=  171 | unique_ids=  119\n",
      "Epoch 001 | train 0.676470 | val 0.672296\n",
      "Epoch 002 | train 0.674953 | val 0.670739\n",
      "Epoch 003 | train 0.673435 | val 0.668401\n",
      "Epoch 004 | train 0.671198 | val 0.665879\n",
      "Epoch 005 | train 0.668708 | val 0.662400\n",
      "Epoch 006 | train 0.665335 | val 0.658079\n",
      "Epoch 007 | train 0.660952 | val 0.652517\n",
      "Epoch 008 | train 0.655405 | val 0.645623\n",
      "Epoch 009 | train 0.648187 | val 0.637348\n",
      "Epoch 010 | train 0.639195 | val 0.627572\n",
      "Epoch 011 | train 0.628178 | val 0.616908\n",
      "Epoch 012 | train 0.616453 | val 0.605738\n",
      "Epoch 013 | train 0.603116 | val 0.594773\n",
      "Epoch 014 | train 0.587932 | val 0.585565\n",
      "Epoch 015 | train 0.571786 | val 0.578180\n",
      "Epoch 016 | train 0.558493 | val 0.573673\n",
      "Epoch 017 | train 0.542976 | val 0.573695\n",
      "Epoch 018 | train 0.530603 | val 0.576790\n",
      "Epoch 019 | train 0.517382 | val 0.581751\n",
      "Epoch 020 | train 0.507272 | val 0.586158\n",
      "Epoch 021 | train 0.496024 | val 0.590512\n",
      "Epoch 022 | train 0.483217 | val 0.591519\n",
      "Epoch 023 | train 0.470750 | val 0.592418\n",
      "Epoch 024 | train 0.456542 | val 0.591595\n",
      "Epoch 025 | train 0.443893 | val 0.592654\n",
      "Epoch 026 | train 0.431276 | val 0.594491\n",
      "Epoch 027 | train 0.417905 | val 0.599021\n",
      "Epoch 028 | train 0.406012 | val 0.611410\n",
      "Epoch 029 | train 0.393532 | val 0.617906\n",
      "Epoch 030 | train 0.380883 | val 0.619636\n",
      "Epoch 031 | train 0.367747 | val 0.616917\n",
      "Epoch 032 | train 0.355079 | val 0.616489\n",
      "Epoch 033 | train 0.341781 | val 0.623710\n",
      "Epoch 034 | train 0.328642 | val 0.629532\n",
      "Epoch 035 | train 0.314175 | val 0.639645\n",
      "Epoch 036 | train 0.299921 | val 0.649098\n",
      "Early stopping at epoch 036 (best val 0.573673)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7752, macro_AUPRC_optthr=0.7626, macro_Brier_optthr=0.1993, macro_ACC_optthr=0.7018, macro_F1_optthr=0.6909, macro_Precision_optthr=0.7215, macro_Recall_optthr=0.6628, macro_Sensitivity_optthr=0.6628, macro_Specificity_optthr=0.7412\n",
      "\n",
      "[CV fold 4/5] scenario=time\n",
      "[split] train | rows=  621 | unique_ids=  252\n",
      "[split] val   | rows=   68 | unique_ids=   61\n",
      "[split] test  | rows=  171 | unique_ids=  110\n",
      "Epoch 001 | train 0.676190 | val 0.673211\n",
      "Epoch 002 | train 0.674064 | val 0.671450\n",
      "Epoch 003 | train 0.671114 | val 0.669213\n",
      "Epoch 004 | train 0.667016 | val 0.665634\n",
      "Epoch 005 | train 0.660972 | val 0.660375\n",
      "Epoch 006 | train 0.651378 | val 0.652939\n",
      "Epoch 007 | train 0.639000 | val 0.643316\n",
      "Epoch 008 | train 0.623452 | val 0.632472\n",
      "Epoch 009 | train 0.603310 | val 0.622555\n",
      "Epoch 010 | train 0.582620 | val 0.615786\n",
      "Epoch 011 | train 0.561006 | val 0.615657\n",
      "Epoch 012 | train 0.540064 | val 0.624748\n",
      "Epoch 013 | train 0.523981 | val 0.643250\n",
      "Epoch 014 | train 0.509309 | val 0.663987\n",
      "Epoch 015 | train 0.499625 | val 0.680558\n",
      "Epoch 016 | train 0.486505 | val 0.682049\n",
      "Epoch 017 | train 0.473482 | val 0.681112\n",
      "Epoch 018 | train 0.462354 | val 0.682897\n",
      "Epoch 019 | train 0.450530 | val 0.694749\n",
      "Epoch 020 | train 0.437250 | val 0.712762\n",
      "Epoch 021 | train 0.424883 | val 0.731664\n",
      "Epoch 022 | train 0.414132 | val 0.741843\n",
      "Epoch 023 | train 0.400582 | val 0.746425\n",
      "Epoch 024 | train 0.387865 | val 0.754764\n",
      "Epoch 025 | train 0.373484 | val 0.770886\n",
      "Epoch 026 | train 0.359683 | val 0.797087\n",
      "Epoch 027 | train 0.345928 | val 0.811236\n",
      "Epoch 028 | train 0.329922 | val 0.806379\n",
      "Epoch 029 | train 0.314903 | val 0.825269\n",
      "Epoch 030 | train 0.300127 | val 0.862224\n",
      "Epoch 031 | train 0.282510 | val 0.912050\n",
      "Early stopping at epoch 031 (best val 0.615657)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8202, macro_AUPRC_optthr=0.8731, macro_Brier_optthr=0.1805, macro_ACC_optthr=0.7661, macro_F1_optthr=0.7938, macro_Precision_optthr=0.8021, macro_Recall_optthr=0.7857, macro_Sensitivity_optthr=0.7857, macro_Specificity_optthr=0.7397\n",
      "\n",
      "[CV fold 5/5] scenario=time\n",
      "[split] train | rows=  774 | unique_ids=  250\n",
      "[split] val   | rows=   86 | unique_ids=   71\n",
      "[split] test  | rows=  171 | unique_ids=   82\n",
      "Epoch 001 | train 0.645546 | val 0.648667\n",
      "Epoch 002 | train 0.641927 | val 0.645689\n",
      "Epoch 003 | train 0.637618 | val 0.642270\n",
      "Epoch 004 | train 0.630796 | val 0.636472\n",
      "Epoch 005 | train 0.621713 | val 0.628183\n",
      "Epoch 006 | train 0.610371 | val 0.617765\n",
      "Epoch 007 | train 0.597119 | val 0.606235\n",
      "Epoch 008 | train 0.582995 | val 0.593137\n",
      "Epoch 009 | train 0.565202 | val 0.580086\n",
      "Epoch 010 | train 0.547176 | val 0.570032\n",
      "Epoch 011 | train 0.531522 | val 0.564918\n",
      "Epoch 012 | train 0.523780 | val 0.566380\n",
      "Epoch 013 | train 0.519511 | val 0.570804\n",
      "Epoch 014 | train 0.514687 | val 0.575458\n",
      "Epoch 015 | train 0.508921 | val 0.576692\n",
      "Epoch 016 | train 0.504304 | val 0.576991\n",
      "Epoch 017 | train 0.499432 | val 0.577642\n",
      "Epoch 018 | train 0.494421 | val 0.578809\n",
      "Epoch 019 | train 0.490184 | val 0.579242\n",
      "Epoch 020 | train 0.483676 | val 0.578110\n",
      "Epoch 021 | train 0.475269 | val 0.576008\n",
      "Epoch 022 | train 0.464993 | val 0.574185\n",
      "Epoch 023 | train 0.458230 | val 0.576657\n",
      "Epoch 024 | train 0.455787 | val 0.577677\n",
      "Epoch 025 | train 0.449258 | val 0.583400\n",
      "Epoch 026 | train 0.443275 | val 0.591783\n",
      "Epoch 027 | train 0.438562 | val 0.587630\n",
      "Epoch 028 | train 0.435078 | val 0.586872\n",
      "Epoch 029 | train 0.431986 | val 0.585707\n",
      "Epoch 030 | train 0.425702 | val 0.587627\n",
      "Epoch 031 | train 0.420272 | val 0.576562\n",
      "Early stopping at epoch 031 (best val 0.564918)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7933, macro_AUPRC_optthr=0.8703, macro_Brier_optthr=0.1836, macro_ACC_optthr=0.7018, macro_F1_optthr=0.7811, macro_Precision_optthr=0.6947, macro_Recall_optthr=0.8922, macro_Sensitivity_optthr=0.8922, macro_Specificity_optthr=0.4203\n",
      "\n",
      "CV averages (Â±95% CI) @0.5:\n",
      "           macro_ACC: 0.6819  (95% CI 0.6432, 0.7205)\n",
      "           macro_AUC: 0.7701  (95% CI 0.7215, 0.8188)\n",
      "         macro_AUPRC: 0.7960  (95% CI 0.7085, 0.8835)\n",
      "         macro_Brier: 0.1959  (95% CI 0.1795, 0.2123)\n",
      "            macro_F1: 0.6789  (95% CI 0.6416, 0.7162)\n",
      "     macro_Precision: 0.7527  (95% CI 0.6787, 0.8268)\n",
      "        macro_Recall: 0.6206  (95% CI 0.5766, 0.6645)\n",
      "   macro_Sensitivity: 0.6206  (95% CI 0.5766, 0.6645)\n",
      "   macro_Specificity: 0.7523  (95% CI 0.6490, 0.8556)\n",
      "          task_1_ACC: 0.6819  (95% CI 0.6432, 0.7205)\n",
      "          task_1_AUC: 0.7701  (95% CI 0.7215, 0.8188)\n",
      "        task_1_AUPRC: 0.7960  (95% CI 0.7085, 0.8835)\n",
      "        task_1_Brier: 0.1959  (95% CI 0.1795, 0.2123)\n",
      "           task_1_F1: 0.6789  (95% CI 0.6416, 0.7162)\n",
      "    task_1_Precision: 0.7527  (95% CI 0.6787, 0.8268)\n",
      "       task_1_Recall: 0.6206  (95% CI 0.5766, 0.6645)\n",
      "  task_1_Sensitivity: 0.6206  (95% CI 0.5766, 0.6645)\n",
      "  task_1_Specificity: 0.7523  (95% CI 0.6490, 0.8556)\n",
      "\n",
      "CV averages (Â±95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.7006  (95% CI 0.6491, 0.7520)\n",
      "    macro_AUC_optthr: 0.7701  (95% CI 0.7215, 0.8188)\n",
      "  macro_AUPRC_optthr: 0.7960  (95% CI 0.7085, 0.8835)\n",
      "  macro_Brier_optthr: 0.1959  (95% CI 0.1795, 0.2123)\n",
      "     macro_F1_optthr: 0.7349  (95% CI 0.6746, 0.7953)\n",
      "macro_Precision_optthr: 0.7062  (95% CI 0.6301, 0.7824)\n",
      " macro_Recall_optthr: 0.7723  (95% CI 0.6703, 0.8744)\n",
      "macro_Sensitivity_optthr: 0.7723  (95% CI 0.6703, 0.8744)\n",
      "macro_Specificity_optthr: 0.6051  (95% CI 0.4338, 0.7764)\n",
      "   task_1_ACC_optthr: 0.7006  (95% CI 0.6491, 0.7520)\n",
      "   task_1_AUC_optthr: 0.7701  (95% CI 0.7215, 0.8188)\n",
      " task_1_AUPRC_optthr: 0.7960  (95% CI 0.7085, 0.8835)\n",
      " task_1_Brier_optthr: 0.1959  (95% CI 0.1795, 0.2123)\n",
      "    task_1_F1_optthr: 0.7349  (95% CI 0.6746, 0.7953)\n",
      "task_1_Precision_optthr: 0.7062  (95% CI 0.6301, 0.7824)\n",
      "task_1_Recall_optthr: 0.7723  (95% CI 0.6703, 0.8744)\n",
      "task_1_Sensitivity_optthr: 0.7723  (95% CI 0.6703, 0.8744)\n",
      "task_1_Specificity_optthr: 0.6051  (95% CI 0.4338, 0.7764)\n",
      "Arch params (used in all folds): {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0, 'use_attention': False, 'use_random': False}\n",
      "Train params (used in all folds): {'lr': 0.0003, 'weight_decay': 3e-06, 'batch_size': 256, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 0.001, 'lambda_ridge': 0.003, 'lambda_orth_latent': 0.003, 'lambda_film_identity': 0.003, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "arch_defaults = dict(d_fix_latent=256, d_rand_latent=256, n_kernels=8, dropout=0.0, use_attention = False, use_random = False)\n",
    "\n",
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"cv_only\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"both\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9138297",
   "metadata": {},
   "source": [
    "### Cases split CV with parameter search test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outer fold 1/3\n",
      "Outer fold macro (optthr): macro_AUC_optthr=0.7936, macro_AUPRC_optthr=0.8187, macro_Brier_optthr=0.1857, macro_ACC_optthr=0.6860, macro_F1_optthr=0.7465, macro_Precision_optthr=0.6570, macro_Recall_optthr=0.8641, macro_Sensitivity_optthr=0.8641, macro_Specificity_optthr=0.4813\n",
      "\n",
      "Outer fold 2/3\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"d_fix_latent\": [128],\n",
    "    \"d_rand_latent\": [128],\n",
    "    \"n_kernels\": [6],\n",
    "    \"dropout\": [0.1],\n",
    "\n",
    "    \"lr\": [1e-4, 3e-4],\n",
    "    \"weight_decay\": [1e-4, 1e-3],\n",
    "    \"batch_size\": [128, 64],\n",
    "    \"max_epochs\": [100],\n",
    "    \"patience\": [10],\n",
    "\n",
    "    \"lambda_mean0\": [1e-4, 1e-3],\n",
    "    \"lambda_ridge\": [1e-4, 3e-4],\n",
    "    \"lambda_orth_latent\": [1e-3, 1e-4],\n",
    "    \"lambda_film_identity\": [1e-4, 1e-3],\n",
    "    \"lambda_kan\": [0.0, 1e-4],\n",
    "}\n",
    "\n",
    "res_cases = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # used for BOTH X_fix and TC\n",
    "    X_fixed_and_random=X_fixed_and_random_np,           # used for Zrand\n",
    "    y=y_np,\n",
    "    y_lags=y_lags_np,\n",
    "    dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np,\n",
    "    time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,         # the dynamic-dims builder we just fixed\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"nested_cv\",                         # or \"cv_only\" / \"nested_cv\"\n",
    "    scenario=\"cases\",                       # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=3,\n",
    "    inner_folds=2,\n",
    "    param_grid=param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec27e2",
   "metadata": {},
   "source": [
    "### Time split CV with parameter search test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cases = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_fix_np,                 # used for BOTH X_fix and TC\n",
    "    X_fixed_and_random=Zrand_np,           # used for Zrand\n",
    "    y=y_np,\n",
    "    y_lags=y_lags_np,\n",
    "    dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np,\n",
    "    time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,         # the dynamic-dims builder we just fixed\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"nested_cv\",                         # or \"cv_only\" / \"nested_cv\"\n",
    "    scenario=\"time\",                       # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=3,\n",
    "    inner_folds=2,\n",
    "    param_grid=param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23d56b",
   "metadata": {},
   "source": [
    "## New outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a226172d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "group",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "beep",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Body_check",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Restr",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Comp",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BE",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EE",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "NA_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "NA_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "NA_3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "NA_4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "NA_5",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "NA_6",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BS_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BS_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SE_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SE_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SS",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_beep",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_beep_copy",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "critical_event",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_5",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_6",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_7",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_8",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_9",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_10",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_11",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_12",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_19",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_20",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_21",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_22",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_23",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_24",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_25",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_26",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_27",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDEQ_28",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS5",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS6",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS7",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS8",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS9",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS10",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS11",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS12",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS13",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS14",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS15",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS16",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS17",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EDDS18",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "education2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "education3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "education4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "critical_event_next",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_beep_diff",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_beep_diff_copy",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "f7fb7d13-d989-4bf3-b995-cc245902f492",
       "rows": [
        [
         "0",
         "1",
         "29",
         "1",
         "0",
         "4",
         "3",
         "3",
         "1",
         "2",
         "1",
         "2",
         "3",
         "2",
         "3",
         "2",
         "2",
         "3",
         "1",
         "2",
         "1",
         "3",
         "4",
         "4",
         "1",
         "6",
         "1",
         "3",
         "3",
         "0",
         "6",
         "1",
         "2",
         "3",
         "6",
         "6",
         "5",
         "1",
         "1",
         "1",
         "5",
         "5",
         "1",
         "5",
         "5",
         "6",
         "6",
         "4",
         "5",
         "4",
         "0",
         "1",
         "10",
         "0",
         "0",
         "1",
         "1",
         "1",
         "1",
         "0",
         "0",
         "0",
         "2",
         "12",
         "5",
         "0",
         "1",
         "0",
         "1",
         "2",
         "2"
        ],
        [
         "1",
         "1",
         "29",
         "1",
         "1",
         "1",
         "3",
         "4",
         "1",
         "2",
         "1",
         "2",
         "3",
         "3",
         "3",
         "3",
         "2",
         "3",
         "3",
         "2",
         "2",
         "4",
         "6",
         "6",
         "1",
         "6",
         "1",
         "3",
         "3",
         "0",
         "6",
         "1",
         "2",
         "3",
         "6",
         "6",
         "5",
         "1",
         "1",
         "1",
         "5",
         "5",
         "1",
         "5",
         "5",
         "6",
         "6",
         "4",
         "5",
         "4",
         "0",
         "1",
         "10",
         "0",
         "0",
         "1",
         "1",
         "1",
         "1",
         "0",
         "0",
         "0",
         "2",
         "12",
         "5",
         "0",
         "1",
         "0",
         "0",
         "1",
         "1"
        ],
        [
         "2",
         "1",
         "29",
         "1",
         "1",
         "2",
         "1",
         "5",
         "1",
         "1",
         "1",
         "2",
         "2",
         "2",
         "2",
         "1",
         "1",
         "2",
         "1",
         "2",
         "1",
         "4",
         "7",
         "7",
         "0",
         "6",
         "1",
         "3",
         "3",
         "0",
         "6",
         "1",
         "2",
         "3",
         "6",
         "6",
         "5",
         "1",
         "1",
         "1",
         "5",
         "5",
         "1",
         "5",
         "5",
         "6",
         "6",
         "4",
         "5",
         "4",
         "0",
         "1",
         "10",
         "0",
         "0",
         "1",
         "1",
         "1",
         "1",
         "0",
         "0",
         "0",
         "2",
         "12",
         "5",
         "0",
         "1",
         "0",
         "1",
         "1",
         "1"
        ],
        [
         "3",
         "1",
         "29",
         "1",
         "1",
         "3",
         "4",
         "3",
         "1",
         "2",
         "1",
         "3",
         "4",
         "2",
         "4",
         "1",
         "2",
         "3",
         "2",
         "2",
         "1",
         "4",
         "8",
         "8",
         "1",
         "6",
         "1",
         "3",
         "3",
         "0",
         "6",
         "1",
         "2",
         "3",
         "6",
         "6",
         "5",
         "1",
         "1",
         "1",
         "5",
         "5",
         "1",
         "5",
         "5",
         "6",
         "6",
         "4",
         "5",
         "4",
         "0",
         "1",
         "10",
         "0",
         "0",
         "1",
         "1",
         "1",
         "1",
         "0",
         "0",
         "0",
         "2",
         "12",
         "5",
         "0",
         "1",
         "0",
         "0",
         "1",
         "1"
        ],
        [
         "4",
         "1",
         "29",
         "1",
         "1",
         "4",
         "4",
         "2",
         "1",
         "2",
         "1",
         "2",
         "4",
         "2",
         "3",
         "1",
         "1",
         "2",
         "2",
         "2",
         "1",
         "3",
         "9",
         "9",
         "0",
         "6",
         "1",
         "3",
         "3",
         "0",
         "6",
         "1",
         "2",
         "3",
         "6",
         "6",
         "5",
         "1",
         "1",
         "1",
         "5",
         "5",
         "1",
         "5",
         "5",
         "6",
         "6",
         "4",
         "5",
         "4",
         "0",
         "1",
         "10",
         "0",
         "0",
         "1",
         "1",
         "1",
         "1",
         "0",
         "0",
         "0",
         "2",
         "12",
         "5",
         "0",
         "1",
         "0",
         "0",
         "2",
         "2"
        ]
       ],
       "shape": {
        "columns": 70,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>group</th>\n",
       "      <th>day</th>\n",
       "      <th>beep</th>\n",
       "      <th>Body_check</th>\n",
       "      <th>Restr</th>\n",
       "      <th>Comp</th>\n",
       "      <th>BE</th>\n",
       "      <th>EE</th>\n",
       "      <th>...</th>\n",
       "      <th>EDDS15</th>\n",
       "      <th>EDDS16</th>\n",
       "      <th>EDDS17</th>\n",
       "      <th>EDDS18</th>\n",
       "      <th>education2</th>\n",
       "      <th>education3</th>\n",
       "      <th>education4</th>\n",
       "      <th>critical_event_next</th>\n",
       "      <th>day_beep_diff</th>\n",
       "      <th>day_beep_diff_copy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age  group  day  beep  Body_check  Restr  Comp  BE  EE  ...  EDDS15  \\\n",
       "0   1   29      1    0     4           3      3     1   2   1  ...       0   \n",
       "1   1   29      1    1     1           3      4     1   2   1  ...       0   \n",
       "2   1   29      1    1     2           1      5     1   1   1  ...       0   \n",
       "3   1   29      1    1     3           4      3     1   2   1  ...       0   \n",
       "4   1   29      1    1     4           4      2     1   2   1  ...       0   \n",
       "\n",
       "   EDDS16  EDDS17  EDDS18  education2  education3  education4  \\\n",
       "0       2      12       5           0           1           0   \n",
       "1       2      12       5           0           1           0   \n",
       "2       2      12       5           0           1           0   \n",
       "3       2      12       5           0           1           0   \n",
       "4       2      12       5           0           1           0   \n",
       "\n",
       "   critical_event_next  day_beep_diff  day_beep_diff_copy  \n",
       "0                    1              2                   2  \n",
       "1                    0              1                   1  \n",
       "2                    1              1                   1  \n",
       "3                    0              1                   1  \n",
       "4                    0              2                   2  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Body_cat_df = pd.read_csv(os.path.join(data_dir, \"Body_data_for_categorical_forecast.csv\"))\n",
    "columns_Body_cat_df = pd.read_csv(os.path.join(data_dir, \"columns_Body_data_for_categorical_forecast.csv\"))\n",
    "\n",
    "Body_cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "395aa2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the outcome column(s) marked with 1 in the \"outcomes\" column of columns_Body_cat_df\n",
    "Body_cat_outcome_cols = columns_Body_cat_df.loc[columns_Body_cat_df['outcomes'] == 1, 'column_name'].tolist()\n",
    "Body_cat_y = Body_cat_df[Body_cat_outcome_cols]\n",
    "Body_cat_y.head()\n",
    "\n",
    "# Same for outcomes lags column(s)\n",
    "Body_cat_outcomes_lags_cols = columns_Body_cat_df.loc[columns_Body_cat_df['outcomes_lags'] == 1, 'column_name'].tolist()\n",
    "Body_cat_outcomes_lags = Body_cat_df[Body_cat_outcomes_lags_cols]\n",
    "\n",
    "# Same for participant column(s)\n",
    "Body_cat_participant_cols = columns_Body_cat_df.loc[columns_Body_cat_df['participant_id'] == 1, 'column_name'].tolist()\n",
    "Body_cat_participant_id = Body_cat_df[Body_cat_participant_cols]\n",
    "\n",
    "# Same for time column(s)\n",
    "Body_cat_time_cols = columns_Body_cat_df.loc[columns_Body_cat_df['time'] == 1, 'column_name'].tolist()\n",
    "Body_cat_time = Body_cat_df[Body_cat_time_cols]\n",
    "\n",
    "# Same for forecast horizons column(s)\n",
    "Body_cat_forecast_horizons_cols = columns_Body_cat_df.loc[columns_Body_cat_df['forecast_horizons'] == 1, 'column_name'].tolist()\n",
    "Body_cat_forecast_horizons = Body_cat_df[Body_cat_forecast_horizons_cols]\n",
    "\n",
    "# Same for fixed effects column(s)\n",
    "Body_cat_only_fixed_cols = columns_Body_cat_df.loc[columns_Body_cat_df['only_fixed'] == 1, 'column_name'].tolist()\n",
    "Body_cat_only_fixed = Body_cat_df[Body_cat_only_fixed_cols]\n",
    "\n",
    "# Same for random effects column(s)\n",
    "Body_cat_fixed_and_random_cols = columns_Body_cat_df.loc[columns_Body_cat_df['fixed_and_random'] == 1, 'column_name'].tolist()\n",
    "Body_cat_fixed_and_random = Body_cat_df[Body_cat_fixed_and_random_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea09b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ---------- targets & aux ----------\n",
    "y_raw      = Body_cat_y.to_numpy(np.float32)\n",
    "y_np       = y_raw if y_raw.ndim == 2 else y_raw.reshape(-1, 1)\n",
    "\n",
    "y_lags_np  = Body_cat_outcomes_lags.to_numpy(np.float32)\n",
    "dt_lags_np = Body_cat_forecast_horizons.to_numpy(np.float32)\n",
    "\n",
    "# ---------- inputs ----------\n",
    "# keep the two blocks SEPARATE; evaluator will:\n",
    "#   - TC  <- only_fixed (after PCA)\n",
    "#   - X_fix <- concat(only_fixed_PCA, fixed_and_random_PCA)\n",
    "#   - Zrand <- fixed_and_random_PCA\n",
    "X_only_fixed_np        = Body_cat_only_fixed.to_numpy(np.float32)\n",
    "X_fixed_and_random_np  = Body_cat_fixed_and_random.to_numpy(np.float32)\n",
    "\n",
    "# ---------- ids & time ----------\n",
    "pid_raw    = Body_cat_participant_id.to_numpy().ravel()\n",
    "pid_uniqs, pid_encoded = np.unique(pid_raw, return_inverse=True)\n",
    "pid_np     = pid_encoded.astype(np.int64)\n",
    "n_ids      = int(len(pid_uniqs))\n",
    "\n",
    "time_ix_np = Body_cat_time.to_numpy().ravel()\n",
    "\n",
    "# ---------- dynamic builder ----------\n",
    "def build_model_fn(\n",
    "    *,\n",
    "    y_dim: int,\n",
    "    d_fix: int,\n",
    "    d_tc: int,        # this will be the PCAâ€™d only_fixed dim (activates FiLM)\n",
    "    d_zrand: int,     # this will be the PCAâ€™d fixed_and_random dim\n",
    "    n_ids: int,\n",
    "    **arch\n",
    "):\n",
    "    return KANAdditiveMixedEffects(\n",
    "        y_dim=y_dim,\n",
    "        d_fix=d_fix,       # = dim(concat(only_fixed_PCA, fixed_and_random_PCA))\n",
    "        d_tc=d_tc,         # = dim(only_fixed_PCA)  -> used by FiLM\n",
    "        d_zrand=d_zrand,   # = dim(fixed_and_random_PCA)\n",
    "        n_ids=n_ids,\n",
    "        **arch\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "arch_defaults = dict(d_fix_latent=32, d_rand_latent=32, n_kernels=6, dropout=0.00)\n",
    "train_defaults = dict(\n",
    "    lr=5e-4, weight_decay=3e-5, batch_size=64,\n",
    "    max_epochs=100, patience=20, threshold=0.5,\n",
    "    lambda_mean0=1e-6, lambda_ridge=1e-5,\n",
    "    lambda_orth_latent=3e-5,\n",
    "    lambda_film_identity=3e-5,\n",
    "    lambda_kan=0,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2b504de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train 0.526399 | val 0.536068\n",
      "Epoch 002 | train 0.504631 | val 0.514953\n",
      "Epoch 003 | train 0.473995 | val 0.491043\n",
      "Epoch 004 | train 0.437780 | val 0.471919\n",
      "Epoch 005 | train 0.404375 | val 0.438891\n",
      "Epoch 006 | train 0.365621 | val 0.424810\n",
      "Epoch 007 | train 0.342439 | val 0.437593\n",
      "Epoch 008 | train 0.331724 | val 0.469246\n",
      "Epoch 009 | train 0.324498 | val 0.485574\n",
      "Epoch 010 | train 0.315082 | val 0.449413\n",
      "Epoch 011 | train 0.302206 | val 0.451521\n",
      "Epoch 012 | train 0.296104 | val 0.458941\n",
      "Epoch 013 | train 0.287374 | val 0.455781\n",
      "Epoch 014 | train 0.280117 | val 0.467125\n",
      "Epoch 015 | train 0.272721 | val 0.477199\n",
      "Epoch 016 | train 0.263935 | val 0.496088\n",
      "Epoch 017 | train 0.259578 | val 0.487866\n",
      "Epoch 018 | train 0.249786 | val 0.514506\n",
      "Epoch 019 | train 0.240466 | val 0.541955\n",
      "Epoch 020 | train 0.230078 | val 0.528945\n",
      "Epoch 021 | train 0.217564 | val 0.551798\n",
      "Epoch 022 | train 0.201423 | val 0.574612\n",
      "Epoch 023 | train 0.190832 | val 0.623385\n",
      "Epoch 024 | train 0.171922 | val 0.654402\n",
      "Epoch 025 | train 0.151953 | val 0.672535\n",
      "Epoch 026 | train 0.137309 | val 0.753851\n",
      "Early stopping at epoch 026 (best val 0.424810)\n",
      "\n",
      "Single-fit test metrics @0.5:\n",
      "        task_1_AUC: 0.7480\n",
      "      task_1_AUPRC: 0.7791\n",
      "      task_1_Brier: 0.2125\n",
      "        task_1_ACC: 0.6889\n",
      "         task_1_F1: 0.6744\n",
      "  task_1_Precision: 0.7632\n",
      "     task_1_Recall: 0.6042\n",
      "task_1_Sensitivity: 0.6042\n",
      "task_1_Specificity: 0.7857\n",
      "         macro_AUC: 0.7480\n",
      "       macro_AUPRC: 0.7791\n",
      "       macro_Brier: 0.2125\n",
      "         macro_ACC: 0.6889\n",
      "          macro_F1: 0.6744\n",
      "   macro_Precision: 0.7632\n",
      "      macro_Recall: 0.6042\n",
      " macro_Sensitivity: 0.6042\n",
      " macro_Specificity: 0.7857\n",
      "\n",
      "Single-fit test metrics @F1-opt per task:\n",
      " task_1_AUC_optthr: 0.7480\n",
      "task_1_AUPRC_optthr: 0.7791\n",
      "task_1_Brier_optthr: 0.2125\n",
      " task_1_ACC_optthr: 0.5778\n",
      "  task_1_F1_optthr: 0.7077\n",
      "task_1_Precision_optthr: 0.5610\n",
      "task_1_Recall_optthr: 0.9583\n",
      "task_1_Sensitivity_optthr: 0.9583\n",
      "task_1_Specificity_optthr: 0.1429\n",
      "  macro_AUC_optthr: 0.7480\n",
      "macro_AUPRC_optthr: 0.7791\n",
      "macro_Brier_optthr: 0.2125\n",
      "  macro_ACC_optthr: 0.5778\n",
      "   macro_F1_optthr: 0.7077\n",
      "macro_Precision_optthr: 0.5610\n",
      "macro_Recall_optthr: 0.9583\n",
      "macro_Sensitivity_optthr: 0.9583\n",
      "macro_Specificity_optthr: 0.1429\n"
     ]
    }
   ],
   "source": [
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"single\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"cases\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e61b5511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train 0.691174 | val 0.674552\n",
      "Epoch 002 | train 0.673155 | val 0.657005\n",
      "Epoch 003 | train 0.657176 | val 0.638801\n",
      "Epoch 004 | train 0.640712 | val 0.618207\n",
      "Epoch 005 | train 0.626919 | val 0.593741\n",
      "Epoch 006 | train 0.609616 | val 0.562917\n",
      "Epoch 007 | train 0.593699 | val 0.524263\n",
      "Epoch 008 | train 0.571390 | val 0.477930\n",
      "Epoch 009 | train 0.546989 | val 0.425699\n",
      "Epoch 010 | train 0.520621 | val 0.375775\n",
      "Epoch 011 | train 0.492534 | val 0.341928\n",
      "Epoch 012 | train 0.476113 | val 0.328653\n",
      "Epoch 013 | train 0.466694 | val 0.323414\n",
      "Epoch 014 | train 0.459762 | val 0.317265\n",
      "Epoch 015 | train 0.450997 | val 0.313831\n",
      "Epoch 016 | train 0.437306 | val 0.325763\n",
      "Epoch 017 | train 0.423853 | val 0.345655\n",
      "Epoch 018 | train 0.414891 | val 0.364518\n",
      "Epoch 019 | train 0.408511 | val 0.371982\n",
      "Epoch 020 | train 0.401708 | val 0.369975\n",
      "Epoch 021 | train 0.395335 | val 0.371225\n",
      "Epoch 022 | train 0.388389 | val 0.373633\n",
      "Epoch 023 | train 0.381316 | val 0.377838\n",
      "Epoch 024 | train 0.373431 | val 0.374006\n",
      "Epoch 025 | train 0.365426 | val 0.366699\n",
      "Epoch 026 | train 0.358890 | val 0.364335\n",
      "Epoch 027 | train 0.350746 | val 0.377296\n",
      "Epoch 028 | train 0.342167 | val 0.388085\n",
      "Epoch 029 | train 0.332647 | val 0.391066\n",
      "Epoch 030 | train 0.323727 | val 0.405465\n",
      "Epoch 031 | train 0.315724 | val 0.423647\n",
      "Epoch 032 | train 0.305099 | val 0.421575\n",
      "Epoch 033 | train 0.295568 | val 0.419013\n",
      "Epoch 034 | train 0.283167 | val 0.422846\n",
      "Epoch 035 | train 0.271008 | val 0.421379\n",
      "Early stopping at epoch 035 (best val 0.313831)\n",
      "\n",
      "Single-fit test metrics @0.5:\n",
      "        task_1_AUC: 0.8381\n",
      "      task_1_AUPRC: 0.8716\n",
      "      task_1_Brier: 0.1695\n",
      "        task_1_ACC: 0.7647\n",
      "         task_1_F1: 0.7447\n",
      "  task_1_Precision: 0.8333\n",
      "     task_1_Recall: 0.6731\n",
      "task_1_Sensitivity: 0.6731\n",
      "task_1_Specificity: 0.8600\n",
      "         macro_AUC: 0.8381\n",
      "       macro_AUPRC: 0.8716\n",
      "       macro_Brier: 0.1695\n",
      "         macro_ACC: 0.7647\n",
      "          macro_F1: 0.7447\n",
      "   macro_Precision: 0.8333\n",
      "      macro_Recall: 0.6731\n",
      " macro_Sensitivity: 0.6731\n",
      " macro_Specificity: 0.8600\n",
      "\n",
      "Single-fit test metrics @F1-opt per task:\n",
      " task_1_AUC_optthr: 0.8381\n",
      "task_1_AUPRC_optthr: 0.8716\n",
      "task_1_Brier_optthr: 0.1695\n",
      " task_1_ACC_optthr: 0.7745\n",
      "  task_1_F1_optthr: 0.7629\n",
      "task_1_Precision_optthr: 0.8222\n",
      "task_1_Recall_optthr: 0.7115\n",
      "task_1_Sensitivity_optthr: 0.7115\n",
      "task_1_Specificity_optthr: 0.8400\n",
      "  macro_AUC_optthr: 0.8381\n",
      "macro_AUPRC_optthr: 0.8716\n",
      "macro_Brier_optthr: 0.1695\n",
      "  macro_ACC_optthr: 0.7745\n",
      "   macro_F1_optthr: 0.7629\n",
      "macro_Precision_optthr: 0.8222\n",
      "macro_Recall_optthr: 0.7115\n",
      "macro_Sensitivity_optthr: 0.7115\n",
      "macro_Specificity_optthr: 0.8400\n"
     ]
    }
   ],
   "source": [
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"single\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"time\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f907af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: macro_AUC_optthr=0.8232, macro_AUPRC_optthr=0.9041, macro_Brier_optthr=0.1664, macro_ACC_optthr=0.8058, macro_F1_optthr=0.8214, macro_Precision_optthr=0.9020, macro_Recall_optthr=0.7541, macro_Sensitivity_optthr=0.7541, macro_Specificity_optthr=0.8810\n",
      "Fold 2: macro_AUC_optthr=0.7019, macro_AUPRC_optthr=0.6877, macro_Brier_optthr=0.3025, macro_ACC_optthr=0.4706, macro_F1_optthr=0.6301, macro_Precision_optthr=0.4792, macro_Recall_optthr=0.9200, macro_Sensitivity_optthr=0.9200, macro_Specificity_optthr=0.0385\n",
      "Fold 3: macro_AUC_optthr=0.2531, macro_AUPRC_optthr=0.6796, macro_Brier_optthr=0.4370, macro_ACC_optthr=0.4804, macro_F1_optthr=0.6345, macro_Precision_optthr=0.7077, macro_Recall_optthr=0.5750, macro_Sensitivity_optthr=0.5750, macro_Specificity_optthr=0.1364\n",
      "Fold 4: macro_AUC_optthr=0.6719, macro_AUPRC_optthr=0.6236, macro_Brier_optthr=0.2280, macro_ACC_optthr=0.3960, macro_F1_optthr=0.5674, macro_Precision_optthr=0.4124, macro_Recall_optthr=0.9091, macro_Sensitivity_optthr=0.9091, macro_Specificity_optthr=0.0000\n",
      "Fold 5: macro_AUC_optthr=0.8632, macro_AUPRC_optthr=0.9270, macro_Brier_optthr=0.1469, macro_ACC_optthr=0.6733, macro_F1_optthr=0.8047, macro_Precision_optthr=0.6800, macro_Recall_optthr=0.9855, macro_Sensitivity_optthr=0.9855, macro_Specificity_optthr=0.0000\n",
      "\n",
      "CV averages (Â±95% CI) @0.5:\n",
      "           macro_ACC: 0.6780  (95% CI 0.4783, 0.8776)\n",
      "           macro_AUC: 0.6627  (95% CI 0.3615, 0.9638)\n",
      "         macro_AUPRC: 0.7644  (95% CI 0.5901, 0.9388)\n",
      "         macro_Brier: 0.2562  (95% CI 0.1097, 0.4026)\n",
      "            macro_F1: 0.7361  (95% CI 0.5984, 0.8738)\n",
      "     macro_Precision: 0.7364  (95% CI 0.5421, 0.9307)\n",
      "        macro_Recall: 0.7619  (95% CI 0.5872, 0.9366)\n",
      "   macro_Sensitivity: 0.7619  (95% CI 0.5872, 0.9366)\n",
      "   macro_Specificity: 0.5366  (95% CI 0.1074, 0.9657)\n",
      "          task_1_ACC: 0.6780  (95% CI 0.4783, 0.8776)\n",
      "          task_1_AUC: 0.6627  (95% CI 0.3615, 0.9638)\n",
      "        task_1_AUPRC: 0.7644  (95% CI 0.5901, 0.9388)\n",
      "        task_1_Brier: 0.2562  (95% CI 0.1097, 0.4026)\n",
      "           task_1_F1: 0.7361  (95% CI 0.5984, 0.8738)\n",
      "    task_1_Precision: 0.7364  (95% CI 0.5421, 0.9307)\n",
      "       task_1_Recall: 0.7619  (95% CI 0.5872, 0.9366)\n",
      "  task_1_Sensitivity: 0.7619  (95% CI 0.5872, 0.9366)\n",
      "  task_1_Specificity: 0.5366  (95% CI 0.1074, 0.9657)\n",
      "\n",
      "CV averages (Â±95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.5652  (95% CI 0.3553, 0.7752)\n",
      "    macro_AUC_optthr: 0.6627  (95% CI 0.3615, 0.9638)\n",
      "  macro_AUPRC_optthr: 0.7644  (95% CI 0.5901, 0.9388)\n",
      "  macro_Brier_optthr: 0.2562  (95% CI 0.1097, 0.4026)\n",
      "     macro_F1_optthr: 0.6916  (95% CI 0.5499, 0.8334)\n",
      "macro_Precision_optthr: 0.6362  (95% CI 0.3939, 0.8786)\n",
      " macro_Recall_optthr: 0.8287  (95% CI 0.6235, 1.0340)\n",
      "macro_Sensitivity_optthr: 0.8287  (95% CI 0.6235, 1.0340)\n",
      "macro_Specificity_optthr: 0.2112  (95% CI -0.2589, 0.6812)\n",
      "   task_1_ACC_optthr: 0.5652  (95% CI 0.3553, 0.7752)\n",
      "   task_1_AUC_optthr: 0.6627  (95% CI 0.3615, 0.9638)\n",
      " task_1_AUPRC_optthr: 0.7644  (95% CI 0.5901, 0.9388)\n",
      " task_1_Brier_optthr: 0.2562  (95% CI 0.1097, 0.4026)\n",
      "    task_1_F1_optthr: 0.6916  (95% CI 0.5499, 0.8334)\n",
      "task_1_Precision_optthr: 0.6362  (95% CI 0.3939, 0.8786)\n",
      "task_1_Recall_optthr: 0.8287  (95% CI 0.6235, 1.0340)\n",
      "task_1_Sensitivity_optthr: 0.8287  (95% CI 0.6235, 1.0340)\n",
      "task_1_Specificity_optthr: 0.2112  (95% CI -0.2589, 0.6812)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"cv_only\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"cases\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "140ec1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV fold 1/5] scenario=cases\n",
      "[split] train | rows=  366 | unique_ids=   27\n",
      "[split] val   | rows=   40 | unique_ids=   15\n",
      "[split] test  | rows=  103 | unique_ids=    6\n",
      "Epoch 001 | train 0.522843 | val 0.569819\n",
      "Epoch 002 | train 0.488875 | val 0.526627\n",
      "Epoch 003 | train 0.450330 | val 0.478078\n",
      "Epoch 004 | train 0.394434 | val 0.416660\n",
      "Epoch 005 | train 0.366790 | val 0.391940\n",
      "Epoch 006 | train 0.352934 | val 0.392635\n",
      "Epoch 007 | train 0.337484 | val 0.371298\n",
      "Epoch 008 | train 0.320750 | val 0.357873\n",
      "Epoch 009 | train 0.309322 | val 0.349957\n",
      "Epoch 010 | train 0.298842 | val 0.338049\n",
      "Epoch 011 | train 0.291214 | val 0.330355\n",
      "Epoch 012 | train 0.284389 | val 0.326210\n",
      "Epoch 013 | train 0.280014 | val 0.342395\n",
      "Epoch 014 | train 0.274001 | val 0.338431\n",
      "Epoch 015 | train 0.268582 | val 0.326771\n",
      "Epoch 016 | train 0.263268 | val 0.345018\n",
      "Epoch 017 | train 0.260025 | val 0.354825\n",
      "Epoch 018 | train 0.255105 | val 0.329520\n",
      "Epoch 019 | train 0.251449 | val 0.353573\n",
      "Epoch 020 | train 0.244268 | val 0.330760\n",
      "Epoch 021 | train 0.237760 | val 0.344027\n",
      "Epoch 022 | train 0.231624 | val 0.341062\n",
      "Epoch 023 | train 0.225755 | val 0.334218\n",
      "Epoch 024 | train 0.221590 | val 0.354898\n",
      "Epoch 025 | train 0.215131 | val 0.360164\n",
      "Epoch 026 | train 0.210914 | val 0.308956\n",
      "Epoch 027 | train 0.204773 | val 0.374061\n",
      "Epoch 028 | train 0.204512 | val 0.367229\n",
      "Epoch 029 | train 0.198499 | val 0.323870\n",
      "Epoch 030 | train 0.190043 | val 0.352298\n",
      "Epoch 031 | train 0.185912 | val 0.392538\n",
      "Epoch 032 | train 0.182211 | val 0.388483\n",
      "Epoch 033 | train 0.178409 | val 0.396237\n",
      "Epoch 034 | train 0.168651 | val 0.362491\n",
      "Epoch 035 | train 0.170348 | val 0.455038\n",
      "Epoch 036 | train 0.162830 | val 0.415334\n",
      "Epoch 037 | train 0.158328 | val 0.480550\n",
      "Epoch 038 | train 0.157980 | val 0.434088\n",
      "Epoch 039 | train 0.154050 | val 0.522630\n",
      "Epoch 040 | train 0.154894 | val 0.452757\n",
      "Epoch 041 | train 0.149116 | val 0.503234\n",
      "Epoch 042 | train 0.141505 | val 0.563294\n",
      "Epoch 043 | train 0.136240 | val 0.580783\n",
      "Epoch 044 | train 0.137245 | val 0.619650\n",
      "Epoch 045 | train 0.129302 | val 0.623435\n",
      "Epoch 046 | train 0.128344 | val 0.611842\n",
      "Early stopping at epoch 046 (best val 0.308956)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7896, macro_AUPRC_optthr=0.8665, macro_Brier_optthr=0.2266, macro_ACC_optthr=0.7379, macro_F1_optthr=0.7429, macro_Precision_optthr=0.8864, macro_Recall_optthr=0.6393, macro_Sensitivity_optthr=0.6393, macro_Specificity_optthr=0.8810\n",
      "\n",
      "[CV fold 2/5] scenario=cases\n",
      "[split] train | rows=  367 | unique_ids=   26\n",
      "[split] val   | rows=   40 | unique_ids=   18\n",
      "[split] test  | rows=  102 | unique_ids=    7\n",
      "Epoch 001 | train 0.473192 | val 0.513566\n",
      "Epoch 002 | train 0.425846 | val 0.500020\n",
      "Epoch 003 | train 0.381206 | val 0.457739\n",
      "Epoch 004 | train 0.336295 | val 0.424938\n",
      "Epoch 005 | train 0.319616 | val 0.431140\n",
      "Epoch 006 | train 0.305516 | val 0.475963\n",
      "Epoch 007 | train 0.290793 | val 0.448940\n",
      "Epoch 008 | train 0.280575 | val 0.427788\n",
      "Epoch 009 | train 0.271604 | val 0.399911\n",
      "Epoch 010 | train 0.262490 | val 0.399769\n",
      "Epoch 011 | train 0.255619 | val 0.416830\n",
      "Epoch 012 | train 0.249817 | val 0.381541\n",
      "Epoch 013 | train 0.241918 | val 0.378179\n",
      "Epoch 014 | train 0.236143 | val 0.369876\n",
      "Epoch 015 | train 0.230549 | val 0.343037\n",
      "Epoch 016 | train 0.226159 | val 0.371370\n",
      "Epoch 017 | train 0.221538 | val 0.340375\n",
      "Epoch 018 | train 0.216546 | val 0.361909\n",
      "Epoch 019 | train 0.209745 | val 0.438287\n",
      "Epoch 020 | train 0.209178 | val 0.367799\n",
      "Epoch 021 | train 0.202919 | val 0.424770\n",
      "Epoch 022 | train 0.199669 | val 0.466044\n",
      "Epoch 023 | train 0.193863 | val 0.455533\n",
      "Epoch 024 | train 0.194751 | val 0.422973\n",
      "Epoch 025 | train 0.190555 | val 0.428051\n",
      "Epoch 026 | train 0.190568 | val 0.422957\n",
      "Epoch 027 | train 0.182727 | val 0.585702\n",
      "Epoch 028 | train 0.182773 | val 0.396815\n",
      "Epoch 029 | train 0.186685 | val 0.346653\n",
      "Epoch 030 | train 0.172238 | val 0.551896\n",
      "Epoch 031 | train 0.172033 | val 0.523729\n",
      "Epoch 032 | train 0.167821 | val 0.354894\n",
      "Epoch 033 | train 0.164111 | val 0.412781\n",
      "Epoch 034 | train 0.161516 | val 0.512565\n",
      "Epoch 035 | train 0.155722 | val 0.580042\n",
      "Epoch 036 | train 0.154214 | val 0.563351\n",
      "Epoch 037 | train 0.152409 | val 0.554141\n",
      "Early stopping at epoch 037 (best val 0.340375)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7175, macro_AUPRC_optthr=0.6682, macro_Brier_optthr=0.3242, macro_ACC_optthr=0.4902, macro_F1_optthr=0.6579, macro_Precision_optthr=0.4902, macro_Recall_optthr=1.0000, macro_Sensitivity_optthr=1.0000, macro_Specificity_optthr=0.0000\n",
      "\n",
      "[CV fold 3/5] scenario=cases\n",
      "[split] train | rows=  367 | unique_ids=   26\n",
      "[split] val   | rows=   40 | unique_ids=   20\n",
      "[split] test  | rows=  102 | unique_ids=    7\n",
      "Epoch 001 | train 0.617931 | val 0.556975\n",
      "Epoch 002 | train 0.560945 | val 0.497011\n",
      "Epoch 003 | train 0.499446 | val 0.433914\n",
      "Epoch 004 | train 0.460418 | val 0.396604\n",
      "Epoch 005 | train 0.443553 | val 0.367690\n",
      "Epoch 006 | train 0.430836 | val 0.349376\n",
      "Epoch 007 | train 0.411219 | val 0.348550\n",
      "Epoch 008 | train 0.399510 | val 0.344697\n",
      "Epoch 009 | train 0.387335 | val 0.336218\n",
      "Epoch 010 | train 0.376429 | val 0.328664\n",
      "Epoch 011 | train 0.368984 | val 0.324328\n",
      "Epoch 012 | train 0.362869 | val 0.315640\n",
      "Epoch 013 | train 0.356705 | val 0.324942\n",
      "Epoch 014 | train 0.348958 | val 0.316156\n",
      "Epoch 015 | train 0.342229 | val 0.310677\n",
      "Epoch 016 | train 0.337890 | val 0.301256\n",
      "Epoch 017 | train 0.331052 | val 0.296830\n",
      "Epoch 018 | train 0.324946 | val 0.290044\n",
      "Epoch 019 | train 0.317881 | val 0.289228\n",
      "Epoch 020 | train 0.309271 | val 0.288516\n",
      "Epoch 021 | train 0.303180 | val 0.277487\n",
      "Epoch 022 | train 0.294621 | val 0.267687\n",
      "Epoch 023 | train 0.288307 | val 0.270054\n",
      "Epoch 024 | train 0.280567 | val 0.299302\n",
      "Epoch 025 | train 0.267461 | val 0.281009\n",
      "Epoch 026 | train 0.265442 | val 0.254287\n",
      "Epoch 027 | train 0.252602 | val 0.279991\n",
      "Epoch 028 | train 0.250802 | val 0.300568\n",
      "Epoch 029 | train 0.242148 | val 0.305157\n",
      "Epoch 030 | train 0.237233 | val 0.259712\n",
      "Epoch 031 | train 0.247680 | val 0.244623\n",
      "Epoch 032 | train 0.245105 | val 0.285415\n",
      "Epoch 033 | train 0.240027 | val 0.310473\n",
      "Epoch 034 | train 0.227022 | val 0.322272\n",
      "Epoch 035 | train 0.232261 | val 0.271725\n",
      "Epoch 036 | train 0.218894 | val 0.271669\n",
      "Epoch 037 | train 0.214768 | val 0.272294\n",
      "Epoch 038 | train 0.215238 | val 0.250302\n",
      "Epoch 039 | train 0.209640 | val 0.253545\n",
      "Epoch 040 | train 0.206120 | val 0.282466\n",
      "Epoch 041 | train 0.199800 | val 0.256958\n",
      "Epoch 042 | train 0.199852 | val 0.239948\n",
      "Epoch 043 | train 0.193334 | val 0.271542\n",
      "Epoch 044 | train 0.193024 | val 0.262252\n",
      "Epoch 045 | train 0.188313 | val 0.259979\n",
      "Epoch 046 | train 0.177398 | val 0.251014\n",
      "Epoch 047 | train 0.180642 | val 0.260014\n",
      "Epoch 048 | train 0.165726 | val 0.281395\n",
      "Epoch 049 | train 0.164878 | val 0.276986\n",
      "Epoch 050 | train 0.165899 | val 0.227995\n",
      "Epoch 051 | train 0.163962 | val 0.267637\n",
      "Epoch 052 | train 0.159675 | val 0.247225\n",
      "Epoch 053 | train 0.146513 | val 0.237004\n",
      "Epoch 054 | train 0.144922 | val 0.247617\n",
      "Epoch 055 | train 0.139395 | val 0.248592\n",
      "Epoch 056 | train 0.138632 | val 0.273410\n",
      "Epoch 057 | train 0.135385 | val 0.216660\n",
      "Epoch 058 | train 0.126613 | val 0.221751\n",
      "Epoch 059 | train 0.123259 | val 0.245007\n",
      "Epoch 060 | train 0.118683 | val 0.243136\n",
      "Epoch 061 | train 0.113883 | val 0.207058\n",
      "Epoch 062 | train 0.120987 | val 0.269473\n",
      "Epoch 063 | train 0.105352 | val 0.263563\n",
      "Epoch 064 | train 0.118009 | val 0.275980\n",
      "Epoch 065 | train 0.107420 | val 0.268681\n",
      "Epoch 066 | train 0.102337 | val 0.266666\n",
      "Epoch 067 | train 0.095115 | val 0.291101\n",
      "Epoch 068 | train 0.091974 | val 0.316978\n",
      "Epoch 069 | train 0.091196 | val 0.280567\n",
      "Epoch 070 | train 0.092044 | val 0.294244\n",
      "Epoch 071 | train 0.101020 | val 0.323278\n",
      "Epoch 072 | train 0.085428 | val 0.311659\n",
      "Epoch 073 | train 0.093167 | val 0.380685\n",
      "Epoch 074 | train 0.088194 | val 0.369422\n",
      "Epoch 075 | train 0.080352 | val 0.316321\n",
      "Epoch 076 | train 0.079806 | val 0.355509\n",
      "Epoch 077 | train 0.076777 | val 0.378112\n",
      "Epoch 078 | train 0.073973 | val 0.376702\n",
      "Epoch 079 | train 0.075725 | val 0.390378\n",
      "Epoch 080 | train 0.077596 | val 0.401390\n",
      "Epoch 081 | train 0.084841 | val 0.431631\n",
      "Early stopping at epoch 081 (best val 0.207058)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.2835, macro_AUPRC_optthr=0.7221, macro_Brier_optthr=0.5672, macro_ACC_optthr=0.3431, macro_F1_optthr=0.4463, macro_Precision_optthr=0.6585, macro_Recall_optthr=0.3375, macro_Sensitivity_optthr=0.3375, macro_Specificity_optthr=0.3636\n",
      "\n",
      "[CV fold 4/5] scenario=cases\n",
      "[split] train | rows=  368 | unique_ids=   25\n",
      "[split] val   | rows=   40 | unique_ids=   20\n",
      "[split] test  | rows=  101 | unique_ids=    6\n",
      "Epoch 001 | train 0.486932 | val 0.479539\n",
      "Epoch 002 | train 0.442492 | val 0.448590\n",
      "Epoch 003 | train 0.395472 | val 0.425580\n",
      "Epoch 004 | train 0.351055 | val 0.428041\n",
      "Epoch 005 | train 0.328918 | val 0.443513\n",
      "Epoch 006 | train 0.309750 | val 0.455003\n",
      "Epoch 007 | train 0.291529 | val 0.465075\n",
      "Epoch 008 | train 0.278732 | val 0.464153\n",
      "Epoch 009 | train 0.268755 | val 0.464492\n",
      "Epoch 010 | train 0.262265 | val 0.457752\n",
      "Epoch 011 | train 0.256873 | val 0.455257\n",
      "Epoch 012 | train 0.251960 | val 0.461087\n",
      "Epoch 013 | train 0.247587 | val 0.478966\n",
      "Epoch 014 | train 0.242634 | val 0.477276\n",
      "Epoch 015 | train 0.238853 | val 0.483100\n",
      "Epoch 016 | train 0.233666 | val 0.483003\n",
      "Epoch 017 | train 0.231208 | val 0.490682\n",
      "Epoch 018 | train 0.226328 | val 0.504294\n",
      "Epoch 019 | train 0.223806 | val 0.520012\n",
      "Epoch 020 | train 0.219240 | val 0.530112\n",
      "Epoch 021 | train 0.216366 | val 0.545779\n",
      "Epoch 022 | train 0.210684 | val 0.556142\n",
      "Epoch 023 | train 0.207277 | val 0.545594\n",
      "Early stopping at epoch 023 (best val 0.425580)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7452, macro_AUPRC_optthr=0.7213, macro_Brier_optthr=0.2096, macro_ACC_optthr=0.5149, macro_F1_optthr=0.6316, macro_Precision_optthr=0.4719, macro_Recall_optthr=0.9545, macro_Sensitivity_optthr=0.9545, macro_Specificity_optthr=0.1754\n",
      "\n",
      "[CV fold 5/5] scenario=cases\n",
      "[split] train | rows=  368 | unique_ids=   26\n",
      "[split] val   | rows=   40 | unique_ids=   17\n",
      "[split] test  | rows=  101 | unique_ids=    7\n",
      "Epoch 001 | train 0.580178 | val 0.568531\n",
      "Epoch 002 | train 0.540525 | val 0.530241\n",
      "Epoch 003 | train 0.491785 | val 0.496008\n",
      "Epoch 004 | train 0.438415 | val 0.489909\n",
      "Epoch 005 | train 0.413859 | val 0.510742\n",
      "Epoch 006 | train 0.403578 | val 0.514266\n",
      "Epoch 007 | train 0.383198 | val 0.525589\n",
      "Epoch 008 | train 0.370402 | val 0.520028\n",
      "Epoch 009 | train 0.359018 | val 0.528489\n",
      "Epoch 010 | train 0.347235 | val 0.545852\n",
      "Epoch 011 | train 0.338414 | val 0.569568\n",
      "Epoch 012 | train 0.331918 | val 0.580000\n",
      "Epoch 013 | train 0.325349 | val 0.610350\n",
      "Epoch 014 | train 0.320742 | val 0.630807\n",
      "Epoch 015 | train 0.315474 | val 0.674584\n",
      "Epoch 016 | train 0.310228 | val 0.696012\n",
      "Epoch 017 | train 0.304910 | val 0.749451\n",
      "Epoch 018 | train 0.301429 | val 0.788744\n",
      "Epoch 019 | train 0.296046 | val 0.787756\n",
      "Epoch 020 | train 0.291137 | val 0.833867\n",
      "Epoch 021 | train 0.283521 | val 0.909873\n",
      "Epoch 022 | train 0.278685 | val 0.948923\n",
      "Epoch 023 | train 0.273981 | val 1.008553\n",
      "Epoch 024 | train 0.268410 | val 1.055607\n",
      "Early stopping at epoch 024 (best val 0.489909)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8528, macro_AUPRC_optthr=0.9043, macro_Brier_optthr=0.1349, macro_ACC_optthr=0.8713, macro_F1_optthr=0.9091, macro_Precision_optthr=0.8784, macro_Recall_optthr=0.9420, macro_Sensitivity_optthr=0.9420, macro_Specificity_optthr=0.7188\n",
      "\n",
      "CV averages (Â±95% CI) @0.5:\n",
      "           macro_ACC: 0.6191  (95% CI 0.3681, 0.8702)\n",
      "           macro_AUC: 0.6777  (95% CI 0.3968, 0.9586)\n",
      "         macro_AUPRC: 0.7765  (95% CI 0.6490, 0.9039)\n",
      "         macro_Brier: 0.2925  (95% CI 0.0842, 0.5008)\n",
      "            macro_F1: 0.6772  (95% CI 0.4689, 0.8856)\n",
      "     macro_Precision: 0.6976  (95% CI 0.4761, 0.9190)\n",
      "        macro_Recall: 0.7171  (95% CI 0.4058, 1.0283)\n",
      "   macro_Sensitivity: 0.7171  (95% CI 0.4058, 1.0283)\n",
      "   macro_Specificity: 0.5238  (95% CI 0.1342, 0.9135)\n",
      "          task_1_ACC: 0.6191  (95% CI 0.3681, 0.8702)\n",
      "          task_1_AUC: 0.6777  (95% CI 0.3968, 0.9586)\n",
      "        task_1_AUPRC: 0.7765  (95% CI 0.6490, 0.9039)\n",
      "        task_1_Brier: 0.2925  (95% CI 0.0842, 0.5008)\n",
      "           task_1_F1: 0.6772  (95% CI 0.4689, 0.8856)\n",
      "    task_1_Precision: 0.6976  (95% CI 0.4761, 0.9190)\n",
      "       task_1_Recall: 0.7171  (95% CI 0.4058, 1.0283)\n",
      "  task_1_Sensitivity: 0.7171  (95% CI 0.4058, 1.0283)\n",
      "  task_1_Specificity: 0.5238  (95% CI 0.1342, 0.9135)\n",
      "\n",
      "CV averages (Â±95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.5915  (95% CI 0.3299, 0.8530)\n",
      "    macro_AUC_optthr: 0.6777  (95% CI 0.3968, 0.9586)\n",
      "  macro_AUPRC_optthr: 0.7765  (95% CI 0.6490, 0.9039)\n",
      "  macro_Brier_optthr: 0.2925  (95% CI 0.0842, 0.5008)\n",
      "     macro_F1_optthr: 0.6775  (95% CI 0.4681, 0.8870)\n",
      "macro_Precision_optthr: 0.6771  (95% CI 0.4274, 0.9267)\n",
      " macro_Recall_optthr: 0.7747  (95% CI 0.4232, 1.1262)\n",
      "macro_Sensitivity_optthr: 0.7747  (95% CI 0.4232, 1.1262)\n",
      "macro_Specificity_optthr: 0.4278  (95% CI -0.0288, 0.8843)\n",
      "   task_1_ACC_optthr: 0.5915  (95% CI 0.3299, 0.8530)\n",
      "   task_1_AUC_optthr: 0.6777  (95% CI 0.3968, 0.9586)\n",
      " task_1_AUPRC_optthr: 0.7765  (95% CI 0.6490, 0.9039)\n",
      " task_1_Brier_optthr: 0.2925  (95% CI 0.0842, 0.5008)\n",
      "    task_1_F1_optthr: 0.6775  (95% CI 0.4681, 0.8870)\n",
      "task_1_Precision_optthr: 0.6771  (95% CI 0.4274, 0.9267)\n",
      "task_1_Recall_optthr: 0.7747  (95% CI 0.4232, 1.1262)\n",
      "task_1_Sensitivity_optthr: 0.7747  (95% CI 0.4232, 1.1262)\n",
      "task_1_Specificity_optthr: 0.4278  (95% CI -0.0288, 0.8843)\n",
      "Arch params (used in all folds): {'d_fix_latent': 32, 'd_rand_latent': 32, 'n_kernels': 6, 'dropout': 0.0}\n",
      "Train params (used in all folds): {'lr': 0.0005, 'weight_decay': 3e-05, 'batch_size': 64, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 1e-06, 'lambda_ridge': 1e-05, 'lambda_orth_latent': 3e-05, 'lambda_film_identity': 3e-05, 'lambda_kan': 0, 'random_state': 42}\n",
      "\n",
      "[CV fold 1/5] scenario=time\n",
      "[split] train | rows=   81 | unique_ids=   31\n",
      "[split] val   | rows=    8 | unique_ids=    7\n",
      "[split] test  | rows=   84 | unique_ids=   28\n",
      "Epoch 001 | train 0.308555 | val 0.380410\n",
      "Epoch 002 | train 0.301147 | val 0.384487\n",
      "Epoch 003 | train 0.294468 | val 0.388852\n",
      "Epoch 004 | train 0.287148 | val 0.391928\n",
      "Epoch 005 | train 0.279579 | val 0.396193\n",
      "Epoch 006 | train 0.271287 | val 0.400951\n",
      "Epoch 007 | train 0.261489 | val 0.406153\n",
      "Epoch 008 | train 0.251899 | val 0.414440\n",
      "Epoch 009 | train 0.241638 | val 0.433321\n",
      "Epoch 010 | train 0.229599 | val 0.465028\n",
      "Epoch 011 | train 0.216374 | val 0.508845\n",
      "Epoch 012 | train 0.204051 | val 0.563444\n",
      "Epoch 013 | train 0.191435 | val 0.629335\n",
      "Epoch 014 | train 0.180412 | val 0.707260\n",
      "Epoch 015 | train 0.169104 | val 0.798235\n",
      "Epoch 016 | train 0.158691 | val 0.899317\n",
      "Epoch 017 | train 0.149975 | val 1.007025\n",
      "Epoch 018 | train 0.144446 | val 1.107556\n",
      "Epoch 019 | train 0.138074 | val 1.194514\n",
      "Epoch 020 | train 0.133672 | val 1.265053\n",
      "Epoch 021 | train 0.128112 | val 1.300000\n",
      "Early stopping at epoch 021 (best val 0.380410)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7112, macro_AUPRC_optthr=0.7684, macro_Brier_optthr=0.2471, macro_ACC_optthr=0.6905, macro_F1_optthr=0.7869, macro_Precision_optthr=0.6761, macro_Recall_optthr=0.9412, macro_Sensitivity_optthr=0.9412, macro_Specificity_optthr=0.3030\n",
      "\n",
      "[CV fold 2/5] scenario=time\n",
      "[split] train | rows=  156 | unique_ids=   33\n",
      "[split] val   | rows=   17 | unique_ids=   14\n",
      "[split] test  | rows=   84 | unique_ids=   24\n",
      "Epoch 001 | train 0.385994 | val 0.512209\n",
      "Epoch 002 | train 0.374491 | val 0.520596\n",
      "Epoch 003 | train 0.361950 | val 0.512739\n",
      "Epoch 004 | train 0.347083 | val 0.501274\n",
      "Epoch 005 | train 0.332153 | val 0.490414\n",
      "Epoch 006 | train 0.316477 | val 0.491010\n",
      "Epoch 007 | train 0.301251 | val 0.517468\n",
      "Epoch 008 | train 0.290462 | val 0.565026\n",
      "Epoch 009 | train 0.278055 | val 0.593840\n",
      "Epoch 010 | train 0.267559 | val 0.621554\n",
      "Epoch 011 | train 0.255245 | val 0.659229\n",
      "Epoch 012 | train 0.243837 | val 0.719630\n",
      "Epoch 013 | train 0.233136 | val 0.796043\n",
      "Epoch 014 | train 0.225534 | val 0.860162\n",
      "Epoch 015 | train 0.215659 | val 0.924299\n",
      "Epoch 016 | train 0.207939 | val 0.951218\n",
      "Epoch 017 | train 0.197784 | val 0.938313\n",
      "Epoch 018 | train 0.187844 | val 0.925189\n",
      "Epoch 019 | train 0.180799 | val 0.908925\n",
      "Epoch 020 | train 0.173983 | val 0.909725\n",
      "Epoch 021 | train 0.167251 | val 0.971388\n",
      "Epoch 022 | train 0.160961 | val 1.070850\n",
      "Epoch 023 | train 0.155077 | val 1.208919\n",
      "Epoch 024 | train 0.150017 | val 1.366690\n",
      "Epoch 025 | train 0.143570 | val 1.536261\n",
      "Early stopping at epoch 025 (best val 0.490414)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8806, macro_AUPRC_optthr=0.9260, macro_Brier_optthr=0.1738, macro_ACC_optthr=0.6667, macro_F1_optthr=0.7742, macro_Precision_optthr=0.6575, macro_Recall_optthr=0.9412, macro_Sensitivity_optthr=0.9412, macro_Specificity_optthr=0.2424\n",
      "\n",
      "[CV fold 3/5] scenario=time\n",
      "[split] train | rows=  232 | unique_ids=   33\n",
      "[split] val   | rows=   25 | unique_ids=   16\n",
      "[split] test  | rows=   84 | unique_ids=   22\n",
      "Epoch 001 | train 0.440000 | val 0.503664\n",
      "Epoch 002 | train 0.417873 | val 0.508318\n",
      "Epoch 003 | train 0.396448 | val 0.502091\n",
      "Epoch 004 | train 0.368640 | val 0.497415\n",
      "Epoch 005 | train 0.340218 | val 0.499643\n",
      "Epoch 006 | train 0.314310 | val 0.527920\n",
      "Epoch 007 | train 0.298071 | val 0.562654\n",
      "Epoch 008 | train 0.287998 | val 0.576791\n",
      "Epoch 009 | train 0.277250 | val 0.551464\n",
      "Epoch 010 | train 0.265900 | val 0.520585\n",
      "Epoch 011 | train 0.256413 | val 0.494750\n",
      "Epoch 012 | train 0.246460 | val 0.474176\n",
      "Epoch 013 | train 0.238894 | val 0.454249\n",
      "Epoch 014 | train 0.230464 | val 0.444778\n",
      "Epoch 015 | train 0.224474 | val 0.438703\n",
      "Epoch 016 | train 0.217066 | val 0.441897\n",
      "Epoch 017 | train 0.211992 | val 0.450509\n",
      "Epoch 018 | train 0.205753 | val 0.449366\n",
      "Epoch 019 | train 0.200240 | val 0.443596\n",
      "Epoch 020 | train 0.195122 | val 0.436221\n",
      "Epoch 021 | train 0.190600 | val 0.431840\n",
      "Epoch 022 | train 0.185454 | val 0.433169\n",
      "Epoch 023 | train 0.181035 | val 0.438174\n",
      "Epoch 024 | train 0.177166 | val 0.446608\n",
      "Epoch 025 | train 0.171067 | val 0.461548\n",
      "Epoch 026 | train 0.165527 | val 0.490553\n",
      "Epoch 027 | train 0.161390 | val 0.524546\n",
      "Epoch 028 | train 0.157266 | val 0.532617\n",
      "Epoch 029 | train 0.152016 | val 0.568022\n",
      "Epoch 030 | train 0.144479 | val 0.627456\n",
      "Epoch 031 | train 0.140336 | val 0.692585\n",
      "Epoch 032 | train 0.137026 | val 0.682751\n",
      "Epoch 033 | train 0.128599 | val 0.708741\n",
      "Epoch 034 | train 0.122226 | val 0.726297\n",
      "Epoch 035 | train 0.119545 | val 0.775780\n",
      "Epoch 036 | train 0.111716 | val 0.847267\n",
      "Epoch 037 | train 0.107836 | val 0.844232\n",
      "Epoch 038 | train 0.098884 | val 0.827825\n",
      "Epoch 039 | train 0.097853 | val 0.830484\n",
      "Epoch 040 | train 0.090932 | val 0.833167\n",
      "Epoch 041 | train 0.082456 | val 0.854884\n",
      "Early stopping at epoch 041 (best val 0.431840)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8173, macro_AUPRC_optthr=0.8764, macro_Brier_optthr=0.1904, macro_ACC_optthr=0.7619, macro_F1_optthr=0.8214, macro_Precision_optthr=0.7667, macro_Recall_optthr=0.8846, macro_Sensitivity_optthr=0.8846, macro_Specificity_optthr=0.5625\n",
      "\n",
      "[CV fold 4/5] scenario=time\n",
      "[split] train | rows=  307 | unique_ids=   33\n",
      "[split] val   | rows=   34 | unique_ids=   23\n",
      "[split] test  | rows=   84 | unique_ids=   19\n",
      "Epoch 001 | train 0.491328 | val 0.414100\n",
      "Epoch 002 | train 0.464439 | val 0.384261\n",
      "Epoch 003 | train 0.435704 | val 0.342230\n",
      "Epoch 004 | train 0.404374 | val 0.288184\n",
      "Epoch 005 | train 0.386956 | val 0.240368\n",
      "Epoch 006 | train 0.374647 | val 0.213399\n",
      "Epoch 007 | train 0.361964 | val 0.205824\n",
      "Epoch 008 | train 0.346535 | val 0.201845\n",
      "Epoch 009 | train 0.333990 | val 0.191811\n",
      "Epoch 010 | train 0.324796 | val 0.174921\n",
      "Epoch 011 | train 0.314207 | val 0.172537\n",
      "Epoch 012 | train 0.303946 | val 0.166692\n",
      "Epoch 013 | train 0.297095 | val 0.158703\n",
      "Epoch 014 | train 0.288621 | val 0.157757\n",
      "Epoch 015 | train 0.281224 | val 0.151814\n",
      "Epoch 016 | train 0.276498 | val 0.155874\n",
      "Epoch 017 | train 0.269963 | val 0.159163\n",
      "Epoch 018 | train 0.266573 | val 0.159358\n",
      "Epoch 019 | train 0.260179 | val 0.162629\n",
      "Epoch 020 | train 0.252937 | val 0.155079\n",
      "Epoch 021 | train 0.246946 | val 0.148320\n",
      "Epoch 022 | train 0.241519 | val 0.155462\n",
      "Epoch 023 | train 0.233521 | val 0.155540\n",
      "Epoch 024 | train 0.224434 | val 0.158566\n",
      "Epoch 025 | train 0.216795 | val 0.168610\n",
      "Epoch 026 | train 0.208302 | val 0.184118\n",
      "Epoch 027 | train 0.206477 | val 0.199210\n",
      "Epoch 028 | train 0.194244 | val 0.208778\n",
      "Epoch 029 | train 0.188790 | val 0.227779\n",
      "Epoch 030 | train 0.180665 | val 0.257064\n",
      "Epoch 031 | train 0.176136 | val 0.271048\n",
      "Epoch 032 | train 0.173331 | val 0.290508\n",
      "Epoch 033 | train 0.160816 | val 0.295545\n",
      "Epoch 034 | train 0.156952 | val 0.318461\n",
      "Epoch 035 | train 0.149483 | val 0.331763\n",
      "Epoch 036 | train 0.152893 | val 0.353803\n",
      "Epoch 037 | train 0.140767 | val 0.381292\n",
      "Epoch 038 | train 0.143747 | val 0.367468\n",
      "Epoch 039 | train 0.142342 | val 0.387309\n",
      "Epoch 040 | train 0.127162 | val 0.385370\n",
      "Epoch 041 | train 0.131464 | val 0.393138\n",
      "Early stopping at epoch 041 (best val 0.148320)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8542, macro_AUPRC_optthr=0.8592, macro_Brier_optthr=0.1738, macro_ACC_optthr=0.7381, macro_F1_optthr=0.6944, macro_Precision_optthr=0.8065, macro_Recall_optthr=0.6098, macro_Sensitivity_optthr=0.6098, macro_Specificity_optthr=0.8605\n",
      "\n",
      "[CV fold 5/5] scenario=time\n",
      "[split] train | rows=  383 | unique_ids=   32\n",
      "[split] val   | rows=   42 | unique_ids=   23\n",
      "[split] test  | rows=   84 | unique_ids=   18\n",
      "Epoch 001 | train 0.532696 | val 0.446579\n",
      "Epoch 002 | train 0.493261 | val 0.409139\n",
      "Epoch 003 | train 0.448450 | val 0.370258\n",
      "Epoch 004 | train 0.413663 | val 0.354667\n",
      "Epoch 005 | train 0.391158 | val 0.325585\n",
      "Epoch 006 | train 0.375113 | val 0.316670\n",
      "Epoch 007 | train 0.356626 | val 0.297156\n",
      "Epoch 008 | train 0.341517 | val 0.285167\n",
      "Epoch 009 | train 0.328118 | val 0.278995\n",
      "Epoch 010 | train 0.319318 | val 0.271681\n",
      "Epoch 011 | train 0.313015 | val 0.272297\n",
      "Epoch 012 | train 0.304976 | val 0.265204\n",
      "Epoch 013 | train 0.297744 | val 0.257483\n",
      "Epoch 014 | train 0.292793 | val 0.246680\n",
      "Epoch 015 | train 0.289514 | val 0.245637\n",
      "Epoch 016 | train 0.284300 | val 0.243524\n",
      "Epoch 017 | train 0.280520 | val 0.243335\n",
      "Epoch 018 | train 0.277493 | val 0.236376\n",
      "Epoch 019 | train 0.271266 | val 0.228427\n",
      "Epoch 020 | train 0.270935 | val 0.219595\n",
      "Epoch 021 | train 0.262095 | val 0.226645\n",
      "Epoch 022 | train 0.259077 | val 0.228497\n",
      "Epoch 023 | train 0.255319 | val 0.214377\n",
      "Epoch 024 | train 0.251132 | val 0.214506\n",
      "Epoch 025 | train 0.246783 | val 0.216619\n",
      "Epoch 026 | train 0.242611 | val 0.208062\n",
      "Epoch 027 | train 0.241837 | val 0.208217\n",
      "Epoch 028 | train 0.234821 | val 0.212036\n",
      "Epoch 029 | train 0.227064 | val 0.220010\n",
      "Epoch 030 | train 0.225334 | val 0.216409\n",
      "Epoch 031 | train 0.219575 | val 0.208273\n",
      "Epoch 032 | train 0.216805 | val 0.217157\n",
      "Epoch 033 | train 0.216268 | val 0.232431\n",
      "Epoch 034 | train 0.205028 | val 0.204745\n",
      "Epoch 035 | train 0.197818 | val 0.220589\n",
      "Epoch 036 | train 0.191971 | val 0.231182\n",
      "Epoch 037 | train 0.186748 | val 0.224660\n",
      "Epoch 038 | train 0.178455 | val 0.235135\n",
      "Epoch 039 | train 0.172221 | val 0.227961\n",
      "Epoch 040 | train 0.169798 | val 0.255264\n",
      "Epoch 041 | train 0.159413 | val 0.243213\n",
      "Epoch 042 | train 0.151237 | val 0.252220\n",
      "Epoch 043 | train 0.147283 | val 0.248773\n",
      "Epoch 044 | train 0.139870 | val 0.267500\n",
      "Epoch 045 | train 0.139402 | val 0.269850\n",
      "Epoch 046 | train 0.149845 | val 0.276021\n",
      "Epoch 047 | train 0.132901 | val 0.286925\n",
      "Epoch 048 | train 0.146656 | val 0.233475\n",
      "Epoch 049 | train 0.130108 | val 0.257747\n",
      "Epoch 050 | train 0.128308 | val 0.296667\n",
      "Epoch 051 | train 0.114009 | val 0.296398\n",
      "Epoch 052 | train 0.111564 | val 0.295163\n",
      "Epoch 053 | train 0.109045 | val 0.324981\n",
      "Epoch 054 | train 0.106011 | val 0.321711\n",
      "Early stopping at epoch 054 (best val 0.204745)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8508, macro_AUPRC_optthr=0.8844, macro_Brier_optthr=0.1655, macro_ACC_optthr=0.7857, macro_F1_optthr=0.7632, macro_Precision_optthr=0.8286, macro_Recall_optthr=0.7073, macro_Sensitivity_optthr=0.7073, macro_Specificity_optthr=0.8605\n",
      "\n",
      "CV averages (Â±95% CI) @0.5:\n",
      "           macro_ACC: 0.7333  (95% CI 0.5739, 0.8927)\n",
      "           macro_AUC: 0.8228  (95% CI 0.7405, 0.9052)\n",
      "         macro_AUPRC: 0.8629  (95% CI 0.7905, 0.9352)\n",
      "         macro_Brier: 0.1901  (95% CI 0.1490, 0.2312)\n",
      "            macro_F1: 0.7233  (95% CI 0.5310, 0.9155)\n",
      "     macro_Precision: 0.8189  (95% CI 0.7182, 0.9196)\n",
      "        macro_Recall: 0.6585  (95% CI 0.4254, 0.8915)\n",
      "   macro_Sensitivity: 0.6585  (95% CI 0.4254, 0.8915)\n",
      "   macro_Specificity: 0.8295  (95% CI 0.7785, 0.8805)\n",
      "          task_1_ACC: 0.7333  (95% CI 0.5739, 0.8927)\n",
      "          task_1_AUC: 0.8228  (95% CI 0.7405, 0.9052)\n",
      "        task_1_AUPRC: 0.8629  (95% CI 0.7905, 0.9352)\n",
      "        task_1_Brier: 0.1901  (95% CI 0.1490, 0.2312)\n",
      "           task_1_F1: 0.7233  (95% CI 0.5310, 0.9155)\n",
      "    task_1_Precision: 0.8189  (95% CI 0.7182, 0.9196)\n",
      "       task_1_Recall: 0.6585  (95% CI 0.4254, 0.8915)\n",
      "  task_1_Sensitivity: 0.6585  (95% CI 0.4254, 0.8915)\n",
      "  task_1_Specificity: 0.8295  (95% CI 0.7785, 0.8805)\n",
      "\n",
      "CV averages (Â±95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.7286  (95% CI 0.6673, 0.7899)\n",
      "    macro_AUC_optthr: 0.8228  (95% CI 0.7405, 0.9052)\n",
      "  macro_AUPRC_optthr: 0.8629  (95% CI 0.7905, 0.9352)\n",
      "  macro_Brier_optthr: 0.1901  (95% CI 0.1490, 0.2312)\n",
      "     macro_F1_optthr: 0.7680  (95% CI 0.7102, 0.8259)\n",
      "macro_Precision_optthr: 0.7471  (95% CI 0.6517, 0.8425)\n",
      " macro_Recall_optthr: 0.8168  (95% CI 0.6302, 1.0035)\n",
      "macro_Sensitivity_optthr: 0.8168  (95% CI 0.6302, 1.0035)\n",
      "macro_Specificity_optthr: 0.5658  (95% CI 0.1999, 0.9316)\n",
      "   task_1_ACC_optthr: 0.7286  (95% CI 0.6673, 0.7899)\n",
      "   task_1_AUC_optthr: 0.8228  (95% CI 0.7405, 0.9052)\n",
      " task_1_AUPRC_optthr: 0.8629  (95% CI 0.7905, 0.9352)\n",
      " task_1_Brier_optthr: 0.1901  (95% CI 0.1490, 0.2312)\n",
      "    task_1_F1_optthr: 0.7680  (95% CI 0.7102, 0.8259)\n",
      "task_1_Precision_optthr: 0.7471  (95% CI 0.6517, 0.8425)\n",
      "task_1_Recall_optthr: 0.8168  (95% CI 0.6302, 1.0035)\n",
      "task_1_Sensitivity_optthr: 0.8168  (95% CI 0.6302, 1.0035)\n",
      "task_1_Specificity_optthr: 0.5658  (95% CI 0.1999, 0.9316)\n",
      "Arch params (used in all folds): {'d_fix_latent': 32, 'd_rand_latent': 32, 'n_kernels': 6, 'dropout': 0.0}\n",
      "Train params (used in all folds): {'lr': 0.0005, 'weight_decay': 3e-05, 'batch_size': 64, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 1e-06, 'lambda_ridge': 1e-05, 'lambda_orth_latent': 3e-05, 'lambda_film_identity': 3e-05, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"cv_only\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"both\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
