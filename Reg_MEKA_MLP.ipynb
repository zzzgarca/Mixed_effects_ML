{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f278667b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1069b",
   "metadata": {},
   "source": [
    "## Root and data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068e5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "root_dir = \"/Users/silviumatu/Desktop/Code/Python/Disertatie/Disertatie_Matu_Silviu_v1\"\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "data_dir = os.path.join(root_dir, \"Data\")\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1166a5e",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d6a9787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_participant_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_gender",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_BDI_TOTAL_pre",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_YSQ_D1_pre",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_YSQ_D2_pre",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_LSAS_ANX_pre",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_LSAS_AVOID_pre",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_day_participant",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_response_within_day_participant",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_time_intervals",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_duration",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "x_q3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q5",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q6",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q7",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q8",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q9",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q10",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q11",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q12",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q13",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_depression",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q2_value_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q2_value_2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q2_value_3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q2_value_4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q2_value_5",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q2_value_6",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_q2_value_7",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_time_intervals_copy",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x_time_difference",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y_dep_score_next",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "3e62f702-7181-4e4f-8ca3-81bd6c7efb20",
       "rows": [
        [
         "0",
         "6",
         "22",
         "1.0",
         "20",
         "133",
         "61",
         "51",
         "47",
         "0",
         "0",
         "0",
         "67.71",
         "5",
         "2",
         "1",
         "1",
         "1",
         "3",
         "3",
         "3",
         "2",
         "1",
         "1",
         "12",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1.0",
         "10.0"
        ],
        [
         "1",
         "6",
         "22",
         "1.0",
         "20",
         "133",
         "61",
         "51",
         "47",
         "0",
         "1",
         "1",
         "59.23",
         "4",
         "2",
         "1",
         "2",
         "1",
         "5",
         "2",
         "5",
         "2",
         "1",
         "1",
         "10",
         "0",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "1.0",
         "2.0"
        ],
        [
         "2",
         "6",
         "22",
         "1.0",
         "20",
         "133",
         "61",
         "51",
         "47",
         "0",
         "2",
         "2",
         "41.122",
         "0",
         "5",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "2",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "2",
         "5.0",
         "4.0"
        ],
        [
         "3",
         "6",
         "22",
         "1.0",
         "20",
         "133",
         "61",
         "51",
         "47",
         "1",
         "0",
         "7",
         "46.619",
         "2",
         "3",
         "1",
         "1",
         "1",
         "1",
         "2",
         "3",
         "1",
         "1",
         "1",
         "4",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "7",
         "3.0",
         "3.0"
        ],
        [
         "4",
         "6",
         "22",
         "1.0",
         "20",
         "133",
         "61",
         "51",
         "47",
         "1",
         "1",
         "10",
         "40.567",
         "0",
         "5",
         "0",
         "0",
         "0",
         "3",
         "0",
         "3",
         "1",
         "1",
         "2",
         "3",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "10",
         "5.0",
         "15.0"
        ]
       ],
       "shape": {
        "columns": 34,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_participant_id</th>\n",
       "      <th>x_age</th>\n",
       "      <th>x_gender</th>\n",
       "      <th>x_BDI_TOTAL_pre</th>\n",
       "      <th>x_YSQ_D1_pre</th>\n",
       "      <th>x_YSQ_D2_pre</th>\n",
       "      <th>x_LSAS_ANX_pre</th>\n",
       "      <th>x_LSAS_AVOID_pre</th>\n",
       "      <th>x_day_participant</th>\n",
       "      <th>x_response_within_day_participant</th>\n",
       "      <th>...</th>\n",
       "      <th>x_q2_value_1</th>\n",
       "      <th>x_q2_value_2</th>\n",
       "      <th>x_q2_value_3</th>\n",
       "      <th>x_q2_value_4</th>\n",
       "      <th>x_q2_value_5</th>\n",
       "      <th>x_q2_value_6</th>\n",
       "      <th>x_q2_value_7</th>\n",
       "      <th>x_time_intervals_copy</th>\n",
       "      <th>x_time_difference</th>\n",
       "      <th>y_dep_score_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>61</td>\n",
       "      <td>51</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>61</td>\n",
       "      <td>51</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>61</td>\n",
       "      <td>51</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>61</td>\n",
       "      <td>51</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>133</td>\n",
       "      <td>61</td>\n",
       "      <td>51</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_participant_id  x_age  x_gender  x_BDI_TOTAL_pre  x_YSQ_D1_pre  \\\n",
       "0                 6     22       1.0               20           133   \n",
       "1                 6     22       1.0               20           133   \n",
       "2                 6     22       1.0               20           133   \n",
       "3                 6     22       1.0               20           133   \n",
       "4                 6     22       1.0               20           133   \n",
       "\n",
       "   x_YSQ_D2_pre  x_LSAS_ANX_pre  x_LSAS_AVOID_pre  x_day_participant  \\\n",
       "0            61              51                47                  0   \n",
       "1            61              51                47                  0   \n",
       "2            61              51                47                  0   \n",
       "3            61              51                47                  1   \n",
       "4            61              51                47                  1   \n",
       "\n",
       "   x_response_within_day_participant  ...  x_q2_value_1  x_q2_value_2  \\\n",
       "0                                  0  ...             0             0   \n",
       "1                                  1  ...             0             1   \n",
       "2                                  2  ...             1             0   \n",
       "3                                  0  ...             0             0   \n",
       "4                                  1  ...             1             0   \n",
       "\n",
       "   x_q2_value_3  x_q2_value_4  x_q2_value_5  x_q2_value_6  x_q2_value_7  \\\n",
       "0             1             0             0             0             0   \n",
       "1             0             0             0             0             0   \n",
       "2             0             0             0             0             0   \n",
       "3             0             1             0             0             0   \n",
       "4             0             0             0             0             0   \n",
       "\n",
       "   x_time_intervals_copy  x_time_difference  y_dep_score_next  \n",
       "0                      0                1.0              10.0  \n",
       "1                      1                1.0               2.0  \n",
       "2                      2                5.0               4.0  \n",
       "3                      7                3.0               3.0  \n",
       "4                     10                5.0              15.0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_reg_df = pd.read_csv(os.path.join(data_dir, \"EXP_regression_data_forecast.csv\"))\n",
    "columns_EXP_reg_df = pd.read_csv(os.path.join(data_dir, \"columns_EXP_regression_data_forecast_S.csv\"))\n",
    "\n",
    "EXP_reg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c174fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the outcome column(s) marked with 1 in the \"outcomes\" column of columns_EXP_reg_df\n",
    "EXP_reg_outcome_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['outcomes'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_y = EXP_reg_df[EXP_reg_outcome_cols]\n",
    "EXP_reg_y.head()\n",
    "\n",
    "# Same for outcomes lags column(s)\n",
    "EXP_reg_outcomes_lags_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['outcomes_lags'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_outcomes_lags = EXP_reg_df[EXP_reg_outcomes_lags_cols]\n",
    "\n",
    "# Same for participant column(s)\n",
    "EXP_reg_participant_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['participant_id'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_participant_id = EXP_reg_df[EXP_reg_participant_cols]\n",
    "\n",
    "# Same for time column(s)\n",
    "EXP_reg_time_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['time'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_time = EXP_reg_df[EXP_reg_time_cols]\n",
    "\n",
    "# Same for forecast horizons column(s)\n",
    "EXP_reg_forecast_horizons_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['forecast_horizons'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_forecast_horizons = EXP_reg_df[EXP_reg_forecast_horizons_cols]\n",
    "\n",
    "# Same for fixed effects column(s)\n",
    "EXP_reg_only_fixed_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['only_fixed'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_only_fixed = EXP_reg_df[EXP_reg_only_fixed_cols]\n",
    "\n",
    "# Same for random effects column(s)\n",
    "EXP_reg_fixed_and_random_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['fixed_and_random'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_fixed_and_random = EXP_reg_df[EXP_reg_fixed_and_random_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b382d0b",
   "metadata": {},
   "source": [
    "# Custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8845d70a",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e00bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "\n",
    "# If efficient_kan is present, we keep the import for API compatibility,\n",
    "# but this MLP backbone does NOT use KAN layers in the forward path.\n",
    "try:\n",
    "    from efficient_kan import KANLinear  # noqa: F401\n",
    "except Exception:\n",
    "    class KANLinear(nn.Module):  # stub to keep .regularization_loss calls safe\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__()\n",
    "        def forward(self, x): return x\n",
    "        def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "            return torch.tensor(0.0, device=x.device if (x := torch.tensor(0.)).is_cuda else torch.device(\"cpu\"))\n",
    "\n",
    "# --- MLP replacement for KAN factory (signature preserved) ---\n",
    "def _make_kan(in_dim, out_dim,\n",
    "              grid_size=8, spline_order=3,\n",
    "              scale_noise=0.1, scale_base=1.0, scale_spline=1.0,\n",
    "              enable_standalone_scale_spline=True,\n",
    "              base_activation=torch.nn.SiLU,\n",
    "              grid_eps=0.02, grid_range=(-1.0, 1.0)):\n",
    "    # We return a plain Linear layer so callers need no change.\n",
    "    return nn.Linear(in_dim, out_dim)\n",
    "\n",
    "# --- MLP block keeping the KANBlock API and ctor signature ---\n",
    "class KANBlock(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims=(128, 64), out_dim=None,\n",
    "                 dropout=0.0,\n",
    "                 grid_size=8, spline_order=3,\n",
    "                 scale_noise=0.1, scale_base=1.0, scale_spline=1.0,\n",
    "                 enable_standalone_scale_spline=True,\n",
    "                 base_activation=torch.nn.SiLU,\n",
    "                 grid_eps=0.02, grid_range=(-1.0, 1.0)):\n",
    "        super().__init__()\n",
    "        dims = [in_dim] + list(hidden_dims)\n",
    "        act = base_activation()\n",
    "        layers: List[nn.Module] = []\n",
    "        for d0, d1 in zip(dims[:-1], dims[1:]):\n",
    "            layers.append(nn.Linear(d0, d1))\n",
    "            layers.append(act)\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "        if out_dim is not None:\n",
    "            layers.append(nn.Linear(dims[-1], out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        # Keep the method for wrapper compatibility; returns 0 for MLP.\n",
    "        reg = 0.0\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, KANLinear):\n",
    "                reg = reg + m.regularization_loss(\n",
    "                    regularize_activation=regularize_activation,\n",
    "                    regularize_entropy=regularize_entropy\n",
    "                )\n",
    "        return reg\n",
    "\n",
    "# --- Attention branch unchanged in interface; internal blocks are MLP now ---\n",
    "class TemporalKernelAttentionKAN(nn.Module):\n",
    "    def __init__(self, n_kernels: int = 4, d_att: int = 32,\n",
    "                 grid_size=8, spline_order=3, dropout=0.0,\n",
    "                 normalize_weights: bool = False):\n",
    "        super().__init__()\n",
    "        self.n_k = n_kernels\n",
    "        self.normalize = normalize_weights\n",
    "        self.pi_logits = nn.Parameter(torch.zeros(n_kernels))\n",
    "        self.lam_raw = nn.Parameter(torch.zeros(n_kernels))\n",
    "        self.summarize = KANBlock(1, hidden_dims=(d_att,), out_dim=d_att,\n",
    "                                  dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "        self.out_head = KANBlock(d_att, hidden_dims=(d_att,), out_dim=1,\n",
    "                                 dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, y_lags, dt_lags):\n",
    "        B, L = y_lags.shape\n",
    "        pi = F.softmax(self.pi_logits, dim=-1)\n",
    "        lam = F.softplus(self.lam_raw) + 1e-6\n",
    "        pi_exp = pi.view(1, 1, -1).expand(B, 1, -1)\n",
    "        lam_exp = lam.view(1, 1, -1).expand(B, 1, -1)\n",
    "        kernel = torch.exp(-lam_exp * dt_lags.unsqueeze(-1))\n",
    "        w_lags = torch.sum(pi_exp * kernel, dim=-1)\n",
    "        if self.normalize:\n",
    "            w_sum = w_lags.sum(dim=1, keepdim=True) + 1e-8\n",
    "            w_lags = w_lags / w_sum\n",
    "        s = torch.sum(w_lags * y_lags, dim=1, keepdim=True)\n",
    "        z_att = self.summarize(s)\n",
    "        e_att = self.out_head(z_att)\n",
    "        return e_att, w_lags, z_att\n",
    "\n",
    "# --- Fixed, Random, TC, Head, FiLM blocks: MLP versions with same APIs ---\n",
    "class FixedBranchKAN(nn.Module):\n",
    "    def __init__(self, d_fix, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = KANBlock(d_fix, hidden_dims=(256, 128), out_dim=d_latent,\n",
    "                            dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "        self.head = KANBlock(d_latent, hidden_dims=(64,), out_dim=1,\n",
    "                             dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, X_fix):\n",
    "        z = self.enc(X_fix)\n",
    "        e = self.head(z)\n",
    "        return e, z\n",
    "\n",
    "class RandEncoderKAN(nn.Module):\n",
    "    def __init__(self, d_zrand, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = KANBlock(d_zrand, hidden_dims=(256, 128), out_dim=d_latent,\n",
    "                            dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "    def forward(self, Zrand):\n",
    "        return self.enc(Zrand)\n",
    "\n",
    "class TCEncoderKAN(nn.Module):\n",
    "    def __init__(self, d_tc, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = KANBlock(d_tc, hidden_dims=(256, 128), out_dim=d_latent,\n",
    "                            dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "    def forward(self, TC):\n",
    "        return self.enc(TC)\n",
    "\n",
    "class RandomHeadKAN(nn.Module):\n",
    "    def __init__(self, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.head = KANBlock(d_latent, hidden_dims=(64,), out_dim=1,\n",
    "                             dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "    def forward(self, z):\n",
    "        return self.head(z)\n",
    "\n",
    "class FiLMFromTC(nn.Module):\n",
    "    def __init__(self, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.gamma = KANBlock(d_latent, hidden_dims=(64,), out_dim=d_latent,\n",
    "                              dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "        self.beta  = KANBlock(d_latent, hidden_dims=(64,), out_dim=d_latent,\n",
    "                              dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "    def forward(self, e_tc):\n",
    "        return self.gamma(e_tc), self.beta(e_tc)\n",
    "\n",
    "# --- Main model: unchanged API & flags; backbone uses the MLP blocks above ---\n",
    "class KANAdditiveMixedEffects(nn.Module):\n",
    "    def __init__(self,\n",
    "                 y_dim: int,\n",
    "                 d_fix: int,\n",
    "                 d_tc: int,\n",
    "                 d_zrand: int,\n",
    "                 n_ids: int = None,\n",
    "                 use_id_intercept: bool = True,\n",
    "                 n_kernels: int = 4,\n",
    "                 d_att: int = 128,\n",
    "                 d_fix_latent: int = 128,\n",
    "                 d_rand_latent: int = 128,   # keep arg for compatibility\n",
    "                 grid_size: int = 8,\n",
    "                 spline_order: int = 3,\n",
    "                 dropout: float = 0.0,\n",
    "                 normalize_att_weights: bool = True,\n",
    "                 # mixing controls (kept exactly as in your version)\n",
    "                 mix_mode: str = \"softmax\",           # \"softmax\" | \"softplus\" | \"sigmoid\"\n",
    "                 per_task_mixing: bool = True,        # per-output weights vs global\n",
    "                 init_mixture: Optional[Tuple[float, float, float]] = None):\n",
    "        super().__init__()\n",
    "        self.y_dim = y_dim\n",
    "        self.d_fix = d_fix\n",
    "        self.d_tc  = d_tc\n",
    "        self.d_zr  = d_zrand\n",
    "        self.d_rand_latent = d_rand_latent\n",
    "\n",
    "        # Branches\n",
    "        self.att_branches = nn.ModuleList([\n",
    "            TemporalKernelAttentionKAN(n_kernels=n_kernels, d_att=d_att,\n",
    "                                       grid_size=grid_size, spline_order=spline_order,\n",
    "                                       dropout=dropout, normalize_weights=normalize_att_weights)\n",
    "            for _ in range(y_dim)\n",
    "        ])\n",
    "        self.fix_branches = nn.ModuleList([\n",
    "            FixedBranchKAN(d_fix=d_fix, d_latent=d_fix_latent,\n",
    "                           grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "            for _ in range(y_dim)\n",
    "        ])\n",
    "        self.rand_encoders = nn.ModuleList([\n",
    "            RandEncoderKAN(d_zrand=d_zrand, d_latent=d_rand_latent,\n",
    "                           grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "            for _ in range(y_dim)\n",
    "        ]) if d_zrand > 0 else None\n",
    "        self.tc_encoders = nn.ModuleList([\n",
    "            TCEncoderKAN(d_tc=d_tc, d_latent=d_rand_latent,\n",
    "                         grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "            for _ in range(y_dim)\n",
    "        ]) if d_tc > 0 else None\n",
    "        self.film_from_tc = nn.ModuleList([\n",
    "            FiLMFromTC(d_latent=d_rand_latent, grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "            for _ in range(y_dim)\n",
    "        ]) if d_tc > 0 else None\n",
    "        self.rand_heads = nn.ModuleList([\n",
    "            RandomHeadKAN(d_latent=d_rand_latent, grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "            for _ in range(y_dim)\n",
    "        ]) if (self.rand_encoders is not None) else None\n",
    "\n",
    "        # Mixing config (kept as-is)\n",
    "        self.mix_mode = mix_mode\n",
    "        self.per_task_mixing = per_task_mixing\n",
    "        size = (y_dim, 3) if per_task_mixing else (1, 3)\n",
    "\n",
    "        mix_logits = torch.zeros(size)\n",
    "        if init_mixture is not None:\n",
    "            a, f, r = init_mixture\n",
    "            eps = 1e-8\n",
    "            probs = torch.tensor([a, f, r], dtype=mix_logits.dtype).clamp_min(eps)\n",
    "            probs = probs / probs.sum()\n",
    "            mix_logits = mix_logits + probs.log()  # broadcast\n",
    "        self.mix_logits = nn.Parameter(mix_logits)\n",
    "\n",
    "    def forward(self, X_fix, TC, Zrand, y_lags, dt_lags,\n",
    "                pid_idx: Optional[torch.Tensor] = None,\n",
    "                pid_seen_mask: Optional[torch.Tensor] = None):\n",
    "        B = X_fix.size(0)\n",
    "\n",
    "        # Per-task lag tensors\n",
    "        if y_lags.dim() == 2:\n",
    "            y_lags_list = [y_lags for _ in range(self.y_dim)]\n",
    "        elif y_lags.dim() == 3:\n",
    "            assert y_lags.size(2) == self.y_dim\n",
    "            y_lags_list = [y_lags[:, :, j] for j in range(self.y_dim)]\n",
    "        else:\n",
    "            raise ValueError(\"y_lags must be [B, L] or [B, L, y_dim].\")\n",
    "\n",
    "        e_att_all, e_fix_all, e_rand_all = [], [], []\n",
    "        z_att_list, z_fix_list = [], []\n",
    "        z_rand_list, z_rand_film_list, z_tc_list = [], [], []\n",
    "        w_lags_list, e_rand_mod_list, gamma_list, beta_list = [], [], [], []\n",
    "\n",
    "        for j in range(self.y_dim):\n",
    "            # attention over lags\n",
    "            e_att_j, w_lags_j, z_att_j = self.att_branches[j](y_lags_list[j], dt_lags)\n",
    "\n",
    "            # fixed branch\n",
    "            e_fix_j, z_fix_j = self.fix_branches[j](X_fix)\n",
    "\n",
    "            # random encoder (or zeros with correct latent width)\n",
    "            if (self.rand_encoders is not None) and (Zrand is not None) and (Zrand.size(1) > 0):\n",
    "                z_rand_j = self.rand_encoders[j](Zrand)\n",
    "            else:\n",
    "                z_rand_j = torch.zeros(B, self.d_rand_latent, device=X_fix.device, dtype=X_fix.dtype)\n",
    "\n",
    "            # TC -> FiLM (or identity if no TC)\n",
    "            if (self.tc_encoders is not None) and (TC is not None) and (TC.size(1) > 0):\n",
    "                z_tc_j = self.tc_encoders[j](TC)\n",
    "                gamma_j, beta_j = self.film_from_tc[j](z_tc_j) if self.film_from_tc is not None else (torch.ones_like(z_rand_j), torch.zeros_like(z_rand_j))\n",
    "            else:\n",
    "                z_tc_j = torch.zeros_like(z_rand_j)\n",
    "                gamma_j = torch.ones_like(z_rand_j)\n",
    "                beta_j  = torch.zeros_like(z_rand_j)\n",
    "\n",
    "            # apply FiLM to random latent then head\n",
    "            z_tilde_j = gamma_j * z_rand_j + beta_j\n",
    "            if self.rand_heads is not None:\n",
    "                e_rand_j  = self.rand_heads[j](z_tilde_j)\n",
    "            else:\n",
    "                e_rand_j  = torch.zeros(B, 1, device=X_fix.device, dtype=X_fix.dtype)\n",
    "\n",
    "            # collect\n",
    "            e_att_all.append(e_att_j)\n",
    "            e_fix_all.append(e_fix_j)\n",
    "            e_rand_all.append(e_rand_j)\n",
    "\n",
    "            z_att_list.append(z_att_j)\n",
    "            z_fix_list.append(z_fix_j)\n",
    "            z_rand_list.append(z_rand_j)\n",
    "            z_rand_film_list.append(z_tilde_j)\n",
    "            z_tc_list.append(z_tc_j)\n",
    "\n",
    "            w_lags_list.append(w_lags_j)\n",
    "            e_rand_mod_list.append(e_rand_j)\n",
    "            gamma_list.append(gamma_j)\n",
    "            beta_list.append(beta_j)\n",
    "\n",
    "        # components\n",
    "        e_att = torch.cat(e_att_all, dim=1) if e_att_all else torch.zeros(B, self.y_dim, device=X_fix.device, dtype=X_fix.dtype)\n",
    "        e_fix = torch.cat(e_fix_all, dim=1) if e_fix_all else torch.zeros(B, self.y_dim, device=X_fix.device, dtype=X_fix.dtype)\n",
    "        e_rand= torch.cat(e_rand_all,dim=1) if e_rand_all else torch.zeros(B, self.y_dim, device=X_fix.device, dtype=X_fix.dtype)\n",
    "\n",
    "        # mixing weights\n",
    "        W = self.mix_logits\n",
    "        if W.size(0) == 1:\n",
    "            W = W.expand(self.y_dim, -1)\n",
    "        if self.mix_mode == \"softmax\":\n",
    "            W = F.softmax(W, dim=-1)\n",
    "        elif self.mix_mode == \"softplus\":\n",
    "            W = F.softplus(W)\n",
    "        elif self.mix_mode == \"sigmoid\":\n",
    "            W = torch.sigmoid(W)\n",
    "        else:\n",
    "            raise ValueError(\"mix_mode must be one of {'softmax','softplus','sigmoid'}\")\n",
    "\n",
    "        e_stack = torch.stack([e_att, e_fix, e_rand], dim=-1)  # (B, y_dim, 3)\n",
    "        y_hat = (e_stack * W.unsqueeze(0)).sum(dim=-1)         # (B, y_dim)\n",
    "\n",
    "        parts: Dict[str, Any] = {\n",
    "            \"e_att\": e_att,\n",
    "            \"e_fix\": e_fix,\n",
    "            \"e_rand\": e_rand,\n",
    "            \"z_att_list\": z_att_list,\n",
    "            \"z_fix_list\": z_fix_list,\n",
    "            \"z_rand_list\": z_rand_list,\n",
    "            \"z_rand_film_list\": z_rand_film_list,\n",
    "            \"z_tc_list\": z_tc_list,\n",
    "            \"w_lags_list\": w_lags_list,\n",
    "            \"e_rand_mod_list\": e_rand_mod_list,\n",
    "            \"film_gamma_list\": gamma_list,\n",
    "            \"film_beta_list\": beta_list,\n",
    "            \"mix_weights\": W,   # (y_dim, 3) columns: [att, fix, rand]\n",
    "        }\n",
    "        return y_hat, parts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba99af",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b19c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "# ---- helpers (no metrics) ----\n",
    "def _regression_loss(\n",
    "    y_hat: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    loss: str = \"mse\",\n",
    "    huber_delta: float = 1.0\n",
    ") -> torch.Tensor:\n",
    "    # allow [B] vs [B,1]\n",
    "    if y_true.shape != y_hat.shape:\n",
    "        if y_true.dim() == 1 and y_hat.dim() == 2 and y_hat.size(1) == 1:\n",
    "            y_true = y_true.unsqueeze(1)\n",
    "        else:\n",
    "            raise ValueError(f\"y_true {y_true.shape} must equal y_hat {y_hat.shape}\")\n",
    "    if loss == \"mse\":   return F.mse_loss(y_hat, y_true)\n",
    "    if loss == \"mae\":   return F.l1_loss(y_hat, y_true)\n",
    "    if loss == \"huber\": return F.huber_loss(y_hat, y_true, delta=huber_delta)\n",
    "    raise ValueError(\"loss must be one of {'mse','mae','huber'}\")\n",
    "\n",
    "def _orthogonality_penalty_latents(z_fix: torch.Tensor, z_rand_film: torch.Tensor) -> torch.Tensor:\n",
    "    B = z_fix.size(0)\n",
    "    if B <= 1:\n",
    "        return z_fix.new_zeros(())\n",
    "    zf = z_fix - z_fix.mean(dim=0, keepdim=True)\n",
    "    zr = z_rand_film - z_rand_film.mean(dim=0, keepdim=True)\n",
    "    M = (zf.T @ zr) / float(B)\n",
    "    return (M ** 2).mean()\n",
    "\n",
    "class KANMixedEffectsRegWrapper:\n",
    "    \"\"\"\n",
    "    Thin training/inference wrapper for regression KANAdditiveMixedEffects.\n",
    "    No metrics here—evaluation code will compute them.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 cfg: Optional[Dict[str, Any]] = None,\n",
    "                 device: Optional[torch.device] = None):\n",
    "        self.model = model\n",
    "        self.cfg = {\n",
    "            # regularization strengths\n",
    "            \"lambda_mean0\": 1e-4,\n",
    "            \"lambda_ridge\": 1e-4,\n",
    "            \"lambda_orth_latent\": 1e-3,\n",
    "            \"lambda_film_identity\": 1e-4,\n",
    "            \"lambda_kan\": 0.0,\n",
    "            \"kan_reg_activation\": 1.0,\n",
    "            \"kan_reg_entropy\": 1.0,\n",
    "            # optimization\n",
    "            \"clip_grad\": 5.0,\n",
    "            \"lr\": 1e-3,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"max_epochs\": 100,\n",
    "            \"patience\": 10,\n",
    "            \"batch_size\": 256,\n",
    "            \"amp\": True,   # CUDA AMP\n",
    "            # regression loss\n",
    "            \"loss\": \"mse\",     # \"mse\" | \"mae\" | \"huber\"\n",
    "            \"huber_delta\": 1.0,\n",
    "        }\n",
    "        if cfg:\n",
    "            self.cfg.update(cfg)\n",
    "\n",
    "        self.device = (\n",
    "            device\n",
    "            or (torch.device(\"mps\") if torch.backends.mps.is_available() else None)\n",
    "            or (torch.device(\"cuda\") if torch.cuda.is_available() else None)\n",
    "            or torch.device(\"cpu\")\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.history_: Dict[str, list] = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    def _sum_kan_regularization(self) -> torch.Tensor:\n",
    "        try:\n",
    "            from efficient_kan import KANLinear\n",
    "        except Exception:\n",
    "            return torch.tensor(0.0, device=next(self.model.parameters()).device)\n",
    "        reg = torch.tensor(0.0, device=next(self.model.parameters()).device)\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, KANLinear):\n",
    "                reg = reg + m.regularization_loss(\n",
    "                    regularize_activation=self.cfg[\"kan_reg_activation\"],\n",
    "                    regularize_entropy=self.cfg[\"kan_reg_entropy\"]\n",
    "                )\n",
    "        return reg\n",
    "\n",
    "    def compute_loss(self,\n",
    "                     y_true: torch.Tensor,\n",
    "                     y_hat: torch.Tensor,\n",
    "                     parts: Dict[str, Any],\n",
    "                     *,\n",
    "                     X_fix: torch.Tensor,\n",
    "                     pid_idx: Optional[torch.Tensor] = None,\n",
    "                     TC: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "        cfg = self.cfg\n",
    "\n",
    "        # 1) supervised regression loss\n",
    "        loss_pred = _regression_loss(y_hat, y_true, loss=cfg[\"loss\"], huber_delta=cfg[\"huber_delta\"])\n",
    "\n",
    "        # 2) random-output regularization\n",
    "        e_rand = parts.get(\"e_rand\", None)\n",
    "        if e_rand is None:\n",
    "            e_rand = torch.zeros_like(y_hat)\n",
    "        loss_mean0 = (e_rand.mean(dim=0) ** 2).sum()\n",
    "        loss_ridge = (e_rand ** 2).mean()\n",
    "\n",
    "        # 3) latent orthogonality (fixed vs FiLM'ed random)\n",
    "        z_fix_list = parts.get(\"z_fix_list\", [])\n",
    "        z_rand_film_list = parts.get(\"z_rand_film_list\", [])\n",
    "        loss_orth = torch.tensor(0.0, device=y_hat.device)\n",
    "        if len(z_fix_list) and len(z_rand_film_list) and len(z_fix_list) == len(z_rand_film_list):\n",
    "            acc = 0.0\n",
    "            for zf, zr in zip(z_fix_list, z_rand_film_list):\n",
    "                acc = acc + _orthogonality_penalty_latents(zf, zr)\n",
    "            loss_orth = acc / float(len(z_fix_list))\n",
    "\n",
    "        # 4) FiLM identity\n",
    "        gamma_list = parts.get(\"film_gamma_list\", [])\n",
    "        beta_list  = parts.get(\"film_beta_list\", [])\n",
    "        loss_film = torch.tensor(0.0, device=y_hat.device)\n",
    "        if len(gamma_list) and len(beta_list):\n",
    "            one = torch.tensor(1.0, device=y_hat.device, dtype=y_hat.dtype)\n",
    "            acc = 0.0\n",
    "            for g, b in zip(gamma_list, beta_list):\n",
    "                acc = acc + ((g - one) ** 2).mean() + (b ** 2).mean()\n",
    "            loss_film = acc / float(len(gamma_list))\n",
    "\n",
    "        # 5) KAN regularization (optional)\n",
    "        kan_reg = torch.tensor(0.0, device=y_hat.device)\n",
    "        if cfg[\"lambda_kan\"] > 0:\n",
    "            kan_reg = self._sum_kan_regularization()\n",
    "\n",
    "        total = (\n",
    "            loss_pred\n",
    "            + cfg[\"lambda_mean0\"] * loss_mean0\n",
    "            + cfg[\"lambda_ridge\"] * loss_ridge\n",
    "            + cfg[\"lambda_orth_latent\"] * loss_orth\n",
    "            + cfg[\"lambda_film_identity\"] * loss_film\n",
    "            + cfg[\"lambda_kan\"] * kan_reg\n",
    "        )\n",
    "        parts_out = {\n",
    "            \"loss_total\": float(total.detach().cpu()),\n",
    "            \"loss_pred\":  float(loss_pred.detach().cpu()),\n",
    "            \"loss_mean0\": float(loss_mean0.detach().cpu()),\n",
    "            \"loss_ridge\": float(loss_ridge.detach().cpu()),\n",
    "            \"loss_orth\":  float(loss_orth.detach().cpu()),\n",
    "            \"loss_fi\":    float(loss_film.detach().cpu()),\n",
    "            \"loss_kan\":   float(kan_reg.detach().cpu()),\n",
    "        }\n",
    "        return total, parts_out\n",
    "\n",
    "    def fit(self, train_loader, val_loader=None, *, verbose=True):\n",
    "        cfg = self.cfg\n",
    "        model = self.model\n",
    "        device = self.device\n",
    "\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "\n",
    "        use_amp = bool(cfg.get(\"amp\", True) and torch.cuda.is_available())\n",
    "        scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "        best_val = float(\"inf\")\n",
    "        best_state = None\n",
    "        no_improve = 0\n",
    "\n",
    "        for epoch in range(1, cfg[\"max_epochs\"] + 1):\n",
    "            # --- train ---\n",
    "            model.train()\n",
    "            total_tr, n_tr = 0.0, 0\n",
    "            for batch in train_loader:\n",
    "                Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b = [b.to(device) for b in batch]\n",
    "                TC_in = TC_b if TC_b.size(1) > 0 else None\n",
    "                Zr_in = Zr_b if Zr_b.size(1) > 0 else None\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "                    y_hat, parts = model(\n",
    "                        X_fix=Xf_b, TC=TC_in, Zrand=Zr_in,\n",
    "                        y_lags=yl_b, dt_lags=dt_b,\n",
    "                        pid_idx=pid_b, pid_seen_mask=seen_b\n",
    "                    )\n",
    "                    loss, _ = self.compute_loss(\n",
    "                        y_true=y_b, y_hat=y_hat, parts=parts,\n",
    "                        X_fix=Xf_b, pid_idx=pid_b, TC=TC_in\n",
    "                    )\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                if cfg.get(\"clip_grad\", None):\n",
    "                    scaler.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"clip_grad\"])\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "\n",
    "                bs = Xf_b.size(0)\n",
    "                total_tr += float(loss.detach().cpu()) * bs\n",
    "                n_tr += bs\n",
    "\n",
    "            train_loss = total_tr / max(1, n_tr)\n",
    "            self.history_[\"train_loss\"].append(train_loss)\n",
    "\n",
    "            # --- validate ---\n",
    "            if val_loader is not None:\n",
    "                model.eval()\n",
    "                total_va, n_va = 0.0, 0\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_loader:\n",
    "                        Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b = [b.to(device) for b in batch]\n",
    "                        TC_in = TC_b if TC_b.size(1) > 0 else None\n",
    "                        Zr_in = Zr_b if Zr_b.size(1) > 0 else None\n",
    "                        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "                            y_hat, parts = model(\n",
    "                                X_fix=Xf_b, TC=TC_in, Zrand=Zr_in,\n",
    "                                y_lags=yl_b, dt_lags=dt_b,\n",
    "                                pid_idx=pid_b, pid_seen_mask=seen_b\n",
    "                            )\n",
    "                            l, _ = self.compute_loss(\n",
    "                                y_true=y_b, y_hat=y_hat, parts=parts,\n",
    "                                X_fix=Xf_b, pid_idx=pid_b, TC=TC_in\n",
    "                            )\n",
    "                        bs = Xf_b.size(0)\n",
    "                        total_va += float(l.detach().cpu()) * bs\n",
    "                        n_va += bs\n",
    "                val_loss = total_va / max(1, n_va)\n",
    "                self.history_[\"val_loss\"].append(val_loss)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:03d} | train {train_loss:.6f} | val {val_loss:.6f}\")\n",
    "\n",
    "                if val_loss < best_val - 1e-6:\n",
    "                    best_val = val_loss\n",
    "                    best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= cfg[\"patience\"]:\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping at epoch {epoch:03d} (best val {best_val:.6f})\")\n",
    "                        break\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:03d} | train {train_loss:.6f}\")\n",
    "\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, X_fix, TC, Zrand, y_lags, dt_lags, pid_idx=None, pid_seen_mask=None) -> torch.Tensor:\n",
    "        self.model.eval()\n",
    "        y_hat, _ = self.model(\n",
    "            X_fix=X_fix, TC=TC, Zrand=Zrand,\n",
    "            y_lags=y_lags, dt_lags=dt_lags,\n",
    "            pid_idx=pid_idx, pid_seen_mask=pid_seen_mask\n",
    "        )\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf6f7b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29fa7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Regression-only KAN evaluation (standalone, with fixed time & group-wise splits)\n",
    "# =========================\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GroupKFold, ParameterGrid\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from scipy.stats import t as student_t\n",
    "\n",
    "# ---------- PCA helpers ----------\n",
    "_VAR_EPS = 1e-8\n",
    "_STD_EPS = 1e-6\n",
    "_CLIP_Z  = 8.0\n",
    "\n",
    "@dataclass\n",
    "class PCAPipeline:\n",
    "    keep_mask: np.ndarray\n",
    "    mean_: np.ndarray\n",
    "    scale_: np.ndarray\n",
    "    pca: PCA\n",
    "\n",
    "def _fit_pca_pipeline(X_train: np.ndarray, var_ratio: float = 0.95, random_state: Optional[int] = None) -> PCAPipeline:\n",
    "    X = np.asarray(X_train, dtype=np.float64)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    var = X.var(axis=0)\n",
    "    keep = var > _VAR_EPS\n",
    "    if not np.any(keep):\n",
    "        pca = PCA(n_components=0, svd_solver='full', random_state=random_state)\n",
    "        return PCAPipeline(keep_mask=keep, mean_=np.array([], dtype=np.float64),\n",
    "                           scale_=np.array([], dtype=np.float64), pca=pca)\n",
    "\n",
    "    Xk = X[:, keep]\n",
    "    mean = Xk.mean(axis=0)\n",
    "    std  = np.maximum(Xk.std(axis=0), _STD_EPS)\n",
    "\n",
    "    Z = (Xk - mean) / std\n",
    "    Z = np.nan_to_num(Z, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    np.clip(Z, -_CLIP_Z, _CLIP_Z, out=Z)\n",
    "\n",
    "    pca = PCA(n_components=var_ratio, svd_solver='full', random_state=random_state)\n",
    "    pca.fit(Z)\n",
    "    if not np.isfinite(pca.components_).all():\n",
    "        raise RuntimeError(\"PCA components contain non-finite values after fit.\")\n",
    "\n",
    "    return PCAPipeline(keep_mask=keep, mean_=mean, scale_=std, pca=pca)\n",
    "\n",
    "def _transform_pca_pipeline(pipe: Optional[PCAPipeline], X: Optional[np.ndarray]) -> Optional[np.ndarray]:\n",
    "    if pipe is None or X is None:\n",
    "        return None\n",
    "\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    if pipe.keep_mask.size == 0 or not np.any(pipe.keep_mask):\n",
    "        return np.zeros((X.shape[0], 0), dtype=np.float32)\n",
    "\n",
    "    Xk = X[:, pipe.keep_mask]\n",
    "    Z = (Xk - pipe.mean_) / pipe.scale_\n",
    "    Z = np.nan_to_num(Z, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    np.clip(Z, -_CLIP_Z, _CLIP_Z, out=Z)\n",
    "\n",
    "    Z64 = np.ascontiguousarray(Z, dtype=np.float64)\n",
    "    CT  = np.ascontiguousarray(pipe.pca.components_.T, dtype=np.float64)\n",
    "    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n",
    "        Xt = Z64 @ CT\n",
    "    Xt = np.nan_to_num(Xt, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return Xt\n",
    "\n",
    "def _concat_safe(*arrays: Optional[np.ndarray]) -> np.ndarray:\n",
    "    parts = [a for a in arrays if a is not None and a.size > 0]\n",
    "    if not parts:\n",
    "        return np.zeros((0, 0), dtype=np.float32)\n",
    "    return np.concatenate(parts, axis=1).astype(np.float32)\n",
    "\n",
    "# ---------- Regression metrics ----------\n",
    "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    if y_true.ndim == 1: y_true = y_true[:, None]\n",
    "    if y_pred.ndim == 1: y_pred = y_pred[:, None]\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    out = {}\n",
    "    for j in range(y_true.shape[1]):\n",
    "        t = y_true[:, j]; p = y_pred[:, j]\n",
    "        mse = float(np.mean((p - t) ** 2))\n",
    "        rmse = float(np.sqrt(mse))\n",
    "        mae = float(np.mean(np.abs(p - t)))\n",
    "        var = float(np.var(t))\n",
    "        r2 = float(1.0 - (mse / var)) if var > 0 else np.nan\n",
    "        if np.std(t) > 0 and np.std(p) > 0:\n",
    "            r = float(np.corrcoef(t, p)[0, 1])\n",
    "        else:\n",
    "            r = np.nan\n",
    "        out[f\"task_{j+1}_MSE\"] = mse\n",
    "        out[f\"task_{j+1}_RMSE\"] = rmse\n",
    "        out[f\"task_{j+1}_MAE\"] = mae\n",
    "        out[f\"task_{j+1}_R2\"] = r2\n",
    "        out[f\"task_{j+1}_PearsonR\"] = r\n",
    "    for k in (\"MSE\",\"RMSE\",\"MAE\",\"R2\",\"PearsonR\"):\n",
    "        vals = [out[f\"task_{j+1}_{k}\"] for j in range(y_true.shape[1])]\n",
    "        out[f\"macro_{k}\"] = float(np.nanmean(vals))\n",
    "    return out\n",
    "\n",
    "def _print_regression_metrics(metrics: dict, title: str = \"Test metrics\"):\n",
    "    print(f\"\\n{title}:\")\n",
    "    macro_keys = [k for k in metrics.keys() if k.startswith(\"macro_\")]\n",
    "    for k in sorted(macro_keys):\n",
    "        print(f\"{k:>16}: {metrics[k]:.6f}\")\n",
    "    task_indices = sorted({int(k.split('_')[1]) for k in metrics.keys() if k.startswith(\"task_\")})\n",
    "    for j in task_indices:\n",
    "        print(f\"task_{j}: \" +\n",
    "              \", \".join(f\"{m}={metrics.get(f'task_{j}_{m}', np.nan):.6f}\"\n",
    "                        for m in (\"MSE\",\"RMSE\",\"MAE\",\"R2\",\"PearsonR\")))\n",
    "\n",
    "# ---------- Split helpers ----------\n",
    "def _split_cases(pid_array, test_fraction=0.2, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    u = np.unique(pid_array)\n",
    "    te_ids = rng.choice(u, size=max(1, int(len(u)*test_fraction)), replace=False)\n",
    "    te_mask = np.isin(pid_array, te_ids)\n",
    "    return np.where(~te_mask)[0], np.where(te_mask)[0]\n",
    "\n",
    "def _make_train_val_split(idx_array: np.ndarray, seed: int, val_frac: float) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "    idx_array = np.asarray(idx_array)\n",
    "    if len(idx_array) <= 10 or val_frac <= 0.0:\n",
    "        return idx_array, None\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm = rng.permutation(len(idx_array))\n",
    "    cut = max(1, int(val_frac * len(idx_array)))\n",
    "    va_sel, tr_sel = perm[:cut], perm[cut:]\n",
    "    return idx_array[tr_sel], idx_array[va_sel]\n",
    "\n",
    "def _make_train_val_split_cases(idx_array: np.ndarray, pid_idx: np.ndarray, seed: int, val_frac: float) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "    \"\"\"Group-wise validation: pick whole participants into val.\"\"\"\n",
    "    idx_array = np.asarray(idx_array)\n",
    "    if len(idx_array) <= 10 or val_frac <= 0.0:\n",
    "        return idx_array, None\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pids_tr = pid_idx[idx_array]\n",
    "    uniq = np.unique(pids_tr)\n",
    "    n_val_ids = max(1, int(val_frac * len(uniq)))\n",
    "    val_ids = rng.choice(uniq, size=n_val_ids, replace=False)\n",
    "    is_val = np.isin(pids_tr, val_ids)\n",
    "    return idx_array[~is_val], idx_array[is_val]\n",
    "\n",
    "def _split_time_last_fraction_per_subject(pid_idx, time_ix, test_fraction=0.30, min_meas=4):\n",
    "    \"\"\"Forward split within subject: last fraction -> test, earlier -> train.\"\"\"\n",
    "    pid = np.asarray(pid_idx); t = np.asarray(time_ix)\n",
    "    tr, te = [], []\n",
    "    for p in np.unique(pid):\n",
    "        idx = np.where(pid == p)[0]\n",
    "        order = idx[np.argsort(t[idx])]\n",
    "        if len(order) < min_meas:\n",
    "            tr.extend(order.tolist())      # keep entirely in train\n",
    "            continue\n",
    "        k = max(1, int(np.ceil(test_fraction * len(order))))\n",
    "        te.extend(order[-k:].tolist())     # last 30% -> test\n",
    "        tr.extend(order[:-k].tolist())     # rest -> train\n",
    "    return np.array(tr, int), np.array(te, int)\n",
    "\n",
    "def _time_kfold_splits_by_subject_global(pid_idx, time_ix, n_splits=5, min_points=4, indices: Optional[np.ndarray] = None):\n",
    "    \"\"\"\n",
    "    Per-subject contiguous K-fold time splits.\n",
    "    Returns list of (train_idx, test_idx) GLOBAL indices.\n",
    "    Subjects with < max(min_points, n_splits) contribute all rows to TRAIN for all folds.\n",
    "    \"\"\"\n",
    "    pid_idx = np.asarray(pid_idx); time_ix = np.asarray(time_ix)\n",
    "    if indices is None:\n",
    "        indices = np.arange(len(pid_idx), dtype=int)\n",
    "    else:\n",
    "        indices = np.asarray(indices, dtype=int)\n",
    "\n",
    "    # group rows by pid inside 'indices'\n",
    "    by_pid = {}\n",
    "    for p in np.unique(pid_idx[indices]):\n",
    "        rows = indices[pid_idx[indices] == p]\n",
    "        by_pid[p] = rows[np.argsort(time_ix[rows])]\n",
    "\n",
    "    folds = []\n",
    "    for i in range(n_splits):\n",
    "        test_rows = []\n",
    "        train_rows = []\n",
    "        for p, rows in by_pid.items():\n",
    "            n_i = len(rows)\n",
    "            if n_i >= max(min_points, n_splits):\n",
    "                cuts = np.floor(np.linspace(0, n_i, n_splits + 1)).astype(int)\n",
    "                a, b = cuts[i], cuts[i+1]\n",
    "                if b > a:\n",
    "                    test_rows.extend(rows[a:b].tolist())\n",
    "                # the rest go to train\n",
    "                keep_train = np.r_[0:a, b:n_i]\n",
    "                if keep_train.size:\n",
    "                    train_rows.extend(rows[keep_train].tolist())\n",
    "            else:\n",
    "                # not enough points: use all as train for this fold\n",
    "                train_rows.extend(rows.tolist())\n",
    "\n",
    "        test_rows = np.array(sorted(set(test_rows)), dtype=int)\n",
    "        train_rows = np.array(sorted(set(train_rows)), dtype=int)\n",
    "        folds.append((train_rows, test_rows))\n",
    "    return folds\n",
    "\n",
    "def _filter_time_test_min_measurements(pid_idx: np.ndarray, test_idx: np.ndarray, min_meas: int = 3):\n",
    "    pid = np.asarray(pid_idx)\n",
    "    counts = {pid_val: np.sum(pid == pid_val) for pid_val in np.unique(pid)}\n",
    "    keep = [i for i in test_idx if counts.get(pid[i], 0) >= min_meas]\n",
    "    return np.array(keep, dtype=int)\n",
    "\n",
    "# ---------- Dataset / Loader ----------\n",
    "class _KANDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns: (X_fix, TC, Zrand, y, y_lags, dt_lags, pid_idx, pid_seen_mask)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_fix: np.ndarray,\n",
    "        TC: Optional[np.ndarray],\n",
    "        Zrand: Optional[np.ndarray],\n",
    "        y: np.ndarray,\n",
    "        y_lags: np.ndarray,\n",
    "        dt_lags: np.ndarray,\n",
    "        pid_idx: np.ndarray,\n",
    "        pid_seen_mask: np.ndarray,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        N = X_fix.shape[0]\n",
    "        assert y.shape[0] == N and y_lags.shape[0] == N and dt_lags.shape[0] == N and pid_idx.shape[0] == N\n",
    "\n",
    "        def _to_tensor(a, dtype=torch.float32):\n",
    "            if a is None:\n",
    "                return torch.zeros((N, 0), dtype=dtype, device=device)\n",
    "            return torch.as_tensor(a, dtype=dtype, device=device)\n",
    "\n",
    "        self.X_fix  = _to_tensor(X_fix, torch.float32)\n",
    "        self.TC     = _to_tensor(TC,    torch.float32)\n",
    "        self.Zrand  = _to_tensor(Zrand, torch.float32)\n",
    "        self.y      = _to_tensor(y,     torch.float32)\n",
    "        self.y_lags = _to_tensor(y_lags, torch.float32)\n",
    "        self.dt     = _to_tensor(dt_lags, torch.float32)\n",
    "        self.pid    = torch.as_tensor(pid_idx, dtype=torch.long, device=device)\n",
    "        self.seen   = torch.as_tensor(pid_seen_mask.astype(bool), dtype=torch.bool, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X_fix.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            self.X_fix[i], self.TC[i], self.Zrand[i],\n",
    "            self.y[i], self.y_lags[i], self.dt[i],\n",
    "            self.pid[i], self.seen[i]\n",
    "        )\n",
    "\n",
    "def _make_loader_kan(\n",
    "    X_fix, TC, Zrand, y, y_lags, dt_lags,\n",
    "    pid_idx, pid_seen_mask,\n",
    "    batch_size: int,\n",
    "    shuffle: bool,\n",
    "    device: torch.device\n",
    ") -> DataLoader:\n",
    "    ds = _KANDataset(X_fix, TC, Zrand, y, y_lags, dt_lags, pid_idx, pid_seen_mask, device)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, drop_last=False, num_workers=0)\n",
    "\n",
    "# ---------- Prepare split & loaders ----------\n",
    "def _prepare_split_and_loaders_kan(\n",
    "    X_only_fixed: np.ndarray,\n",
    "    X_fixed_and_random: Optional[np.ndarray],\n",
    "    y: np.ndarray,\n",
    "    y_lags: np.ndarray,\n",
    "    dt_lags: np.ndarray,\n",
    "    pid_idx_full: np.ndarray,\n",
    "    indices_train: np.ndarray,\n",
    "    indices_val: Optional[np.ndarray],\n",
    "    indices_test: np.ndarray,\n",
    "    scenario: str,\n",
    "    batch_size: int,\n",
    "    device: torch.device,\n",
    "    random_state: int = 42,\n",
    "    pca_var_ratio: float = 0.95,\n",
    "):\n",
    "    # PCA fit on TRAIN only\n",
    "    of_pipe = _fit_pca_pipeline(X_only_fixed[indices_train], var_ratio=pca_var_ratio, random_state=random_state)\n",
    "    fr_pipe = None\n",
    "    if X_fixed_and_random is not None and X_fixed_and_random.shape[1] > 0:\n",
    "        fr_pipe = _fit_pca_pipeline(X_fixed_and_random[indices_train], var_ratio=pca_var_ratio, random_state=random_state)\n",
    "\n",
    "    def transform_block(idxs):\n",
    "        of = _transform_pca_pipeline(of_pipe, X_only_fixed[idxs])                              # OF\n",
    "        fr = _transform_pca_pipeline(fr_pipe, None if X_fixed_and_random is None else X_fixed_and_random[idxs])  # FR\n",
    "        X_fix = _concat_safe(of, fr)   # FIXED = OF ⊕ FR\n",
    "        TC    = of                     # TC    = OF   (for FiLM input)\n",
    "        Zr    = fr                     # RANDOM = FR\n",
    "        return X_fix, TC, Zr\n",
    "\n",
    "    Xf_tr, TC_tr, Zr_tr = transform_block(indices_train)\n",
    "    Xf_te, TC_te, Zr_te = transform_block(indices_test)\n",
    "    if indices_val is not None:\n",
    "        Xf_va, TC_va, Zr_va = transform_block(indices_val)\n",
    "    else:\n",
    "        Xf_va = TC_va = Zr_va = None\n",
    "\n",
    "    # seen masks\n",
    "    if scenario == \"cases\":\n",
    "        seen_pids = set(pid_idx_full[indices_train].tolist())\n",
    "        seen_tr = np.ones(indices_train.shape[0], dtype=bool)\n",
    "        seen_va = np.ones(indices_val.shape[0], dtype=bool) if indices_val is not None else None\n",
    "        seen_te = np.array([p in seen_pids for p in pid_idx_full[indices_test]], dtype=bool)\n",
    "    else:  # \"time\" : within-subject splits -> all test pids are seen\n",
    "        seen_tr = np.ones(indices_train.shape[0], dtype=bool)\n",
    "        seen_va = np.ones(indices_val.shape[0], dtype=bool) if indices_val is not None else None\n",
    "        seen_te = np.ones(indices_test.shape[0], dtype=bool)\n",
    "\n",
    "    tr_loader = _make_loader_kan(\n",
    "        Xf_tr, TC_tr, Zr_tr, y[indices_train], y_lags[indices_train], dt_lags[indices_train],\n",
    "        pid_idx_full[indices_train], seen_tr, batch_size=batch_size, shuffle=True, device=device\n",
    "    )\n",
    "    va_loader = None\n",
    "    if indices_val is not None:\n",
    "        va_loader = _make_loader_kan(\n",
    "            Xf_va, TC_va, Zr_va, y[indices_val], y_lags[indices_val], dt_lags[indices_val],\n",
    "            pid_idx_full[indices_val], seen_va, batch_size=batch_size, shuffle=False, device=device\n",
    "        )\n",
    "    te_loader = _make_loader_kan(\n",
    "        Xf_te, TC_te, Zr_te, y[indices_test], y_lags[indices_test], dt_lags[indices_test],\n",
    "        pid_idx_full[indices_test], seen_te, batch_size=batch_size, shuffle=False, device=device\n",
    "    )\n",
    "\n",
    "    preprocessors = {\n",
    "        \"of_pipe\": of_pipe,\n",
    "        \"fr_pipe\": fr_pipe,\n",
    "        \"d_fix\":   Xf_tr.shape[1],                                           # dim(OF ⊕ FR)\n",
    "        \"d_tc\":    (Xf_tr.shape[1] - (0 if Zr_tr is None else Zr_tr.shape[1])),  # dim(OF)\n",
    "        \"d_zrand\": 0 if Zr_tr is None else Zr_tr.shape[1],                   # dim(FR)\n",
    "        \"n_ids\":   int(len(np.unique(pid_idx_full)))\n",
    "    }\n",
    "    loaders = {\"train\": tr_loader, \"val\": va_loader, \"test\": te_loader}\n",
    "    return preprocessors, loaders\n",
    "\n",
    "# ---------- Fit once + evaluate ----------\n",
    "def _fit_eval_once_kan(\n",
    "    build_model_fn, wrapper_cls,\n",
    "    arch_params: Dict[str, Any],\n",
    "    train_params: Dict[str, Any],\n",
    "    X_only_fixed: np.ndarray,\n",
    "    X_fixed_and_random: Optional[np.ndarray],\n",
    "    y: np.ndarray,\n",
    "    y_lags: np.ndarray,\n",
    "    dt_lags: np.ndarray,\n",
    "    pid_idx_full: np.ndarray,\n",
    "    tr_idx: np.ndarray,\n",
    "    va_idx: Optional[np.ndarray],\n",
    "    te_idx: np.ndarray,\n",
    "    device: torch.device,\n",
    "    scenario: str,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    preprocessors, loaders = _prepare_split_and_loaders_kan(\n",
    "        X_only_fixed, X_fixed_and_random, y, y_lags, dt_lags, pid_idx_full,\n",
    "        tr_idx, va_idx, te_idx, scenario,\n",
    "        batch_size=train_params.get(\"batch_size\", 256),\n",
    "        device=device,\n",
    "        random_state=train_params.get(\"random_state\", 42),\n",
    "        pca_var_ratio=train_params.get(\"pca_var_ratio\", 0.95),\n",
    "    )\n",
    "\n",
    "    y_dim = y.shape[1] if y.ndim == 2 else 1\n",
    "    model = build_model_fn(\n",
    "        y_dim=y_dim,\n",
    "        d_fix=preprocessors[\"d_fix\"],\n",
    "        d_tc=preprocessors[\"d_tc\"],\n",
    "        d_zrand=preprocessors[\"d_zrand\"],\n",
    "        n_ids=preprocessors[\"n_ids\"],\n",
    "        **arch_params\n",
    "    ).to(device)\n",
    "\n",
    "    wrapper = wrapper_cls(model, cfg=train_params, device=device)\n",
    "\n",
    "    # Train\n",
    "    wrapper.fit(loaders[\"train\"], loaders[\"val\"], verbose=verbose)\n",
    "\n",
    "    # Predict on TEST\n",
    "    model.eval()\n",
    "    preds_all, y_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b in loaders[\"test\"]:\n",
    "            y_hat = wrapper.predict(\n",
    "                X_fix=Xf_b,\n",
    "                TC=TC_b if TC_b.size(1) > 0 else None,\n",
    "                Zrand=Zr_b if Zr_b.size(1) > 0 else None,\n",
    "                y_lags=yl_b,\n",
    "                dt_lags=dt_b,\n",
    "                pid_idx=pid_b,\n",
    "                pid_seen_mask=seen_b\n",
    "            )\n",
    "            preds_all.append(y_hat.cpu().numpy())\n",
    "            y_all.append(y_b.cpu().numpy())\n",
    "\n",
    "    y_pred_te = np.vstack(preds_all)\n",
    "    y_true_te = np.vstack(y_all)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = regression_metrics(y_true_te, y_pred_te)\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"preprocessors\": preprocessors,\n",
    "        \"wrapper\": wrapper,\n",
    "        \"model\": wrapper.model,\n",
    "        \"y_true_test\": y_true_te,\n",
    "        \"y_pred_test\": y_pred_te,\n",
    "    }\n",
    "\n",
    "# ---------- CV summarizer ----------\n",
    "def _summarize_cv_folds(results_folds: List[Dict[str, float]]) -> Dict[str, float]:\n",
    "    if not results_folds:\n",
    "        return {}\n",
    "    all_keys = set().union(*results_folds)\n",
    "    summary = {}\n",
    "    for k in sorted(all_keys):\n",
    "        vals = np.array([fold.get(k, np.nan) for fold in results_folds], dtype=float)\n",
    "        mask = np.isfinite(vals); n = int(mask.sum())\n",
    "        if n == 0:\n",
    "            m = low = high = np.nan\n",
    "        elif n == 1:\n",
    "            m = float(vals[mask][0]); low = high = np.nan\n",
    "        else:\n",
    "            m = float(np.nanmean(vals)); s = float(np.nanstd(vals, ddof=1)); se = s/np.sqrt(n)\n",
    "            tcrit = float(student_t.ppf(0.975, df=n-1))\n",
    "            low, high = m - tcrit*se, m + tcrit*se\n",
    "        summary[f\"{k}_mean\"] = m\n",
    "        summary[f\"{k}_95ci_low\"] = low\n",
    "        summary[f\"{k}_95ci_high\"] = high\n",
    "    return summary\n",
    "\n",
    "# ---------- Public entry (single / cv_only / nested_cv) ----------\n",
    "def run_training_and_eval_kan(\n",
    "    X_only_fixed: np.ndarray,\n",
    "    X_fixed_and_random: Optional[np.ndarray],\n",
    "    y: np.ndarray,\n",
    "    y_lags: np.ndarray,\n",
    "    dt_lags: np.ndarray,\n",
    "    pid_idx: np.ndarray,\n",
    "    time_index: np.ndarray,\n",
    "    build_model_fn,               # callable(y_dim, d_fix, d_tc, d_zrand, n_ids, **arch)\n",
    "    wrapper_cls,                  # e.g., KANMixedEffectsRegWrapper\n",
    "    *,\n",
    "    mode: str = \"single\",         # \"single\" | \"cv_only\" | \"nested_cv\"\n",
    "    scenario: str = \"cases\",      # \"cases\" | \"time\"\n",
    "    outer_folds: int = 5,\n",
    "    inner_folds: int = 3,\n",
    "    param_grid: Optional[Dict[str, List]] = None,\n",
    "    arch_defaults: Optional[Dict[str, Any]] = None,\n",
    "    train_defaults: Optional[Dict[str, Any]] = None,\n",
    "    device: Optional[torch.device] = None,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    device = (\n",
    "        device\n",
    "        or (torch.device(\"mps\") if torch.backends.mps.is_available() else None)\n",
    "        or (torch.device(\"cuda\") if torch.cuda.is_available() else None)\n",
    "        or torch.device(\"cpu\")\n",
    "    )\n",
    "\n",
    "    # sanitize\n",
    "    X_of = np.asarray(X_only_fixed, dtype=np.float32)\n",
    "    X_fr = None if X_fixed_and_random is None else np.asarray(X_fixed_and_random, dtype=np.float32)\n",
    "    y    = np.asarray(y, dtype=np.float32)\n",
    "    if y.ndim == 1: y = y[:, None]\n",
    "    y_lags = np.asarray(y_lags, dtype=np.float32)\n",
    "    dt_lags = np.asarray(dt_lags, dtype=np.float32)\n",
    "    pid_idx = np.asarray(pid_idx, dtype=np.int64)\n",
    "    time_ix = np.asarray(time_index)\n",
    "\n",
    "    arch_defaults = arch_defaults or {}\n",
    "    train_defaults = train_defaults or {}\n",
    "    rnd = int(train_defaults.get(\"random_state\", 42))\n",
    "    val_frac = float(train_defaults.get(\"val_fraction\", 0.10))\n",
    "    min_meas = int(train_defaults.get(\"min_meas_test\", 4))\n",
    "    time_test_frac = float(train_defaults.get(\"time_test_fraction\", 0.30))\n",
    "\n",
    "    # -------------------- SINGLE --------------------\n",
    "    if mode == \"single\":\n",
    "        if scenario == \"cases\":\n",
    "            tr_idx_all, te_idx = _split_cases(pid_idx, test_fraction=0.2, seed=rnd)\n",
    "            # group-wise validation (no participant leakage)\n",
    "            tr_idx, va_idx = _make_train_val_split_cases(tr_idx_all, pid_idx, seed=rnd, val_frac=val_frac)\n",
    "        elif scenario == \"time\":\n",
    "            tr_idx, te_idx = _split_time_last_fraction_per_subject(pid_idx, time_ix, test_fraction=time_test_frac, min_meas=min_meas)\n",
    "            # plain row-wise val inside train (you can replace with time-aware if desired)\n",
    "            tr_idx, va_idx = _make_train_val_split(tr_idx, seed=rnd, val_frac=val_frac)\n",
    "        else:\n",
    "            raise ValueError(\"scenario must be 'cases' or 'time'\")\n",
    "\n",
    "        res = _fit_eval_once_kan(\n",
    "            build_model_fn, wrapper_cls,\n",
    "            arch_defaults, train_defaults,\n",
    "            X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "            tr_idx, va_idx, te_idx,\n",
    "            device=device, scenario=scenario,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            _print_regression_metrics(res[\"metrics\"], title=\"Single-fit test metrics\")\n",
    "        return res\n",
    "\n",
    "    # -------------------- CV-ONLY --------------------\n",
    "    if mode == \"cv_only\":\n",
    "        fold_metrics: List[Dict[str, float]] = []\n",
    "\n",
    "        if scenario == \"cases\":\n",
    "            outer = GroupKFold(n_splits=outer_folds)\n",
    "            outer_iter = outer.split(X_of, y[:, 0], groups=pid_idx)\n",
    "\n",
    "            for fold_id, (tr_idx_all, te_idx) in enumerate(outer_iter, start=1):\n",
    "                # group-wise val inside the train fold\n",
    "                tr_idx, va_idx = _make_train_val_split_cases(np.asarray(tr_idx_all), pid_idx, seed=rnd + fold_id, val_frac=val_frac)\n",
    "                res = _fit_eval_once_kan(\n",
    "                    build_model_fn, wrapper_cls,\n",
    "                    arch_defaults, train_defaults,\n",
    "                    X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                    tr_idx, va_idx, te_idx,\n",
    "                    device=device, scenario=scenario,\n",
    "                    verbose=False\n",
    "                )\n",
    "                fold_metrics.append(res[\"metrics\"])\n",
    "                if verbose:\n",
    "                    print(f\"\\nFold {fold_id}:\")\n",
    "                    _print_regression_metrics(res[\"metrics\"], title=\"Per-fold test metrics\")\n",
    "\n",
    "        else:  # scenario == \"time\"\n",
    "            folds = _time_kfold_splits_by_subject_global(pid_idx, time_ix, n_splits=outer_folds, min_points=min_meas, indices=None)\n",
    "            for fold_id, (tr_idx_all, te_idx) in enumerate(folds, start=1):\n",
    "                # row-wise val inside the train fold (can be swapped for time-aware val if desired)\n",
    "                tr_idx, va_idx = _make_train_val_split(np.asarray(tr_idx_all), seed=rnd + fold_id, val_frac=val_frac)\n",
    "                if te_idx.size == 0 or tr_idx.size == 0:\n",
    "                    if verbose:\n",
    "                        print(f\"Fold {fold_id}: skipped (empty train/test).\")\n",
    "                    continue\n",
    "                res = _fit_eval_once_kan(\n",
    "                    build_model_fn, wrapper_cls,\n",
    "                    arch_defaults, train_defaults,\n",
    "                    X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                    tr_idx, va_idx, te_idx,\n",
    "                    device=device, scenario=scenario,\n",
    "                    verbose=False\n",
    "                )\n",
    "                fold_metrics.append(res[\"metrics\"])\n",
    "                if verbose:\n",
    "                    print(f\"\\nFold {fold_id}:\")\n",
    "                    _print_regression_metrics(res[\"metrics\"], title=\"Per-fold test metrics\")\n",
    "\n",
    "        cv_summary = _summarize_cv_folds(fold_metrics)\n",
    "        if verbose:\n",
    "            print(\"\\nCV averages (±95% CI):\")\n",
    "            for key in sorted(cv_summary.keys()):\n",
    "                if key.endswith(\"_mean\"):\n",
    "                    base = key[:-5]\n",
    "                    low = cv_summary.get(f\"{base}_95ci_low\", np.nan)\n",
    "                    high = cv_summary.get(f\"{base}_95ci_high\", np.nan)\n",
    "                    print(f\"{base:>20}: {cv_summary[key]:.6f}  (95% CI {low:.6f}, {high:.6f})\")\n",
    "\n",
    "        return {\n",
    "            \"cv_folds_metrics\": fold_metrics,\n",
    "            \"cv_summary\": cv_summary,\n",
    "        }\n",
    "\n",
    "    # -------------------- NESTED CV --------------------\n",
    "    if mode == \"nested_cv\":\n",
    "        if not param_grid:\n",
    "            param_grid = {\n",
    "                \"d_fix_latent\": [128, 256],\n",
    "                \"d_rand_latent\": [128],\n",
    "                \"n_kernels\": [4, 8],\n",
    "                \"dropout\": [0.0, 0.1],\n",
    "                \"lr\": [1e-3, 3e-4],\n",
    "                \"weight_decay\": [0.0, 1e-4],\n",
    "                \"batch_size\": [256],\n",
    "                \"max_epochs\": [100],\n",
    "                \"patience\": [10],\n",
    "            }\n",
    "\n",
    "        results_folds = []\n",
    "        best_score_global, best_params_global = -np.inf, None\n",
    "\n",
    "        # Outer iterator\n",
    "        if scenario == \"cases\":\n",
    "            outer = GroupKFold(n_splits=outer_folds)\n",
    "            outer_iter = list(outer.split(X_of, y[:, 0], groups=pid_idx))\n",
    "        else:\n",
    "            outer_iter = _time_kfold_splits_by_subject_global(pid_idx, time_ix, n_splits=outer_folds, min_points=min_meas, indices=None)\n",
    "\n",
    "        for fold_id, (tr_idx_all, te_idx) in enumerate(outer_iter, start=1):\n",
    "            if verbose:\n",
    "                print(f\"\\nOuter fold {fold_id}/{outer_folds}\")\n",
    "\n",
    "            # ----- INNER CV: select by macro_R2 (higher is better) -----\n",
    "            def inner_iter():\n",
    "                if scenario == \"cases\":\n",
    "                    inner = GroupKFold(n_splits=inner_folds)\n",
    "                    return list(inner.split(X_of[tr_idx_all], y[tr_idx_all, 0], groups=pid_idx[tr_idx_all]))\n",
    "                else:\n",
    "                    # per-subject time folds restricted to outer-train rows\n",
    "                    return _time_kfold_splits_by_subject_global(\n",
    "                        pid_idx, time_ix, n_splits=inner_folds, min_points=min_meas, indices=np.asarray(tr_idx_all)\n",
    "                    )\n",
    "\n",
    "            best_inner_score, best_inner_params = -np.inf, None\n",
    "            for params in ParameterGrid(param_grid):\n",
    "                arch_params = dict(arch_defaults)\n",
    "                train_params = dict(train_defaults)\n",
    "                for k, v in params.items():\n",
    "                    if k in (\"d_fix_latent\", \"d_rand_latent\", \"n_kernels\", \"dropout\"):\n",
    "                        arch_params[k] = v\n",
    "                    else:\n",
    "                        train_params[k] = v\n",
    "\n",
    "                inner_scores = []\n",
    "                for in_tr, in_va in inner_iter():\n",
    "                    # add group-wise val ONLY for 'cases' (no leakage)\n",
    "                    if scenario == \"cases\":\n",
    "                        tr_idx_inner, va_idx_inner = _make_train_val_split_cases(np.asarray(in_tr), pid_idx, seed=rnd + fold_id, val_frac=val_frac)\n",
    "                    else:\n",
    "                        tr_idx_inner, va_idx_inner = _make_train_val_split(np.asarray(in_tr), seed=rnd + fold_id, val_frac=val_frac)\n",
    "\n",
    "                    res_inner = _fit_eval_once_kan(\n",
    "                        build_model_fn, wrapper_cls,\n",
    "                        arch_params, train_params,\n",
    "                        X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                        tr_idx_inner, va_idx_inner, np.asarray(in_va),\n",
    "                        device=device, scenario=scenario,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    score = res_inner[\"metrics\"].get(\"macro_R2\", np.nan)\n",
    "                    inner_scores.append(score)\n",
    "\n",
    "                avg_score = float(np.nanmean(inner_scores)) if len(inner_scores) else -np.inf\n",
    "                if avg_score > best_inner_score:\n",
    "                    best_inner_score = avg_score\n",
    "                    best_inner_params = (arch_params, train_params)\n",
    "\n",
    "            if best_inner_params is None:\n",
    "                if verbose: print(\"No viable inner config; skipping outer fold.\")\n",
    "                continue\n",
    "\n",
    "            arch_params, train_params = best_inner_params\n",
    "\n",
    "            # final train/val split inside outer-train for the final fit before testing\n",
    "            if scenario == \"cases\":\n",
    "                tr_idx_outer, va_idx_outer = _make_train_val_split_cases(np.asarray(tr_idx_all), pid_idx, seed=rnd + fold_id * 17, val_frac=val_frac)\n",
    "            else:\n",
    "                tr_idx_outer, va_idx_outer = _make_train_val_split(np.asarray(tr_idx_all), seed=rnd + fold_id * 17, val_frac=val_frac)\n",
    "\n",
    "            res_outer = _fit_eval_once_kan(\n",
    "                build_model_fn, wrapper_cls,\n",
    "                arch_params, train_params,\n",
    "                X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                tr_idx_outer, va_idx_outer, np.asarray(te_idx),\n",
    "                device=device, scenario=scenario,\n",
    "                verbose=False\n",
    "            )\n",
    "            results_folds.append(res_outer)\n",
    "\n",
    "            score_outer = res_outer[\"metrics\"].get(\"macro_R2\", -np.inf)\n",
    "            if score_outer > best_score_global:\n",
    "                best_score_global = score_outer\n",
    "                best_params_global = (arch_params, train_params)\n",
    "\n",
    "            if verbose:\n",
    "                _print_regression_metrics(res_outer[\"metrics\"], title=\"Outer fold test metrics\")\n",
    "\n",
    "        # summarize folds\n",
    "        def _summarize_block(results_list: List[Dict[str, Any]], which: str = \"metrics\") -> Dict[str, float]:\n",
    "            keys = list(results_list[0][which].keys())\n",
    "            out = {}\n",
    "            for k in keys:\n",
    "                arr = np.array([res[which][k] for res in results_list], dtype=float)\n",
    "                m = float(np.nanmean(arr)); s = float(np.nanstd(arr, ddof=1)); n = len(arr)\n",
    "                se = s / np.sqrt(n) if n > 1 else np.nan\n",
    "                if n > 1:\n",
    "                    tcrit = float(student_t.ppf(0.975, df=n-1))\n",
    "                    ci = (m - tcrit * se, m + tcrit * se)\n",
    "                else:\n",
    "                    ci = (np.nan, np.nan)\n",
    "                out[k + \"_mean\"] = m\n",
    "                out[k + \"_95ci_low\"] = ci[0]\n",
    "                out[k + \"_95ci_high\"] = ci[1]\n",
    "            return out\n",
    "\n",
    "        cv_summary = _summarize_block(results_folds, \"metrics\")\n",
    "\n",
    "        if verbose and best_params_global is not None:\n",
    "            print(\"\\nBest params (by outer macro_R2):\")\n",
    "            arch_p, train_p = best_params_global\n",
    "            print(\"[ARCH]:\");   [print(f\"  {k}: {v}\") for k, v in arch_p.items()]\n",
    "            print(\"[TRAIN]:\");  [print(f\"  {k}: {v}\") for k, v in train_p.items()]\n",
    "        \n",
    "        print(\"\\nCross-validation results:\")\n",
    "        print(\"CV Summary:\")\n",
    "        print(cv_summary)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"outer_folds\": results_folds,\n",
    "            \"cv_summary\": cv_summary,\n",
    "            \"best_params\": (best_params_global[0], best_params_global[1]) if best_params_global else None,\n",
    "        }\n",
    "\n",
    "    raise ValueError(\"mode must be one of {'single','cv_only','nested_cv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ef43a",
   "metadata": {},
   "source": [
    "## Model test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a6c308",
   "metadata": {},
   "source": [
    "### Define variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35da766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ---------- targets & aux ----------\n",
    "y_raw      = EXP_reg_y.to_numpy(np.float32)\n",
    "y_np       = y_raw if y_raw.ndim == 2 else y_raw.reshape(-1, 1)\n",
    "\n",
    "y_lags_np  = EXP_reg_outcomes_lags.to_numpy(np.float32)\n",
    "dt_lags_np = EXP_reg_forecast_horizons.to_numpy(np.float32)\n",
    "\n",
    "# ---------- inputs ----------\n",
    "# keep the two blocks SEPARATE; evaluator will:\n",
    "#   - TC  <- only_fixed (after PCA)\n",
    "#   - X_fix <- concat(only_fixed_PCA, fixed_and_random_PCA)\n",
    "#   - Zrand <- fixed_and_random_PCA\n",
    "X_only_fixed_np        = EXP_reg_only_fixed.to_numpy(np.float32)\n",
    "X_fixed_and_random_np  = EXP_reg_fixed_and_random.to_numpy(np.float32)\n",
    "\n",
    "# ---------- ids & time ----------\n",
    "pid_raw    = EXP_reg_participant_id.to_numpy().ravel()\n",
    "pid_uniqs, pid_encoded = np.unique(pid_raw, return_inverse=True)\n",
    "pid_np     = pid_encoded.astype(np.int64)\n",
    "n_ids      = int(len(pid_uniqs))\n",
    "\n",
    "time_ix_np = EXP_reg_time.to_numpy().ravel()\n",
    "\n",
    "def build_model_fn(\n",
    "    *,\n",
    "    y_dim: int,\n",
    "    d_fix: int,\n",
    "    d_tc: int,      # PCA(only_fixed)\n",
    "    d_zrand: int,   # PCA(fixed_and_random)\n",
    "    n_ids: int,\n",
    "    **arch\n",
    "):\n",
    "    return KANAdditiveMixedEffects(\n",
    "        y_dim=y_dim,\n",
    "        d_fix=d_fix,     # concat(only_fixed_PCA, fixed_and_random_PCA)\n",
    "        d_tc=d_tc,       # only_fixed_PCA -> FiLM\n",
    "        d_zrand=d_zrand, # fixed_and_random_PCA -> random branch\n",
    "        n_ids=n_ids,\n",
    "        **arch\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "arch_defaults = dict(d_fix_latent=256, d_rand_latent=256, n_kernels=16, dropout=0.0)\n",
    "train_defaults = dict(\n",
    "    lr=3e-3, weight_decay=1e-5, batch_size=64,\n",
    "    max_epochs=100, patience=10, threshold=0.5,\n",
    "    lambda_mean0=1e-5, lambda_ridge=1e-5,\n",
    "    lambda_orth_latent=1e-5,\n",
    "    lambda_film_identity=1e-5,\n",
    "    lambda_kan=0,\n",
    "    random_state=42,\n",
    "    val_fraction=0.2, min_meas_test=3,\n",
    "    loss=\"mse\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba72c8e",
   "metadata": {},
   "source": [
    "### Simple cases split test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8348913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train 13.396842 | val 10.651239\n",
      "Epoch 002 | train 10.540825 | val 10.287268\n",
      "Epoch 003 | train 9.591392 | val 9.838263\n",
      "Epoch 004 | train 9.160029 | val 10.329488\n",
      "Epoch 005 | train 7.787717 | val 11.320188\n",
      "Epoch 006 | train 5.915417 | val 11.911542\n",
      "Epoch 007 | train 4.604090 | val 12.951723\n",
      "Epoch 008 | train 3.710806 | val 13.314583\n",
      "Epoch 009 | train 3.165755 | val 13.397321\n",
      "Epoch 010 | train 2.560015 | val 15.683063\n",
      "Epoch 011 | train 2.171884 | val 12.801417\n",
      "Epoch 012 | train 1.846762 | val 13.514762\n",
      "Epoch 013 | train 1.642524 | val 13.121567\n",
      "Early stopping at epoch 013 (best val 9.838263)\n",
      "\n",
      "Single-fit test metrics:\n",
      "       macro_MAE: 2.186242\n",
      "       macro_MSE: 10.216752\n",
      "  macro_PearsonR: 0.423506\n",
      "        macro_R2: 0.158055\n",
      "      macro_RMSE: 3.196365\n",
      "task_1: MSE=10.216752, RMSE=3.196365, MAE=2.186242, R2=0.158055, PearsonR=0.423506\n"
     ]
    }
   ],
   "source": [
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"single\",\n",
    "    scenario=\"cases\",\n",
    "    train_defaults=train_defaults\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664dd4b",
   "metadata": {},
   "source": [
    "### Simple time split test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9edf5e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train 15.191086 | val 10.176481\n",
      "Epoch 002 | train 11.078441 | val 9.775215\n",
      "Epoch 003 | train 10.360999 | val 9.474136\n",
      "Epoch 004 | train 9.841178 | val 9.219025\n",
      "Epoch 005 | train 8.132365 | val 9.806080\n",
      "Epoch 006 | train 6.187256 | val 9.917340\n",
      "Epoch 007 | train 4.364584 | val 11.813448\n",
      "Epoch 008 | train 3.225908 | val 10.781802\n",
      "Epoch 009 | train 2.544779 | val 11.750495\n",
      "Epoch 010 | train 2.147396 | val 12.068660\n",
      "Epoch 011 | train 1.714293 | val 11.302686\n",
      "Epoch 012 | train 1.449885 | val 11.919052\n",
      "Epoch 013 | train 1.283006 | val 11.595913\n",
      "Epoch 014 | train 1.236620 | val 11.143318\n",
      "Early stopping at epoch 014 (best val 9.219025)\n",
      "\n",
      "Single-fit test metrics:\n",
      "       macro_MAE: 2.200420\n",
      "       macro_MSE: 10.739383\n",
      "  macro_PearsonR: 0.594772\n",
      "        macro_R2: 0.311435\n",
      "      macro_RMSE: 3.277100\n",
      "task_1: MSE=10.739383, RMSE=3.277100, MAE=2.200420, R2=0.311435, PearsonR=0.594772\n"
     ]
    }
   ],
   "source": [
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"single\",\n",
    "    scenario=\"time\",\n",
    "    train_defaults=train_defaults,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea479b91",
   "metadata": {},
   "source": [
    "### Cases split test CV without parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11b8a291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.331588\n",
      "       macro_MSE: 11.063968\n",
      "  macro_PearsonR: 0.581218\n",
      "        macro_R2: 0.335367\n",
      "      macro_RMSE: 3.326254\n",
      "task_1: MSE=11.063968, RMSE=3.326254, MAE=2.331588, R2=0.335367, PearsonR=0.581218\n",
      "\n",
      "Fold 2:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.608558\n",
      "       macro_MSE: 11.315410\n",
      "  macro_PearsonR: 0.712683\n",
      "        macro_R2: 0.490572\n",
      "      macro_RMSE: 3.363839\n",
      "task_1: MSE=11.315410, RMSE=3.363839, MAE=2.608558, R2=0.490572, PearsonR=0.712683\n",
      "\n",
      "Fold 3:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.297089\n",
      "       macro_MSE: 10.312981\n",
      "  macro_PearsonR: 0.522063\n",
      "        macro_R2: 0.261172\n",
      "      macro_RMSE: 3.211383\n",
      "task_1: MSE=10.312981, RMSE=3.211383, MAE=2.297089, R2=0.261172, PearsonR=0.522063\n",
      "\n",
      "Fold 4:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.406271\n",
      "       macro_MSE: 11.299912\n",
      "  macro_PearsonR: 0.528698\n",
      "        macro_R2: 0.277016\n",
      "      macro_RMSE: 3.361534\n",
      "task_1: MSE=11.299912, RMSE=3.361534, MAE=2.406271, R2=0.277016, PearsonR=0.528698\n",
      "\n",
      "Fold 5:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.470058\n",
      "       macro_MSE: 11.514628\n",
      "  macro_PearsonR: 0.299674\n",
      "        macro_R2: -0.054124\n",
      "      macro_RMSE: 3.393321\n",
      "task_1: MSE=11.514628, RMSE=3.393321, MAE=2.470058, R2=-0.054124, PearsonR=0.299674\n",
      "\n",
      "CV averages (±95% CI):\n",
      "           macro_MAE: 2.422713  (95% CI 2.269208, 2.576217)\n",
      "           macro_MSE: 11.101380  (95% CI 10.519328, 11.683431)\n",
      "      macro_PearsonR: 0.528867  (95% CI 0.343546, 0.714188)\n",
      "            macro_R2: 0.262000  (95% CI 0.015390, 0.508611)\n",
      "          macro_RMSE: 3.331266  (95% CI 3.242975, 3.419558)\n",
      "          task_1_MAE: 2.422713  (95% CI 2.269208, 2.576217)\n",
      "          task_1_MSE: 11.101380  (95% CI 10.519328, 11.683431)\n",
      "     task_1_PearsonR: 0.528867  (95% CI 0.343546, 0.714188)\n",
      "           task_1_R2: 0.262000  (95% CI 0.015390, 0.508611)\n",
      "         task_1_RMSE: 3.331266  (95% CI 3.242975, 3.419558)\n"
     ]
    }
   ],
   "source": [
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"cv_only\",\n",
    "    scenario=\"cases\",\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    outer_folds=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8ef24",
   "metadata": {},
   "source": [
    "### Time split test CV without parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f565faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.243249\n",
      "       macro_MSE: 10.638878\n",
      "  macro_PearsonR: 0.601547\n",
      "        macro_R2: 0.352936\n",
      "      macro_RMSE: 3.261729\n",
      "task_1: MSE=10.638878, RMSE=3.261729, MAE=2.243249, R2=0.352936, PearsonR=0.601547\n",
      "\n",
      "Fold 2:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.299955\n",
      "       macro_MSE: 10.555529\n",
      "  macro_PearsonR: 0.626576\n",
      "        macro_R2: 0.390701\n",
      "      macro_RMSE: 3.248927\n",
      "task_1: MSE=10.555529, RMSE=3.248927, MAE=2.299955, R2=0.390701, PearsonR=0.626576\n",
      "\n",
      "Fold 3:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.167619\n",
      "       macro_MSE: 10.229458\n",
      "  macro_PearsonR: 0.609926\n",
      "        macro_R2: 0.364116\n",
      "      macro_RMSE: 3.198352\n",
      "task_1: MSE=10.229458, RMSE=3.198352, MAE=2.167619, R2=0.364116, PearsonR=0.609926\n",
      "\n",
      "Fold 4:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.173821\n",
      "       macro_MSE: 9.616213\n",
      "  macro_PearsonR: 0.581642\n",
      "        macro_R2: 0.319144\n",
      "      macro_RMSE: 3.101002\n",
      "task_1: MSE=9.616213, RMSE=3.101002, MAE=2.173821, R2=0.319144, PearsonR=0.581642\n",
      "\n",
      "Fold 5:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.258153\n",
      "       macro_MSE: 10.138047\n",
      "  macro_PearsonR: 0.633943\n",
      "        macro_R2: 0.376263\n",
      "      macro_RMSE: 3.184030\n",
      "task_1: MSE=10.138047, RMSE=3.184030, MAE=2.258153, R2=0.376263, PearsonR=0.633943\n",
      "\n",
      "CV averages (±95% CI):\n",
      "           macro_MAE: 2.228559  (95% CI 2.158050, 2.299069)\n",
      "           macro_MSE: 10.235625  (95% CI 9.731954, 10.739296)\n",
      "      macro_PearsonR: 0.610727  (95% CI 0.584971, 0.636482)\n",
      "            macro_R2: 0.360632  (95% CI 0.326963, 0.394301)\n",
      "          macro_RMSE: 3.198808  (95% CI 3.119654, 3.277963)\n",
      "          task_1_MAE: 2.228559  (95% CI 2.158050, 2.299069)\n",
      "          task_1_MSE: 10.235625  (95% CI 9.731954, 10.739296)\n",
      "     task_1_PearsonR: 0.610727  (95% CI 0.584971, 0.636482)\n",
      "           task_1_R2: 0.360632  (95% CI 0.326963, 0.394301)\n",
      "         task_1_RMSE: 3.198808  (95% CI 3.119654, 3.277963)\n"
     ]
    }
   ],
   "source": [
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"cv_only\",\n",
    "    scenario=\"time\",\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    outer_folds=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef6057",
   "metadata": {},
   "source": [
    "### Time split test CV WITH parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outer fold 1/5\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "\n",
    "    \"d_fix_latent\": [64, 128],\n",
    "    \"d_rand_latent\": [64, 128],\n",
    "    \"n_kernels\": [4, 8],\n",
    "    \"dropout\": [0.0, 0.1, 0.2],\n",
    "\n",
    "    \"lr\": [1e-3, 3e-4],\n",
    "    \"weight_decay\": [0.0, 1e-4],\n",
    "    \"batch_size\": [256],\n",
    "    \"max_epochs\": [120],\n",
    "    \"patience\": [12],\n",
    "    \"lambda_contrast\": [1e-2, 5e-3, 2e-3],\n",
    "    \"contrast_temp\": [0.2, 0.1],\n",
    "    \"lambda_orth\": [1e-3, 5e-4],\n",
    "    \"lambda_mean0\": [1e-3],\n",
    "    \"lambda_ridge\": [1e-4, 5e-4],\n",
    "    \"lambda_kan\": [0.0, 1e-4],\n",
    "}\n",
    "\n",
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"nested_cv\",\n",
    "    scenario=\"time\",\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    outer_folds=5,\n",
    "    inner_folds=2,\n",
    "    param_grid=param_grid,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031db303",
   "metadata": {},
   "source": [
    "### Cases split test CV WITH parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc49eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"nested_cv\",\n",
    "    scenario=\"cases\",\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    outer_folds=5,\n",
    "    inner_folds=2,\n",
    "    param_grid=param_grid,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
