{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f278667b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1069b",
   "metadata": {},
   "source": [
    "## Root and data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "068e5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "root_dir = \"/Users/silviumatu/Desktop/Code/Python/Disertatie/Disertatie_Matu_Silviu_v1\"\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "data_dir = os.path.join(root_dir, \"Data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1166a5e",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a9787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_rows': 1031,\n",
       " 'n_unique_participants': 257,\n",
       " 'avg_rows_per_participant': np.float64(4.011673151750973),\n",
       " 'min_rows_per_participant': 1,\n",
       " 'max_rows_per_participant': 20,\n",
       " 'avg_age_per_participant': np.float64(19.424124513618676),\n",
       " 'avg_age_row_weighted': np.float64(19.446168768186226),\n",
       " 'num_ones_next_outcome': np.int64(556),\n",
       " 'pct_positive_next_outcome': np.float64(53.9282250242483)}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GHQ_cat_df = pd.read_csv(os.path.join(data_dir, \"PED_GHQ_categorical_data_forecast.csv\"))\n",
    "columns_GHQ_cat_df = pd.read_csv(os.path.join(data_dir, \"columns_PED_GHQ_categorical_data_forecast_S.csv\"))\n",
    "\n",
    "GHQ_cat_df.head()\n",
    "\n",
    "\n",
    "\n",
    "n_rows = len(GHQ_cat_df)\n",
    "n_unique_participants = GHQ_cat_df[\"x_participant_id\"].nunique()\n",
    "\n",
    "# Measurements per participant\n",
    "group_sizes = GHQ_cat_df.groupby(\"x_participant_id\").size()\n",
    "avg_rows_per_id = group_sizes.mean()\n",
    "min_rows_per_id = group_sizes.min()\n",
    "max_rows_per_id = group_sizes.max()\n",
    "\n",
    "# Age\n",
    "avg_age_per_participant = (\n",
    "    GHQ_cat_df.groupby(\"x_participant_id\")[\"x_age_baseline\"].first().mean()\n",
    ")\n",
    "avg_age_row_weighted = GHQ_cat_df[\"x_age_baseline\"].mean()\n",
    "\n",
    "# Next-outcome positives\n",
    "num_ones_next_outcome = (GHQ_cat_df[\"y_GHQ_TOTAL_score_category_next\"] == 1).sum()\n",
    "pct_positive_next_outcome = 100.0 * (\n",
    "    GHQ_cat_df[\"y_GHQ_TOTAL_score_category_next\"] == 1\n",
    ").mean()  # % over all rows\n",
    "\n",
    "summary = {\n",
    "    \"n_rows\": n_rows,\n",
    "    \"n_unique_participants\": n_unique_participants,\n",
    "    \"avg_rows_per_participant\": avg_rows_per_id,\n",
    "    \"min_rows_per_participant\": min_rows_per_id,\n",
    "    \"max_rows_per_participant\": max_rows_per_id,\n",
    "    \"avg_age_per_participant\": avg_age_per_participant,\n",
    "    \"avg_age_row_weighted\": avg_age_row_weighted,\n",
    "    \"num_ones_next_outcome\": num_ones_next_outcome,\n",
    "    \"pct_positive_next_outcome\": pct_positive_next_outcome,\n",
    "}\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c174fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the outcome column(s) marked with 1 in the \"outcomes\" column of columns_GHQ_cat_df\n",
    "GHQ_cat_outcome_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['outcomes'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_y = GHQ_cat_df[GHQ_cat_outcome_cols]\n",
    "GHQ_cat_y.head()\n",
    "\n",
    "# Same for outcomes lags column(s)\n",
    "GHQ_cat_outcomes_lags_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['outcomes_lags'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_outcomes_lags = GHQ_cat_df[GHQ_cat_outcomes_lags_cols]\n",
    "\n",
    "# Same for participant column(s)\n",
    "GHQ_cat_participant_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['participant_id'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_participant_id = GHQ_cat_df[GHQ_cat_participant_cols]\n",
    "\n",
    "# Same for time column(s)\n",
    "GHQ_cat_time_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['time'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_time = GHQ_cat_df[GHQ_cat_time_cols]\n",
    "\n",
    "# Same for forecast horizons column(s)\n",
    "GHQ_cat_forecast_horizons_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['forecast_horizons'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_forecast_horizons = GHQ_cat_df[GHQ_cat_forecast_horizons_cols]\n",
    "\n",
    "# Same for fixed effects column(s)\n",
    "GHQ_cat_only_fixed_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['only_fixed'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_only_fixed = GHQ_cat_df[GHQ_cat_only_fixed_cols]\n",
    "\n",
    "# Same for random effects column(s)\n",
    "GHQ_cat_fixed_and_random_cols = columns_GHQ_cat_df.loc[columns_GHQ_cat_df['fixed_and_random'] == 1, 'column_name'].tolist()\n",
    "GHQ_cat_fixed_and_random = GHQ_cat_df[GHQ_cat_fixed_and_random_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b382d0b",
   "metadata": {},
   "source": [
    "# Custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8845d70a",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9e00bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from efficient_kan import KANLinear\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "\n",
    "def _make_kan(in_dim, out_dim,\n",
    "              grid_size=8, spline_order=3,\n",
    "              scale_noise=0.1, scale_base=1.0, scale_spline=1.0,\n",
    "              enable_standalone_scale_spline=True,\n",
    "              base_activation=torch.nn.SiLU,\n",
    "              grid_eps=0.02, grid_range=(-1.0, 1.0)):\n",
    "    return KANLinear(\n",
    "        in_features=in_dim,\n",
    "        out_features=out_dim,\n",
    "        grid_size=grid_size,\n",
    "        spline_order=spline_order,\n",
    "        scale_noise=scale_noise,\n",
    "        scale_base=scale_base,\n",
    "        scale_spline=scale_spline,\n",
    "        enable_standalone_scale_spline=enable_standalone_scale_spline,\n",
    "        base_activation=base_activation,\n",
    "        grid_eps=grid_eps,\n",
    "        grid_range=list(grid_range),\n",
    "    )\n",
    "\n",
    "class KANBlock(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims=(128, 64), out_dim=None,\n",
    "                 dropout=0.0,\n",
    "                 grid_size=8, spline_order=3,\n",
    "                 scale_noise=0.1, scale_base=1.0, scale_spline=1.0,\n",
    "                 enable_standalone_scale_spline=True,\n",
    "                 base_activation=torch.nn.SiLU,\n",
    "                 grid_eps=0.02, grid_range=(-1.0, 1.0)):\n",
    "        super().__init__()\n",
    "        dims = [in_dim] + list(hidden_dims)\n",
    "        layers = []\n",
    "        for d0, d1 in zip(dims[:-1], dims[1:]):\n",
    "            layers.append(_make_kan(\n",
    "                d0, d1,\n",
    "                grid_size=grid_size, spline_order=spline_order,\n",
    "                scale_noise=scale_noise, scale_base=scale_base, scale_spline=scale_spline,\n",
    "                enable_standalone_scale_spline=enable_standalone_scale_spline,\n",
    "                base_activation=base_activation,\n",
    "                grid_eps=grid_eps, grid_range=grid_range\n",
    "            ))\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "        if out_dim is not None:\n",
    "            layers.append(_make_kan(\n",
    "                dims[-1], out_dim,\n",
    "                grid_size=grid_size, spline_order=spline_order,\n",
    "                scale_noise=scale_noise, scale_base=scale_base, scale_spline=scale_spline,\n",
    "                enable_standalone_scale_spline=enable_standalone_scale_spline,\n",
    "                base_activation=base_activation,\n",
    "                grid_eps=grid_eps, grid_range=grid_range\n",
    "            ))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        reg = 0.0\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, KANLinear):\n",
    "                reg = reg + m.regularization_loss(\n",
    "                    regularize_activation=regularize_activation,\n",
    "                    regularize_entropy=regularize_entropy\n",
    "                )\n",
    "        return reg\n",
    "\n",
    "class TemporalKernelAttentionKAN(nn.Module):\n",
    "    def __init__(self, n_kernels: int = 4, d_att: int = 32,\n",
    "                 grid_size=8, spline_order=3, dropout=0.0,\n",
    "                 normalize_weights: bool = False):\n",
    "        super().__init__()\n",
    "        self.n_k = n_kernels\n",
    "        self.normalize = normalize_weights\n",
    "        self.pi_logits = nn.Parameter(torch.zeros(n_kernels))\n",
    "        self.lam_raw = nn.Parameter(torch.zeros(n_kernels))\n",
    "        self.summarize = KANBlock(1, hidden_dims=(d_att,), out_dim=d_att,\n",
    "                                  dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "        self.out_head = KANBlock(d_att, hidden_dims=(d_att,), out_dim=1,\n",
    "                                 dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, y_lags, dt_lags):\n",
    "        B, L = y_lags.shape\n",
    "        pi = F.softmax(self.pi_logits, dim=-1)\n",
    "        lam = F.softplus(self.lam_raw) + 1e-6\n",
    "        pi_exp = pi.view(1, 1, -1).expand(B, 1, -1)\n",
    "        lam_exp = lam.view(1, 1, -1).expand(B, 1, -1)\n",
    "        kernel = torch.exp(-lam_exp * dt_lags.unsqueeze(-1))\n",
    "        w_lags = torch.sum(pi_exp * kernel, dim=-1)\n",
    "        if self.normalize:\n",
    "            w_sum = w_lags.sum(dim=1, keepdim=True) + 1e-8\n",
    "            w_lags = w_lags / w_sum\n",
    "        s = torch.sum(w_lags * y_lags, dim=1, keepdim=True)\n",
    "        z_att = self.summarize(s)\n",
    "        e_att = self.out_head(z_att)\n",
    "        return e_att, w_lags, z_att\n",
    "\n",
    "class FixedBranchKAN(nn.Module):\n",
    "    def __init__(self, d_fix, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = KANBlock(d_fix, hidden_dims=(256, 128), out_dim=d_latent,\n",
    "                            dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "        self.head = KANBlock(d_latent, hidden_dims=(64,), out_dim=1,\n",
    "                             dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, X_fix):\n",
    "        z = self.enc(X_fix)\n",
    "        e = self.head(z)\n",
    "        return e, z\n",
    "\n",
    "class RandEncoderKAN(nn.Module):\n",
    "    def __init__(self, d_zrand, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = KANBlock(d_zrand, hidden_dims=(256, 128), out_dim=d_latent,\n",
    "                            dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, Zrand):\n",
    "        return self.enc(Zrand)\n",
    "\n",
    "class TCEncoderKAN(nn.Module):\n",
    "    def __init__(self, d_tc, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = KANBlock(d_tc, hidden_dims=(256, 128), out_dim=d_latent,\n",
    "                            dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, TC):\n",
    "        return self.enc(TC)\n",
    "\n",
    "class RandomHeadKAN(nn.Module):\n",
    "    def __init__(self, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.head = KANBlock(d_latent, hidden_dims=(64,), out_dim=1,\n",
    "                             dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.head(z)\n",
    "\n",
    "class FiLMFromTC(nn.Module):\n",
    "    def __init__(self, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.gamma = KANBlock(d_latent, hidden_dims=(64,), out_dim=d_latent,\n",
    "                              dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "        self.beta = KANBlock(d_latent, hidden_dims=(64,), out_dim=d_latent,\n",
    "                             dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, e_tc):\n",
    "        return self.gamma(e_tc), self.beta(e_tc)\n",
    "\n",
    "class KANAdditiveMixedEffects(nn.Module):\n",
    "    def __init__(self,\n",
    "                 y_dim: int,\n",
    "                 d_fix: int,\n",
    "                 d_tc: int,\n",
    "                 d_zrand: int,\n",
    "                 n_ids: int = None,\n",
    "                 use_id_intercept: bool = False,\n",
    "                 n_kernels: int = 4,\n",
    "                 d_att: int = 128,\n",
    "                 d_fix_latent: int = 128,\n",
    "                 d_rand_latent: int = 128,\n",
    "                 grid_size: int = 8,\n",
    "                 spline_order: int = 3,\n",
    "                 dropout: float = 0.0,\n",
    "                 normalize_att_weights: bool = True,\n",
    "                 use_attention: bool = True,\n",
    "                 use_random: bool = True):\n",
    "        super().__init__()\n",
    "        self.y_dim = y_dim\n",
    "        self.d_fix = d_fix\n",
    "        self.d_tc  = d_tc\n",
    "        self.d_zr  = d_zrand\n",
    "        self.d_rand_latent = d_rand_latent\n",
    "        self.d_att = d_att\n",
    "        self.use_attention = use_attention\n",
    "        self.use_random = use_random\n",
    "\n",
    "        self.att_branches = nn.ModuleList([\n",
    "            TemporalKernelAttentionKAN(n_kernels=n_kernels, d_att=d_att,\n",
    "                                       grid_size=grid_size, spline_order=spline_order,\n",
    "                                       dropout=dropout, normalize_weights=normalize_att_weights)\n",
    "            for _ in range(y_dim)\n",
    "        ]) if use_attention else None\n",
    "\n",
    "        self.fix_branches = nn.ModuleList([\n",
    "            FixedBranchKAN(d_fix=d_fix, d_latent=d_fix_latent,\n",
    "                           grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "            for _ in range(y_dim)\n",
    "        ])\n",
    "\n",
    "        if use_random:\n",
    "            self.rand_encoders = nn.ModuleList([\n",
    "                RandEncoderKAN(d_zrand=d_zrand, d_latent=d_rand_latent,\n",
    "                               grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "                for _ in range(y_dim)\n",
    "            ]) if d_zrand > 0 else None\n",
    "            self.tc_encoders = nn.ModuleList([\n",
    "                TCEncoderKAN(d_tc=d_tc, d_latent=d_rand_latent,\n",
    "                             grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "                for _ in range(y_dim)\n",
    "            ]) if d_tc > 0 else None\n",
    "            self.film_from_tc = nn.ModuleList([\n",
    "                FiLMFromTC(d_latent=d_rand_latent, grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "                for _ in range(y_dim)\n",
    "            ]) if d_tc > 0 else None\n",
    "            self.rand_heads = nn.ModuleList([\n",
    "                RandomHeadKAN(d_latent=d_rand_latent, grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "                for _ in range(y_dim)\n",
    "            ])\n",
    "        else:\n",
    "            self.rand_encoders = None\n",
    "            self.tc_encoders = None\n",
    "            self.film_from_tc = None\n",
    "            self.rand_heads = None\n",
    "\n",
    "    def forward(self, X_fix, TC, Zrand, y_lags, dt_lags,\n",
    "                pid_idx: Optional[torch.Tensor] = None,\n",
    "                pid_seen_mask: Optional[torch.Tensor] = None):\n",
    "        B = X_fix.size(0)\n",
    "\n",
    "        if y_lags.dim() == 2:\n",
    "            y_lags_list = [y_lags for _ in range(self.y_dim)]\n",
    "        elif y_lags.dim() == 3:\n",
    "            assert y_lags.size(2) == self.y_dim\n",
    "            y_lags_list = [y_lags[:, :, j] for j in range(self.y_dim)]\n",
    "        else:\n",
    "            raise ValueError(\"y_lags must be [B, L] or [B, L, y_dim].\")\n",
    "\n",
    "        e_att_all, e_fix_all, e_rand_all = [], [], []\n",
    "        z_att_list, z_fix_list = [], []\n",
    "        z_rand_list, z_rand_film_list, z_tc_list = [], [], []\n",
    "        w_lags_list, e_rand_mod_list, gamma_list, beta_list = [], [], [], []\n",
    "\n",
    "        L = dt_lags.size(1) if dt_lags is not None and dt_lags.dim() == 2 else 1\n",
    "\n",
    "        for j in range(self.y_dim):\n",
    "            if self.use_attention and (self.att_branches is not None):\n",
    "                e_att_j, w_lags_j, z_att_j = self.att_branches[j](y_lags_list[j], dt_lags)\n",
    "            else:\n",
    "                e_att_j = torch.zeros(B, 1, device=X_fix.device, dtype=X_fix.dtype)\n",
    "                w_lags_j = torch.zeros(B, L, device=X_fix.device, dtype=X_fix.dtype)\n",
    "                z_att_j = torch.zeros(B, self.d_att, device=X_fix.device, dtype=X_fix.dtype)\n",
    "\n",
    "            e_fix_j, z_fix_j = self.fix_branches[j](X_fix)\n",
    "\n",
    "            if self.use_random and (self.rand_heads is not None):\n",
    "                if (Zrand is not None) and (Zrand.size(1) > 0) and (self.rand_encoders is not None):\n",
    "                    z_rand_j = self.rand_encoders[j](Zrand)\n",
    "                else:\n",
    "                    z_rand_j = torch.zeros(B, self.d_rand_latent, device=X_fix.device, dtype=X_fix.dtype)\n",
    "\n",
    "                if self.d_tc > 0 and (TC is not None) and (TC.size(1) > 0) and (self.tc_encoders is not None):\n",
    "                    z_tc_j = self.tc_encoders[j](TC)\n",
    "                    gamma_j, beta_j = self.film_from_tc[j](z_tc_j) if self.film_from_tc is not None else (torch.ones_like(z_rand_j), torch.zeros_like(z_rand_j))\n",
    "                else:\n",
    "                    z_tc_j = torch.zeros_like(z_rand_j)\n",
    "                    gamma_j = torch.ones_like(z_rand_j)\n",
    "                    beta_j  = torch.zeros_like(z_rand_j)\n",
    "\n",
    "                z_tilde_j = gamma_j * z_rand_j + beta_j\n",
    "                e_rand_j  = self.rand_heads[j](z_tilde_j)\n",
    "            else:\n",
    "                z_rand_j = torch.zeros(B, self.d_rand_latent, device=X_fix.device, dtype=X_fix.dtype)\n",
    "                z_tc_j   = torch.zeros_like(z_rand_j)\n",
    "                gamma_j  = torch.ones_like(z_rand_j)\n",
    "                beta_j   = torch.zeros_like(z_rand_j)\n",
    "                z_tilde_j= z_rand_j\n",
    "                e_rand_j = torch.zeros(B, 1, device=X_fix.device, dtype=X_fix.dtype)\n",
    "\n",
    "            e_att_all.append(e_att_j)\n",
    "            e_fix_all.append(e_fix_j)\n",
    "            e_rand_all.append(e_rand_j)\n",
    "\n",
    "            z_att_list.append(z_att_j)\n",
    "            z_fix_list.append(z_fix_j)\n",
    "            z_rand_list.append(z_rand_j)\n",
    "            z_rand_film_list.append(z_tilde_j)\n",
    "            z_tc_list.append(z_tc_j)\n",
    "\n",
    "            w_lags_list.append(w_lags_j)\n",
    "            e_rand_mod_list.append(e_rand_j)\n",
    "            gamma_list.append(gamma_j)\n",
    "            beta_list.append(beta_j)\n",
    "\n",
    "        e_att = torch.cat(e_att_all, dim=1) if e_att_all else torch.zeros(B, self.y_dim, device=X_fix.device, dtype=X_fix.dtype)\n",
    "        e_fix = torch.cat(e_fix_all, dim=1) if e_fix_all else torch.zeros(B, self.y_dim, device=X_fix.device, dtype=X_fix.dtype)\n",
    "        e_rand= torch.cat(e_rand_all,dim=1) if e_rand_all else torch.zeros(B, self.y_dim, device=X_fix.device, dtype=X_fix.dtype)\n",
    "        logits = e_att + e_fix + e_rand\n",
    "\n",
    "        parts = {\n",
    "            \"e_att\": e_att,\n",
    "            \"e_fix\": e_fix,\n",
    "            \"e_rand\": e_rand,\n",
    "            \"z_att_list\": z_att_list,\n",
    "            \"z_fix_list\": z_fix_list,\n",
    "            \"z_rand_list\": z_rand_list,\n",
    "            \"z_rand_film_list\": z_rand_film_list,\n",
    "            \"z_tc_list\": z_tc_list,\n",
    "            \"w_lags_list\": w_lags_list,\n",
    "            \"e_rand_mod_list\": e_rand_mod_list,\n",
    "            \"film_gamma_list\": gamma_list,\n",
    "            \"film_beta_list\": beta_list,\n",
    "        }\n",
    "        return logits, parts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba99af",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "14b19c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Optional, Tuple, Any\n",
    "\n",
    "def _bce_logits(\n",
    "    y_hat_logits: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    pos_weight: Optional[torch.Tensor] = None,\n",
    "    sample_weight: Optional[torch.Tensor] = None,\n",
    ") -> torch.Tensor:\n",
    "    if y_true.shape != y_hat_logits.shape:\n",
    "        if y_true.dim() == 1 and y_hat_logits.dim() == 2 and y_hat_logits.size(1) == 1:\n",
    "            y_true = y_true.unsqueeze(1)\n",
    "        else:\n",
    "            raise ValueError(f\"y_true shape {y_true.shape} must equal logits shape {y_hat_logits.shape}\")\n",
    "    pw = None\n",
    "    if pos_weight is not None:\n",
    "        pw = pos_weight.to(y_hat_logits.device, dtype=y_hat_logits.dtype)\n",
    "        if pw.numel() == 1:\n",
    "            pw = pw.view(1)\n",
    "        elif y_hat_logits.dim() == 2 and pw.numel() == y_hat_logits.size(1):\n",
    "            pw = pw.view(-1)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"pos_weight has {pw.numel()} elements but needs 1 or {y_hat_logits.size(-1)}\"\n",
    "            )\n",
    "    sw = None\n",
    "    if sample_weight is not None:\n",
    "        sw = sample_weight.to(y_hat_logits.device, dtype=y_hat_logits.dtype)\n",
    "    return F.binary_cross_entropy_with_logits(\n",
    "        y_hat_logits, y_true.float(), weight=sw, pos_weight=pw\n",
    "    )\n",
    "\n",
    "def _orthogonality_penalty_latents(z_fix: torch.Tensor, z_rand_film: torch.Tensor) -> torch.Tensor:\n",
    "    B = z_fix.size(0)\n",
    "    if B <= 1:\n",
    "        return z_fix.new_zeros(())\n",
    "    zf = z_fix - z_fix.mean(dim=0, keepdim=True)\n",
    "    zr = z_rand_film - z_rand_film.mean(dim=0, keepdim=True)\n",
    "    M = (zf.T @ zr) / float(B)\n",
    "    return (M ** 2).mean()\n",
    "\n",
    "def _optimal_threshold_exact_np(y_true: np.ndarray, y_prob: np.ndarray, beta: float = 1.0) -> float:\n",
    "    y = np.asarray(y_true, dtype=int).ravel()\n",
    "    p = np.asarray(y_prob, dtype=float).ravel()\n",
    "    if y.size == 0:\n",
    "        return 0.5\n",
    "    P = int(y.sum()); N = y.size - P\n",
    "    if P == 0:\n",
    "        return 1.0\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "    o = np.argsort(-p)\n",
    "    p = p[o]; y = y[o]\n",
    "    tp = np.cumsum(y)\n",
    "    fp = np.cumsum(1 - y)\n",
    "    prec = tp / np.maximum(1, tp + fp)\n",
    "    rec  = tp / max(1, P)\n",
    "    f = (1 + beta**2) * prec * rec / np.maximum(1e-12, beta**2 * prec + rec)\n",
    "    idx = int(np.nanargmax(f))\n",
    "    next_p = p[idx+1] if idx + 1 < len(p) else -np.inf\n",
    "    thr = (p[idx] + next_p) / 2.0 if np.isfinite(next_p) and next_p < p[idx] else max(0.0, p[idx] - np.finfo(p.dtype).eps)\n",
    "    return float(thr)\n",
    "\n",
    "def _combine_logits_from_parts(parts: dict, combine: str = \"all\"):\n",
    "    alias = {\n",
    "        \"time_constant\": \"only_fixed\",\n",
    "        \"time_varying\": \"all\",\n",
    "        \"fixed\": \"only_fixed\",\n",
    "    }\n",
    "    mode = alias.get(combine, combine)\n",
    "    e_fix  = parts[\"e_fix\"]\n",
    "    e_rand = parts.get(\"e_rand\", None)\n",
    "    e_att  = parts.get(\"e_att\", None)\n",
    "    if mode == \"only_fixed\":\n",
    "        return e_fix\n",
    "    elif mode == \"fixed_and_random\":\n",
    "        if e_rand is None:\n",
    "            e_rand = torch.zeros_like(e_fix)\n",
    "        return e_fix + e_rand\n",
    "    elif mode == \"all\":\n",
    "        if e_rand is None:\n",
    "            e_rand = torch.zeros_like(e_fix)\n",
    "        if e_att is None:\n",
    "            e_att = torch.zeros_like(e_fix)\n",
    "        return e_fix + e_rand + e_att\n",
    "    else:\n",
    "        raise ValueError(\"combine must be one of {'only_fixed','fixed_and_random','all','time_constant','time_varying','fixed'}\")\n",
    "\n",
    "class KANMixedEffectsWrapper:\n",
    "    def __init__(self, model: nn.Module, cfg: Optional[Dict[str, Any]] = None, device: Optional[torch.device] = None):\n",
    "        self.model = model\n",
    "        self.cfg = {\n",
    "            \"lambda_mean0\": 1e-4,\n",
    "            \"lambda_ridge\": 1e-4,\n",
    "            \"lambda_orth_latent\": 1e-3,\n",
    "            \"lambda_film_identity\": 1e-4,\n",
    "            \"lambda_kan\": 0.0,\n",
    "            \"kan_reg_activation\": 1.0,\n",
    "            \"kan_reg_entropy\": 1.0,\n",
    "            \"clip_grad\": 5.0,\n",
    "            \"lr\": 1e-3,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"max_epochs\": 100,\n",
    "            \"patience\": 10,\n",
    "            \"batch_size\": 256,\n",
    "            \"amp\": True,\n",
    "            \"threshold\": 0.5,\n",
    "            \"auto_pos_weight\": True,\n",
    "            \"pos_weight_eps\": 1e-6,\n",
    "            \"sample_weight_index\": None,\n",
    "        }\n",
    "        if cfg:\n",
    "            self.cfg.update(cfg)\n",
    "        self.device = (\n",
    "            device\n",
    "            or (torch.device(\"mps\") if torch.backends.mps.is_available() else None)\n",
    "            or (torch.device(\"cuda\") if torch.cuda.is_available() else None)\n",
    "            or torch.device(\"cpu\")\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.thresholds_: Optional[np.ndarray] = None\n",
    "        self.history_: Dict[str, list] = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X_fix: torch.Tensor,\n",
    "        TC: Optional[torch.Tensor],\n",
    "        Zrand: Optional[torch.Tensor],\n",
    "        y_lags: torch.Tensor,\n",
    "        dt_lags: torch.Tensor,\n",
    "        pid_idx: Optional[torch.Tensor] = None,\n",
    "        pid_seen_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        return self.model(\n",
    "            X_fix=X_fix,\n",
    "            TC=TC,\n",
    "            Zrand=Zrand,\n",
    "            y_lags=y_lags,\n",
    "            dt_lags=dt_lags,\n",
    "            pid_idx=pid_idx,\n",
    "            pid_seen_mask=pid_seen_mask\n",
    "        )\n",
    "\n",
    "    def _sum_kan_regularization(self) -> torch.Tensor:\n",
    "        if hasattr(self.model, \"regularization_loss\"):\n",
    "            return self.model.regularization_loss(\n",
    "                regularize_activation=self.cfg[\"kan_reg_activation\"],\n",
    "                regularize_entropy=self.cfg[\"kan_reg_entropy\"]\n",
    "            )\n",
    "        device = self.device\n",
    "        reg = torch.tensor(0.0, device=device)\n",
    "        use_att = bool(getattr(self.model, \"use_attention\", True))\n",
    "        use_rand = bool(getattr(self.model, \"use_random\", True))\n",
    "        if hasattr(self.model, \"fix_branches\") and self.model.fix_branches is not None:\n",
    "            for fb in self.model.fix_branches:\n",
    "                if hasattr(fb, \"enc\"):  reg = reg + fb.enc.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "                if hasattr(fb, \"head\"): reg = reg + fb.head.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "        if use_att and hasattr(self.model, \"att_branches\") and self.model.att_branches is not None:\n",
    "            for ab in self.model.att_branches:\n",
    "                if hasattr(ab, \"summarize\"): reg = reg + ab.summarize.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "                if hasattr(ab, \"out_head\"):  reg = reg + ab.out_head.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "        if use_rand:\n",
    "            if hasattr(self.model, \"rand_encoders\") and self.model.rand_encoders is not None:\n",
    "                for re in self.model.rand_encoders:\n",
    "                    if hasattr(re, \"enc\"): reg = reg + re.enc.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "            if hasattr(self.model, \"tc_encoders\") and self.model.tc_encoders is not None:\n",
    "                for te in self.model.tc_encoders:\n",
    "                    if hasattr(te, \"enc\"): reg = reg + te.enc.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "            if hasattr(self.model, \"film_from_tc\") and self.model.film_from_tc is not None:\n",
    "                for film in self.model.film_from_tc:\n",
    "                    if hasattr(film, \"gamma\"): reg = reg + film.gamma.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "                    if hasattr(film, \"beta\"):  reg = reg + film.beta.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "            if hasattr(self.model, \"rand_heads\") and self.model.rand_heads is not None:\n",
    "                for rh in self.model.rand_heads:\n",
    "                    if hasattr(rh, \"head\"): reg = reg + rh.head.regularization_loss(self.cfg[\"kan_reg_activation\"], self.cfg[\"kan_reg_entropy\"])\n",
    "        return reg\n",
    "\n",
    "    def _compute_pos_weight_from_loader(self, train_loader) -> torch.Tensor:\n",
    "        device = self.device\n",
    "        pos_sum = None\n",
    "        total_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in train_loader:\n",
    "                y_b = batch[3].to(device)\n",
    "                if y_b.dim() == 1:\n",
    "                    y_b = y_b.unsqueeze(1)\n",
    "                bs, y_dim = y_b.shape\n",
    "                if pos_sum is None:\n",
    "                    pos_sum = torch.zeros(y_dim, device=device, dtype=torch.float32)\n",
    "                pos_sum += y_b.float().sum(dim=0)\n",
    "                total_sum += bs\n",
    "        if pos_sum is None:\n",
    "            return torch.ones(1, device=device, dtype=torch.float32)\n",
    "        P = pos_sum\n",
    "        T = torch.tensor(float(total_sum), device=device, dtype=torch.float32)\n",
    "        N = T - P\n",
    "        eps = float(self.cfg.get(\"pos_weight_eps\", 1e-6))\n",
    "        pos_weight = N / torch.clamp(P, min=eps)\n",
    "        pos_weight = torch.where(torch.isfinite(pos_weight), pos_weight, torch.ones_like(pos_weight))\n",
    "        pos_weight = torch.clamp(pos_weight, min=eps)\n",
    "        return pos_weight\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        y_true: torch.Tensor,\n",
    "        logits: torch.Tensor,\n",
    "        parts: Dict[str, Any],\n",
    "        *,\n",
    "        X_fix: torch.Tensor,\n",
    "        pid_idx: Optional[torch.Tensor] = None,\n",
    "        TC: Optional[torch.Tensor] = None,\n",
    "        pos_weight: Optional[torch.Tensor] = None,\n",
    "        sample_weight: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "        cfg = self.cfg\n",
    "        use_rand = bool(getattr(self.model, \"use_random\", True))\n",
    "\n",
    "        loss_pred = _bce_logits(logits, y_true, pos_weight=pos_weight, sample_weight=sample_weight)\n",
    "\n",
    "        if use_rand:\n",
    "            e_rand = parts.get(\"e_rand\", None)\n",
    "            if e_rand is None:\n",
    "                e_rand = torch.zeros_like(logits)\n",
    "            loss_mean0 = (e_rand.mean(dim=0) ** 2).sum()\n",
    "            loss_ridge = (e_rand ** 2).mean()\n",
    "            z_fix_list = parts.get(\"z_fix_list\", [])\n",
    "            z_rand_film_list = parts.get(\"z_rand_film_list\", [])\n",
    "            loss_orth = torch.tensor(0.0, device=logits.device)\n",
    "            if len(z_fix_list) and len(z_rand_film_list) and len(z_fix_list) == len(z_rand_film_list):\n",
    "                acc = 0.0\n",
    "                for zf, zr in zip(z_fix_list, z_rand_film_list):\n",
    "                    acc = acc + _orthogonality_penalty_latents(zf, zr)\n",
    "                loss_orth = acc / float(len(z_fix_list))\n",
    "            gamma_list = parts.get(\"film_gamma_list\", [])\n",
    "            beta_list  = parts.get(\"film_beta_list\", [])\n",
    "            loss_film = torch.tensor(0.0, device=logits.device)\n",
    "            if len(gamma_list) and len(beta_list):\n",
    "                acc = 0.0\n",
    "                one = torch.tensor(1.0, device=logits.device, dtype=logits.dtype)\n",
    "                for g, b in zip(gamma_list, beta_list):\n",
    "                    acc = acc + ((g - one) ** 2).mean() + (b ** 2).mean()\n",
    "                loss_film = acc / float(len(gamma_list))\n",
    "        else:\n",
    "            loss_mean0 = torch.tensor(0.0, device=logits.device)\n",
    "            loss_ridge = torch.tensor(0.0, device=logits.device)\n",
    "            loss_orth  = torch.tensor(0.0, device=logits.device)\n",
    "            loss_film  = torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "        kan_reg = torch.tensor(0.0, device=logits.device)\n",
    "        if cfg[\"lambda_kan\"] > 0:\n",
    "            kan_reg = self._sum_kan_regularization()\n",
    "\n",
    "        total_loss = (\n",
    "            loss_pred\n",
    "            + cfg[\"lambda_mean0\"] * loss_mean0\n",
    "            + cfg[\"lambda_ridge\"] * loss_ridge\n",
    "            + cfg[\"lambda_orth_latent\"] * loss_orth\n",
    "            + cfg[\"lambda_film_identity\"] * loss_film\n",
    "            + cfg[\"lambda_kan\"] * kan_reg\n",
    "        )\n",
    "\n",
    "        parts_out = {\n",
    "            \"loss_total\": float(total_loss.detach().cpu()),\n",
    "            \"loss_pred\":  float(loss_pred.detach().cpu()),\n",
    "            \"loss_mean0\": float(loss_mean0.detach().cpu()),\n",
    "            \"loss_ridge\": float(loss_ridge.detach().cpu()),\n",
    "            \"loss_orth\":  float(loss_orth.detach().cpu()),\n",
    "            \"loss_fi\":    float(loss_film.detach().cpu()),\n",
    "            \"loss_kan\":   float(kan_reg.detach().cpu()),\n",
    "        }\n",
    "        return total_loss, parts_out\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_loader,\n",
    "        val_loader=None,\n",
    "        *,\n",
    "        verbose: bool = True,\n",
    "        pos_weight: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        cfg = self.cfg\n",
    "        model = self.model\n",
    "        device = self.device\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "        use_amp = bool(cfg.get(\"amp\", True) and torch.cuda.is_available())\n",
    "        scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "        if pos_weight is None and cfg.get(\"auto_pos_weight\", True):\n",
    "            pos_weight = self._compute_pos_weight_from_loader(train_loader)\n",
    "        if pos_weight is not None:\n",
    "            pos_weight = pos_weight.to(device, dtype=next(model.parameters()).dtype)\n",
    "        best_val = float(\"inf\")\n",
    "        best_state = None\n",
    "        no_improve = 0\n",
    "        for epoch in range(1, cfg[\"max_epochs\"] + 1):\n",
    "            model.train()\n",
    "            total_tr, n_tr = 0.0, 0\n",
    "            for batch in train_loader:\n",
    "                tensors = [b.to(device) for b in batch]\n",
    "                Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b = tensors[:8]\n",
    "                sample_w_b = None\n",
    "                if cfg.get(\"sample_weight_index\") is not None:\n",
    "                    idx_w = int(cfg[\"sample_weight_index\"])\n",
    "                    if idx_w < len(tensors):\n",
    "                        sample_w_b = tensors[idx_w]\n",
    "                TC_in = TC_b if TC_b.size(1) > 0 else None\n",
    "                Zr_in = Zr_b if Zr_b.size(1) > 0 else None\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "                    logits, parts = self.forward(\n",
    "                        X_fix=Xf_b, TC=TC_in, Zrand=Zr_in,\n",
    "                        y_lags=yl_b, dt_lags=dt_b,\n",
    "                        pid_idx=pid_b, pid_seen_mask=seen_b\n",
    "                    )\n",
    "                    loss, _ = self.compute_loss(\n",
    "                        y_true=y_b, logits=logits, parts=parts,\n",
    "                        X_fix=Xf_b, pid_idx=pid_b, TC=TC_in,\n",
    "                        pos_weight=pos_weight,\n",
    "                        sample_weight=sample_w_b,\n",
    "                    )\n",
    "                scaler.scale(loss).backward()\n",
    "                if cfg.get(\"clip_grad\", None):\n",
    "                    scaler.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"clip_grad\"])\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                bs = Xf_b.size(0)\n",
    "                total_tr += float(loss.detach().cpu()) * bs\n",
    "                n_tr += bs\n",
    "            train_loss = total_tr / max(1, n_tr)\n",
    "            self.history_[\"train_loss\"].append(train_loss)\n",
    "            if val_loader is not None:\n",
    "                model.eval()\n",
    "                total_va, n_va = 0.0, 0\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_loader:\n",
    "                        tensors = [b.to(device) for b in batch]\n",
    "                        Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b = tensors[:8]\n",
    "                        sample_w_b = None\n",
    "                        TC_in = TC_b if TC_b.size(1) > 0 else None\n",
    "                        Zr_in = Zr_b if Zr_b.size(1) > 0 else None\n",
    "                        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "                            logits, parts = self.forward(\n",
    "                                X_fix=Xf_b, TC=TC_in, Zrand=Zr_in,\n",
    "                                y_lags=yl_b, dt_lags=dt_b,\n",
    "                                pid_idx=pid_b, pid_seen_mask=seen_b\n",
    "                            )\n",
    "                            l, _ = self.compute_loss(\n",
    "                                y_true=y_b, logits=logits, parts=parts,\n",
    "                                X_fix=Xf_b, pid_idx=pid_b, TC=TC_in,\n",
    "                                pos_weight=pos_weight,\n",
    "                                sample_weight=sample_w_b,\n",
    "                            )\n",
    "                        bs = Xf_b.size(0)\n",
    "                        total_va += float(l.detach().cpu()) * bs\n",
    "                        n_va += bs\n",
    "                val_loss = total_va / max(1, n_va)\n",
    "                self.history_[\"val_loss\"].append(val_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:03d} | train {train_loss:.6f} | val {val_loss:.6f}\")\n",
    "                if val_loss < best_val - 1e-6:\n",
    "                    best_val = val_loss\n",
    "                    best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= cfg[\"patience\"]:\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping at epoch {epoch:03d} (best val {best_val:.6f})\")\n",
    "                        break\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:03d} | train {train_loss:.6f}\")\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        self.thresholds_ = None\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            all_prob, all_true = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    tensors = [b.to(device) for b in batch]\n",
    "                    Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b = tensors[:8]\n",
    "                    TC_in = TC_b if TC_b.size(1) > 0 else None\n",
    "                    Zr_in = Zr_b if Zr_b.size(1) > 0 else None\n",
    "                    logits, _ = self.forward(\n",
    "                        X_fix=Xf_b, TC=TC_in, Zrand=Zr_in,\n",
    "                        y_lags=yl_b, dt_lags=dt_b,\n",
    "                        pid_idx=pid_b, pid_seen_mask=seen_b\n",
    "                    )\n",
    "                    all_prob.append(torch.sigmoid(logits).cpu().numpy())\n",
    "                    all_true.append(y_b.cpu().numpy())\n",
    "            y_prob = np.vstack(all_prob)\n",
    "            y_true = np.vstack(all_true)\n",
    "            y_dim = y_prob.shape[1] if y_prob.ndim == 2 else 1\n",
    "            thr_vec = np.zeros((y_dim,), dtype=float)\n",
    "            for j in range(y_dim):\n",
    "                thr_vec[j] = _optimal_threshold_exact_np(y_true[:, j], y_prob[:, j], beta=1.0)\n",
    "            self.thresholds_ = thr_vec\n",
    "        return {\"best_val_loss\": (best_val if val_loader is not None else self.history_[\"train_loss\"][-1])}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_logits(\n",
    "        self,\n",
    "        X_fix: torch.Tensor,\n",
    "        TC: Optional[torch.Tensor],\n",
    "        Zrand: Optional[torch.Tensor],\n",
    "        y_lags: torch.Tensor,\n",
    "        dt_lags: torch.Tensor,\n",
    "        pid_idx: Optional[torch.Tensor] = None,\n",
    "        pid_seen_mask: Optional[torch.Tensor] = None,\n",
    "        *,\n",
    "        combine: str = \"all\",\n",
    "    ) -> torch.Tensor:\n",
    "        self.model.eval()\n",
    "        logits_full, parts = self.forward(X_fix, TC, Zrand, y_lags, dt_lags, pid_idx, pid_seen_mask)\n",
    "        return _combine_logits_from_parts(parts, combine=combine)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self, *args, combine: str = \"all\", **kwargs) -> torch.Tensor:\n",
    "        logits = self.predict_logits(*args, combine=combine, **kwargs)\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(\n",
    "        self,\n",
    "        *args,\n",
    "        threshold: Optional[float] = None,\n",
    "        thresholds: Optional[np.ndarray] = None,\n",
    "        use_fitted_thresholds: bool = False,\n",
    "        combine: str = \"all\",\n",
    "        **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        probs = self.predict_proba(*args, combine=combine, **kwargs)\n",
    "        if probs.ndim == 2:\n",
    "            B, y_dim = probs.shape\n",
    "        else:\n",
    "            B, y_dim = probs.numel(), 1\n",
    "        if thresholds is not None:\n",
    "            thr = torch.as_tensor(thresholds, device=probs.device, dtype=probs.dtype).view(1, -1)\n",
    "        elif use_fitted_thresholds and (self.thresholds_ is not None):\n",
    "            thr = torch.as_tensor(self.thresholds_, device=probs.device, dtype=probs.dtype).view(1, -1)\n",
    "        else:\n",
    "            thr_scalar = self.cfg.get(\"threshold\", 0.5) if threshold is None else threshold\n",
    "            thr = torch.tensor([thr_scalar], device=probs.device, dtype=probs.dtype).view(1, 1)\n",
    "            if probs.ndim == 2 and probs.size(1) > 1:\n",
    "                thr = thr.expand(1, probs.size(1))\n",
    "        return (probs >= thr).to(torch.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf6f7b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "29fa7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import t as student_t\n",
    "from dataclasses import dataclass\n",
    "\n",
    "_VAR_EPS = 1e-8\n",
    "_STD_EPS = 1e-6\n",
    "_CLIP_Z  = 8.0\n",
    "\n",
    "@dataclass\n",
    "class PCAPipeline:\n",
    "    keep_mask: np.ndarray\n",
    "    mean_: np.ndarray\n",
    "    scale_: np.ndarray\n",
    "    pca: PCA\n",
    "\n",
    "def _fit_pca_pipeline(X_train: np.ndarray, var_ratio: float = 0.95, random_state: int | None = None) -> PCAPipeline:\n",
    "    X = np.asarray(X_train, dtype=np.float64)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    var = X.var(axis=0)\n",
    "    keep = var > _VAR_EPS\n",
    "    if not np.any(keep):\n",
    "        pca = PCA(n_components=0, svd_solver='full', random_state=random_state)\n",
    "        return PCAPipeline(keep_mask=keep, mean_=np.array([], dtype=np.float64),\n",
    "                           scale_=np.array([], dtype=np.float64), pca=pca)\n",
    "    Xk = X[:, keep]\n",
    "    mean = Xk.mean(axis=0)\n",
    "    std  = Xk.std(axis=0)\n",
    "    std  = np.maximum(std, _STD_EPS)\n",
    "    Z = (Xk - mean) / std\n",
    "    Z = np.nan_to_num(Z, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    np.clip(Z, -_CLIP_Z, _CLIP_Z, out=Z)\n",
    "    pca = PCA(n_components=var_ratio, svd_solver='full', random_state=random_state)\n",
    "    pca.fit(Z)\n",
    "    if not np.isfinite(pca.components_).all():\n",
    "        raise RuntimeError(\"PCA components contain non-finite values after fit.\")\n",
    "    return PCAPipeline(keep_mask=keep, mean_=mean, scale_=std, pca=pca)\n",
    "\n",
    "def _transform_pca_pipeline(pipe: PCAPipeline | None, X: np.ndarray | None) -> np.ndarray | None:\n",
    "    if pipe is None or X is None:\n",
    "        return None\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if pipe.keep_mask.size == 0 or not np.any(pipe.keep_mask):\n",
    "        return np.zeros((X.shape[0], 0), dtype=np.float32)\n",
    "    Xk = X[:, pipe.keep_mask]\n",
    "    Z = (Xk - pipe.mean_) / pipe.scale_\n",
    "    Z = np.nan_to_num(Z, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    np.clip(Z, -_CLIP_Z, _CLIP_Z, out=Z)\n",
    "    if not np.isfinite(Z).all():\n",
    "        bad = np.argwhere(~np.isfinite(Z))[0]\n",
    "        raise RuntimeError(f\"[our PCA] Z non-finite at {tuple(bad)}: {Z[tuple(bad)]}\")\n",
    "    if np.abs(Z).max() > 1e6:\n",
    "        raise RuntimeError(f\"[our PCA] Z max |z| too large: {np.abs(Z).max()}\")\n",
    "    if not np.isfinite(pipe.pca.components_).all():\n",
    "        raise RuntimeError(\"[our PCA] components_ non-finite\")\n",
    "    if hasattr(pipe.pca, \"mean_\") and not np.isfinite(pipe.pca.mean_).all():\n",
    "        raise RuntimeError(\"[our PCA] mean_ non-finite\")\n",
    "    Z64 = np.ascontiguousarray(Z, dtype=np.float64)\n",
    "    CT  = np.ascontiguousarray(pipe.pca.components_.T, dtype=np.float64)\n",
    "    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n",
    "        Xt = Z64 @ CT\n",
    "    Xt = np.nan_to_num(Xt, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return Xt\n",
    "\n",
    "def _metrics_binary_full(y_true, y_prob, thr=0.5) -> dict:\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob).astype(float)\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    try: auc = float(roc_auc_score(y_true, y_prob))\n",
    "    except: auc = float(\"nan\")\n",
    "    try: auprc = float(average_precision_score(y_true, y_prob))\n",
    "    except: auprc = float(\"nan\")\n",
    "    brier = float(np.mean((y_prob - y_true) ** 2))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    acc = float((y_pred == y_true).mean())\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "        sens = float(tp / (tp + fn)) if (tp + fn) > 0 else float(\"nan\")\n",
    "        spec = float(tn / (tn + fp)) if (tn + fp) > 0 else float(\"nan\")\n",
    "    except:\n",
    "        sens, spec = float(\"nan\"), float(\"nan\")\n",
    "    return {\"AUC\": auc, \"AUPRC\": auprc, \"Brier\": brier, \"ACC\": acc, \"F1\": float(f1),\n",
    "            \"Precision\": float(prec), \"Recall\": float(rec),\n",
    "            \"Sensitivity\": sens, \"Specificity\": spec}\n",
    "\n",
    "def evaluate_multitask(y_true: np.ndarray, y_prob: np.ndarray, thr=0.5) -> dict:\n",
    "    y_true = np.asarray(y_true); y_prob = np.asarray(y_prob)\n",
    "    if y_true.ndim == 1: y_true = y_true[:, None]\n",
    "    if y_prob.ndim == 1: y_prob = y_prob[:, None]\n",
    "    assert y_true.shape == y_prob.shape\n",
    "    y_dim = y_true.shape[1]\n",
    "    out, macro = {}, {}\n",
    "    for j in range(y_dim):\n",
    "        m = _metrics_binary_full(y_true[:, j], y_prob[:, j], thr)\n",
    "        for k, v in m.items():\n",
    "            out[f\"task_{j+1}_{k}\"] = float(v)\n",
    "            macro.setdefault(k, []).append(float(v))\n",
    "    for k, vals in macro.items():\n",
    "        out[f\"macro_{k}\"] = float(np.nanmean(np.array(vals, dtype=float)))\n",
    "    return out\n",
    "\n",
    "def _evaluate_with_thresholds(y_true: np.ndarray, y_prob: np.ndarray, thresholds: np.ndarray) -> dict:\n",
    "    if y_true.ndim == 1: y_true = y_true[:, None]\n",
    "    if y_prob.ndim == 1: y_prob = y_prob[:, None]\n",
    "    y_dim = y_true.shape[1]\n",
    "    out, macro = {}, {}\n",
    "    for j in range(y_dim):\n",
    "        m = _metrics_binary_full(y_true[:, j], y_prob[:, j], thr=float(thresholds[j]))\n",
    "        for k, v in m.items():\n",
    "            out[f\"task_{j+1}_{k}_optthr\"] = float(v)\n",
    "            macro.setdefault(k, []).append(float(v))\n",
    "    for k, vals in macro.items():\n",
    "        out[f\"macro_{k}_optthr\"] = float(np.nanmean(np.array(vals, dtype=float)))\n",
    "    return out\n",
    "\n",
    "def compute_calibration_curves(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10, strategy: str = \"quantile\") -> dict:\n",
    "    y_true = np.asarray(y_true); y_prob = np.asarray(y_prob)\n",
    "    if y_true.ndim == 1: y_true = y_true[:, None]\n",
    "    if y_prob.ndim == 1: y_prob = y_prob[:, None]\n",
    "    y_dim = y_true.shape[1]\n",
    "    per_task, eces, mces = {}, [], []\n",
    "    for j in range(y_dim):\n",
    "        t = y_true[:, j].astype(int)\n",
    "        p = np.clip(y_prob[:, j].astype(float), 1e-6, 1-1e-6)\n",
    "        try:\n",
    "            frac_pos, mean_pred = calibration_curve(t, p, n_bins=n_bins, strategy=strategy)\n",
    "        except Exception:\n",
    "            frac_pos, mean_pred = np.array([]), np.array([])\n",
    "        if strategy == \"uniform\":\n",
    "            edges = np.linspace(0, 1, n_bins+1)\n",
    "        else:\n",
    "            qs = np.linspace(0, 1, n_bins+1)\n",
    "            edges = np.quantile(p, qs); edges[0], edges[-1] = 0.0, 1.0\n",
    "        bin_ids = np.digitize(p, edges[1:-1], right=True)\n",
    "        counts = np.bincount(bin_ids, minlength=n_bins).astype(float)\n",
    "        N = max(1, len(p))\n",
    "        if len(mean_pred) == n_bins:\n",
    "            weights = counts / N\n",
    "        else:\n",
    "            if len(mean_pred) > 0:\n",
    "                idxs = np.digitize(mean_pred, edges[1:-1], right=True)\n",
    "                weights = counts[idxs] / N\n",
    "            else:\n",
    "                weights = np.array([])\n",
    "        gaps = np.abs(frac_pos - mean_pred) if len(mean_pred) else np.array([np.nan])\n",
    "        ece = float(np.nansum(weights * gaps)) if len(mean_pred) else float(\"nan\")\n",
    "        mce = float(np.nanmax(gaps)) if len(mean_pred) else float(\"nan\")\n",
    "        brier = float(np.mean((p - t) ** 2)) if N > 0 else float(\"nan\")\n",
    "        per_task[j] = {\"mean_pred\": mean_pred.tolist(), \"frac_pos\": frac_pos.tolist(),\n",
    "                       \"counts\": counts.tolist(), \"ece\": ece, \"mce\": mce, \"brier\": brier}\n",
    "        eces.append(ece); mces.append(mce)\n",
    "    return {\"per_task\": per_task, \"macro_ECE\": float(np.nanmean(eces)), \"macro_MCE\": float(np.nanmean(mces))}\n",
    "\n",
    "def _split_cases(pid_array, test_fraction=0.2, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    u = np.unique(pid_array)\n",
    "    te_ids = rng.choice(u, size=max(1, int(len(u)*test_fraction)), replace=False)\n",
    "    te_mask = np.isin(pid_array, te_ids)\n",
    "    return np.where(~te_mask)[0], np.where(te_mask)[0]\n",
    "\n",
    "def _split_time_basic(time_index, test_fraction=0.2):\n",
    "    order = np.argsort(time_index)\n",
    "    n = len(order)\n",
    "    split = int(np.floor(n*(1.0 - test_fraction)))\n",
    "    return order[:split], order[split:]\n",
    "\n",
    "def _filter_time_test_min_measurements(pid_idx: np.ndarray, test_idx: np.ndarray, min_meas: int = 2):\n",
    "    pid = np.asarray(pid_idx)\n",
    "    counts = {pid_val: np.sum(pid == pid_val) for pid_val in np.unique(pid)}\n",
    "    keep = [i for i in test_idx if counts.get(pid[i], 0) >= min_meas]\n",
    "    return np.array(keep, dtype=int)\n",
    "\n",
    "def _concat_safe(*arrays: Optional[np.ndarray]) -> np.ndarray:\n",
    "    parts = [a for a in arrays if a is not None and a.size > 0]\n",
    "    if not parts:\n",
    "        return np.zeros((0, 0), dtype=np.float32)\n",
    "    return np.concatenate(parts, axis=1).astype(np.float32)\n",
    "\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import GroupKFold, TimeSeriesSplit, ParameterGrid\n",
    "from scipy.stats import t as student_t\n",
    "\n",
    "class _KANDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_fix: np.ndarray,\n",
    "        TC: Optional[np.ndarray],\n",
    "        Zrand: Optional[np.ndarray],\n",
    "        y: np.ndarray,\n",
    "        y_lags: np.ndarray,\n",
    "        dt_lags: np.ndarray,\n",
    "        pid_idx: np.ndarray,\n",
    "        pid_seen_mask: np.ndarray,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        N = X_fix.shape[0]\n",
    "        assert y.shape[0] == N and y_lags.shape[0] == N and dt_lags.shape[0] == N and pid_idx.shape[0] == N\n",
    "        def _to_tensor(a, dtype=torch.float32):\n",
    "            if a is None:\n",
    "                return torch.zeros((N, 0), dtype=dtype, device=device)\n",
    "            return torch.as_tensor(a, dtype=dtype, device=device)\n",
    "        self.X_fix  = _to_tensor(X_fix, torch.float32)\n",
    "        self.TC     = _to_tensor(TC,    torch.float32)\n",
    "        self.Zrand  = _to_tensor(Zrand, torch.float32)\n",
    "        self.y      = _to_tensor(y,     torch.float32)\n",
    "        self.y_lags = _to_tensor(y_lags, torch.float32)\n",
    "        self.dt     = _to_tensor(dt_lags, torch.float32)\n",
    "        self.pid    = torch.as_tensor(pid_idx, dtype=torch.long, device=device)\n",
    "        self.seen   = torch.as_tensor(pid_seen_mask.astype(bool), dtype=torch.bool, device=device)\n",
    "    def __len__(self):\n",
    "        return self.X_fix.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            self.X_fix[i], self.TC[i], self.Zrand[i],\n",
    "            self.y[i], self.y_lags[i], self.dt[i],\n",
    "            self.pid[i], self.seen[i]\n",
    "        )\n",
    "\n",
    "def _make_loader_kan(\n",
    "    X_fix, TC, Zrand, y, y_lags, dt_lags,\n",
    "    pid_idx, pid_seen_mask,\n",
    "    batch_size: int,\n",
    "    shuffle: bool,\n",
    "    device: torch.device\n",
    ") -> DataLoader:\n",
    "    ds = _KANDataset(\n",
    "        X_fix, TC, Zrand, y, y_lags, dt_lags, pid_idx, pid_seen_mask, device\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, drop_last=False, num_workers=0)\n",
    "\n",
    "def _optimal_threshold_exact_np(y_true: np.ndarray, y_prob: np.ndarray, beta: float = 1.0) -> float:\n",
    "    y = y_true.astype(int).ravel()\n",
    "    p = y_prob.astype(float).ravel()\n",
    "    if y.size == 0:\n",
    "        return 0.5\n",
    "    P = int(y.sum()); N = y.size - P\n",
    "    if P == 0: return 1.0\n",
    "    if N == 0: return 0.0\n",
    "    o = np.argsort(-p); p = p[o]; y = y[o]\n",
    "    tp = np.cumsum(y); fp = np.cumsum(1 - y)\n",
    "    prec = tp / np.maximum(1, tp + fp)\n",
    "    rec  = tp / max(1, P)\n",
    "    f = (1 + beta**2) * prec * rec / np.maximum(1e-12, beta**2 * prec + rec)\n",
    "    idx = int(np.nanargmax(f))\n",
    "    next_p = p[idx+1] if idx+1 < len(p) else -np.inf\n",
    "    thr = (p[idx] + next_p)/2.0 if np.isfinite(next_p) and next_p < p[idx] else max(0.0, p[idx] - np.finfo(p.dtype).eps)\n",
    "    return float(thr)\n",
    "\n",
    "def _find_best_thresholds_from_loader(wrapper, loader: DataLoader) -> Tuple[np.ndarray, Dict[str, float]]:\n",
    "    wrapper.model.eval()\n",
    "    probs_all, y_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b in loader:\n",
    "            logits = wrapper.predict_logits(\n",
    "                X_fix=Xf_b,\n",
    "                TC=TC_b if TC_b.size(1) > 0 else None,\n",
    "                Zrand=Zr_b if Zr_b.size(1) > 0 else None,\n",
    "                y_lags=yl_b,\n",
    "                dt_lags=dt_b,\n",
    "                pid_idx=pid_b,\n",
    "                pid_seen_mask=seen_b,\n",
    "            )\n",
    "            probs_all.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            y_all.append(y_b.cpu().numpy())\n",
    "    y_prob = np.vstack(probs_all); y_true = np.vstack(y_all)\n",
    "    y_dim = y_prob.shape[1]\n",
    "    thr = np.zeros((y_dim,), dtype=float)\n",
    "    for j in range(y_dim):\n",
    "        thr[j] = _optimal_threshold_exact_np(y_true[:, j], y_prob[:, j], beta=1.0)\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    preds = (y_prob >= thr[None, :]).astype(int)\n",
    "    f1s = []\n",
    "    for j in range(y_dim):\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_true[:, j], preds[:, j], average='binary', zero_division=0)\n",
    "        f1s.append(float(f1))\n",
    "    return thr, {\"macro_F1_trainthr\": float(np.nanmean(f1s))}\n",
    "\n",
    "def _print_split_info(name, idxs, pid_idx_full):\n",
    "    n = int(len(idxs))\n",
    "    u = int(len(np.unique(pid_idx_full[idxs]))) if n > 0 else 0\n",
    "    print(f\"[split] {name:<5} | rows={n:5d} | unique_ids={u:5d}\")\n",
    "\n",
    "def _prepare_split_and_loaders_kan(\n",
    "    X_only_fixed: np.ndarray,\n",
    "    X_fixed_and_random: Optional[np.ndarray],\n",
    "    y: np.ndarray,\n",
    "    y_lags: np.ndarray,\n",
    "    dt_lags: np.ndarray,\n",
    "    pid_idx_full: np.ndarray,\n",
    "    indices_train: np.ndarray,\n",
    "    indices_val: Optional[np.ndarray],\n",
    "    indices_test: np.ndarray,\n",
    "    scenario: str,\n",
    "    batch_size: int,\n",
    "    device: torch.device,\n",
    "    random_state: int = 42,\n",
    "    pca_var_ratio: float = 0.95,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    of_pipe = _fit_pca_pipeline(X_only_fixed[indices_train], var_ratio=pca_var_ratio, random_state=random_state)\n",
    "    fr_pipe = None\n",
    "    if X_fixed_and_random is not None and X_fixed_and_random.shape[1] > 0:\n",
    "        fr_pipe = _fit_pca_pipeline(X_fixed_and_random[indices_train], var_ratio=pca_var_ratio, random_state=random_state)\n",
    "    def transform_block(idxs):\n",
    "        of = _transform_pca_pipeline(of_pipe, X_only_fixed[idxs])\n",
    "        fr = _transform_pca_pipeline(fr_pipe, None if X_fixed_and_random is None else X_fixed_and_random[idxs])\n",
    "        X_fix = _concat_safe(of, fr)\n",
    "        TC    = of\n",
    "        Zr    = fr\n",
    "        return X_fix, TC, Zr\n",
    "    if verbose:\n",
    "        _print_split_info(\"train\", indices_train, pid_idx_full)\n",
    "        if indices_val is not None:\n",
    "            _print_split_info(\"val\", indices_val, pid_idx_full)\n",
    "        _print_split_info(\"test\", indices_test, pid_idx_full)\n",
    "    Xf_tr, TC_tr, Zr_tr = transform_block(indices_train)\n",
    "    Xf_te, TC_te, Zr_te = transform_block(indices_test)\n",
    "    if indices_val is not None:\n",
    "        Xf_va, TC_va, Zr_va = transform_block(indices_val)\n",
    "    else:\n",
    "        Xf_va = TC_va = Zr_va = None\n",
    "    if scenario == \"cases\":\n",
    "        seen_pids = set(pid_idx_full[indices_train].tolist())\n",
    "        seen_tr = np.ones(indices_train.shape[0], dtype=bool)\n",
    "        seen_va = np.ones(indices_val.shape[0], dtype=bool) if indices_val is not None else None\n",
    "        seen_te = np.array([p in seen_pids for p in pid_idx_full[indices_test]], dtype=bool)\n",
    "    else:\n",
    "        seen_tr = np.ones(indices_train.shape[0], dtype=bool)\n",
    "        seen_va = np.ones(indices_val.shape[0], dtype=bool) if indices_val is not None else None\n",
    "        seen_te = np.ones(indices_test.shape[0], dtype=bool)\n",
    "    tr_loader = _make_loader_kan(\n",
    "        Xf_tr, TC_tr, Zr_tr, y[indices_train], y_lags[indices_train], dt_lags[indices_train],\n",
    "        pid_idx_full[indices_train], seen_tr,\n",
    "        batch_size=batch_size, shuffle=True, device=device\n",
    "    )\n",
    "    va_loader = None\n",
    "    if indices_val is not None:\n",
    "        va_loader = _make_loader_kan(\n",
    "            Xf_va, TC_va, Zr_va, y[indices_val], y_lags[indices_val], dt_lags[indices_val],\n",
    "            pid_idx_full[indices_val], seen_va,\n",
    "            batch_size=batch_size, shuffle=False, device=device\n",
    "        )\n",
    "    te_loader = _make_loader_kan(\n",
    "        Xf_te, TC_te, Zr_te, y[indices_test], y_lags[indices_test], dt_lags[indices_test],\n",
    "        pid_idx_full[indices_test], seen_te,\n",
    "        batch_size=batch_size, shuffle=False, device=device\n",
    "    )\n",
    "    preprocessors = {\n",
    "        \"of_pipe\": of_pipe,\n",
    "        \"fr_pipe\": fr_pipe,\n",
    "        \"d_fix\":   Xf_tr.shape[1],\n",
    "        \"d_tc\":    (Xf_tr.shape[1] - (0 if Zr_tr is None else Zr_tr.shape[1])),\n",
    "        \"d_zrand\": 0 if Zr_tr is None else Zr_tr.shape[1],\n",
    "        \"n_ids\":   int(len(np.unique(pid_idx_full))),\n",
    "    }\n",
    "    loaders = {\"train\": tr_loader, \"val\": va_loader, \"test\": te_loader}\n",
    "    return preprocessors, loaders\n",
    "\n",
    "def _model_param_count(model: torch.nn.Module) -> int:\n",
    "    return int(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "def _fit_eval_once_kan(\n",
    "    build_model_fn, wrapper_cls,\n",
    "    arch_params: Dict[str, Any],\n",
    "    train_params: Dict[str, Any],\n",
    "    X_only_fixed: np.ndarray,\n",
    "    X_fixed_and_random: Optional[np.ndarray],\n",
    "    y: np.ndarray,\n",
    "    y_lags: np.ndarray,\n",
    "    dt_lags: np.ndarray,\n",
    "    pid_idx_full: np.ndarray,\n",
    "    tr_idx: np.ndarray,\n",
    "    va_idx: Optional[np.ndarray],\n",
    "    te_idx: np.ndarray,\n",
    "    device: torch.device,\n",
    "    scenario: str,\n",
    "    threshold_selection_source: str = \"train\",\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    preprocessors, loaders = _prepare_split_and_loaders_kan(\n",
    "        X_only_fixed, X_fixed_and_random, y, y_lags, dt_lags, pid_idx_full,\n",
    "        tr_idx, va_idx, te_idx, scenario,\n",
    "        batch_size=train_params.get(\"batch_size\", 256),\n",
    "        device=device,\n",
    "        random_state=train_params.get(\"random_state\", 42),\n",
    "        pca_var_ratio=train_params.get(\"pca_var_ratio\", 0.95),\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    y_dim = y.shape[1] if y.ndim == 2 else 1\n",
    "    model = build_model_fn(\n",
    "        y_dim=y_dim,\n",
    "        d_fix=preprocessors[\"d_fix\"],\n",
    "        d_tc=preprocessors[\"d_tc\"],\n",
    "        d_zrand=preprocessors[\"d_zrand\"],\n",
    "        n_ids=preprocessors[\"n_ids\"],\n",
    "        **arch_params\n",
    "    ).to(device)\n",
    "    wrapper = wrapper_cls(model, cfg=train_params, device=device)\n",
    "    wrapper.fit(\n",
    "        loaders[\"train\"], loaders[\"val\"],\n",
    "        verbose=verbose,\n",
    "        pos_weight=train_params.get(\"pos_weight\", None)\n",
    "    )\n",
    "    wrapper.model.eval()\n",
    "    probs_all, y_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b in loaders[\"test\"]:\n",
    "            logits = wrapper.predict_logits(\n",
    "                X_fix=Xf_b,\n",
    "                TC=TC_b if TC_b.size(1) > 0 else None,\n",
    "                Zrand=Zr_b if Zr_b.size(1) > 0 else None,\n",
    "                y_lags=yl_b,\n",
    "                dt_lags=dt_b,\n",
    "                pid_idx=pid_b,\n",
    "                pid_seen_mask=seen_b,\n",
    "            )\n",
    "            probs_all.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            y_all.append(y_b.cpu().numpy())\n",
    "    y_prob_te = np.vstack(probs_all); y_true_te = np.vstack(y_all)\n",
    "    metrics_050 = evaluate_multitask(y_true_te, y_prob_te, thr=train_params.get(\"threshold\", 0.5))\n",
    "    if threshold_selection_source == \"train\":\n",
    "        thr_vec, thr_summary = _find_best_thresholds_from_loader(wrapper, loaders[\"train\"])\n",
    "    elif threshold_selection_source == \"val\" and loaders[\"val\"] is not None:\n",
    "        thr_vec, thr_summary = _find_best_thresholds_from_loader(wrapper, loaders[\"val\"])\n",
    "    else:\n",
    "        thr_vec, thr_summary = _find_best_thresholds_from_loader(wrapper, loaders[\"test\"])\n",
    "    metrics_opt = _evaluate_with_thresholds(y_true_te, y_prob_te, thr_vec)\n",
    "    calib = compute_calibration_curves(\n",
    "        y_true_te, y_prob_te,\n",
    "        n_bins=train_params.get(\"calibration_bins\", 10),\n",
    "        strategy=train_params.get(\"calibration_strategy\", \"quantile\"),\n",
    "    )\n",
    "    y_pred_050 = (y_prob_te >= float(train_params.get(\"threshold\", 0.5))).astype(int)\n",
    "    y_pred_opt = (y_prob_te >= thr_vec[None, :]).astype(int)\n",
    "    return {\n",
    "        \"metrics@0.5\": metrics_050,\n",
    "        \"metrics@optthr\": metrics_opt,\n",
    "        \"opt_thresholds\": thr_vec,\n",
    "        \"opt_thresholds_summary\": thr_summary,\n",
    "        \"preprocessors\": preprocessors,\n",
    "        \"wrapper\": wrapper,\n",
    "        \"model\": wrapper.model,\n",
    "        \"model_class\": wrapper.model.__class__.__name__,\n",
    "        \"model_param_count\": _model_param_count(wrapper.model),\n",
    "        \"arch_params_final\": dict(arch_params),\n",
    "        \"train_params_final\": dict(train_params),\n",
    "        \"macro_ECE\": calib[\"macro_ECE\"],\n",
    "        \"macro_MCE\": calib[\"macro_MCE\"],\n",
    "        \"y_true_test\": y_true_te,\n",
    "        \"y_prob_test\": y_prob_te,\n",
    "        \"y_pred_test@0.5\": y_pred_050,\n",
    "        \"y_pred_test@optthr\": y_pred_opt,\n",
    "    }\n",
    "\n",
    "def run_training_and_eval_kan(\n",
    "    X_only_fixed: np.ndarray,\n",
    "    X_fixed_and_random: Optional[np.ndarray],\n",
    "    y: np.ndarray,\n",
    "    y_lags: np.ndarray,\n",
    "    dt_lags: np.ndarray,\n",
    "    pid_idx: np.ndarray,\n",
    "    time_index: np.ndarray,\n",
    "    build_model_fn,\n",
    "    wrapper_cls,\n",
    "    *,\n",
    "    mode: str = \"single\",\n",
    "    scenario: str = \"cases\",\n",
    "    outer_folds: int = 5,\n",
    "    inner_folds: int = 3,\n",
    "    param_grid: Optional[Dict[str, List]] = None,\n",
    "    arch_defaults: Optional[Dict[str, Any]] = None,\n",
    "    train_defaults: Optional[Dict[str, Any]] = None,\n",
    "    device: Optional[torch.device] = None,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    device = (\n",
    "        device\n",
    "        or (torch.device(\"mps\") if torch.backends.mps.is_available() else None)\n",
    "        or (torch.device(\"cuda\") if torch.cuda.is_available() else None)\n",
    "        or torch.device(\"cpu\")\n",
    "    )\n",
    "    if scenario == \"both\":\n",
    "        out_cases = run_training_and_eval_kan(\n",
    "            X_only_fixed, X_fixed_and_random, y, y_lags, dt_lags, pid_idx, time_index,\n",
    "            build_model_fn, wrapper_cls,\n",
    "            mode=mode, scenario=\"cases\", outer_folds=outer_folds, inner_folds=inner_folds,\n",
    "            param_grid=param_grid, arch_defaults=arch_defaults, train_defaults=train_defaults,\n",
    "            device=device, verbose=verbose\n",
    "        )\n",
    "        out_time = run_training_and_eval_kan(\n",
    "            X_only_fixed, X_fixed_and_random, y, y_lags, dt_lags, pid_idx, time_index,\n",
    "            build_model_fn, wrapper_cls,\n",
    "            mode=mode, scenario=\"time\", outer_folds=outer_folds, inner_folds=inner_folds,\n",
    "            param_grid=param_grid, arch_defaults=arch_defaults, train_defaults=train_defaults,\n",
    "            device=device, verbose=verbose\n",
    "        )\n",
    "        return {\"scenario_cases\": out_cases, \"scenario_time\": out_time}\n",
    "    X_of = np.asarray(X_only_fixed, dtype=np.float32)\n",
    "    X_fr = None if X_fixed_and_random is None else np.asarray(X_fixed_and_random, dtype=np.float32)\n",
    "    y    = np.asarray(y, dtype=np.float32)\n",
    "    if y.ndim == 1: y = y[:, None]\n",
    "    y_lags = np.asarray(y_lags, dtype=np.float32)\n",
    "    dt_lags = np.asarray(dt_lags, dtype=np.float32)\n",
    "    pid_idx = np.asarray(pid_idx, dtype=np.int64)\n",
    "    time_ix = np.asarray(time_index)\n",
    "    arch_defaults = arch_defaults or {}\n",
    "    train_defaults = train_defaults or {}\n",
    "    rnd = int(train_defaults.get(\"random_state\", 42))\n",
    "    val_frac = float(train_defaults.get(\"val_fraction\", 0.10))\n",
    "    thr_source = train_defaults.get(\"threshold_selection_source\", \"train\")\n",
    "    def _make_train_val_split(idx_array: np.ndarray, seed: int) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        idx_array = np.asarray(idx_array)\n",
    "        if len(idx_array) <= 10 or val_frac <= 0.0:\n",
    "            return idx_array, None\n",
    "        rng = np.random.default_rng(seed)\n",
    "        perm = rng.permutation(len(idx_array))\n",
    "        cut = max(1, int(val_frac * len(idx_array)))\n",
    "        va_sel, tr_sel = perm[:cut], perm[cut:]\n",
    "        return idx_array[tr_sel], idx_array[va_sel]\n",
    "    if mode == \"single\":\n",
    "        if scenario == \"cases\":\n",
    "            tr_idx_all, te_idx = _split_cases(pid_idx, test_fraction=0.2, seed=rnd)\n",
    "        elif scenario == \"time\":\n",
    "            tr_idx_all, te_idx_raw = _split_time_basic(time_ix, test_fraction=0.2)\n",
    "            te_idx = _filter_time_test_min_measurements(\n",
    "                pid_idx, te_idx_raw, min_meas=train_defaults.get(\"min_meas_test\", 3)\n",
    "            )\n",
    "            if len(te_idx) == 0:\n",
    "                raise RuntimeError(\"Time split produced empty test after 3-measurements filter.\")\n",
    "        else:\n",
    "            raise ValueError(\"scenario must be 'cases' or 'time'\")\n",
    "        tr_idx, va_idx = _make_train_val_split(tr_idx_all, seed=rnd)\n",
    "        res = _fit_eval_once_kan(\n",
    "            build_model_fn, wrapper_cls,\n",
    "            arch_defaults, train_defaults,\n",
    "            X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "            tr_idx, va_idx, te_idx,\n",
    "            device=device, scenario=scenario,\n",
    "            threshold_selection_source=thr_source,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"\\nSingle-fit test metrics @0.5:\")\n",
    "            for k, v in res[\"metrics@0.5\"].items():\n",
    "                print(f\"{k:>18}: {v:.4f}\")\n",
    "            print(\"\\nSingle-fit test metrics @F1-opt per task:\")\n",
    "            for k, v in res[\"metrics@optthr\"].items():\n",
    "                print(f\"{k:>18}: {v:.4f}\")\n",
    "            print(f\"\\nModel: {res['model_class']} | params={res['model_param_count']}\")\n",
    "            print(\"Final arch params:\", res[\"arch_params_final\"])\n",
    "            print(\"Final train params:\", res[\"train_params_final\"])\n",
    "        return res\n",
    "    if mode == \"cv_only\":\n",
    "        fold_metrics_050, fold_metrics_opt = [], []\n",
    "        if scenario == \"cases\":\n",
    "            outer = GroupKFold(n_splits=outer_folds)\n",
    "            outer_iter = outer.split(X_of, y[:, 0], groups=pid_idx)\n",
    "        else:\n",
    "            tss = TimeSeriesSplit(n_splits=outer_folds)\n",
    "            order = np.argsort(time_ix)\n",
    "            X_order = X_of[order]; y_order = y[order]\n",
    "            outer_iter = ((order[tr], order[te]) for tr, te in tss.split(X_order, y_order[:, 0]))\n",
    "        for fold_id, (tr_idx_all, te_idx) in enumerate(outer_iter, start=1):\n",
    "            tr_idx, va_idx = _make_train_val_split(np.asarray(tr_idx_all), seed=rnd + fold_id)\n",
    "            if verbose:\n",
    "                print(f\"\\n[CV fold {fold_id}/{outer_folds}] scenario={scenario}\")\n",
    "            res = _fit_eval_once_kan(\n",
    "                build_model_fn, wrapper_cls,\n",
    "                arch_defaults, train_defaults,\n",
    "                X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                tr_idx, va_idx, te_idx,\n",
    "                device=device, scenario=scenario,\n",
    "                threshold_selection_source=thr_source,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            fold_metrics_050.append(res[\"metrics@0.5\"])\n",
    "            fold_metrics_opt.append(res[\"metrics@optthr\"])\n",
    "            if verbose:\n",
    "                macro_keys = [k for k in res[\"metrics@optthr\"].keys() if k.startswith(\"macro_\")]\n",
    "                print(\"Fold macro (optthr): \" + \", \".join(f\"{k}={res['metrics@optthr'][k]:.4f}\" for k in macro_keys))\n",
    "        def _summarize_cv_folds(results_folds: List[Dict[str, float]]) -> Dict[str, float]:\n",
    "            if not results_folds:\n",
    "                return {}\n",
    "            all_keys = set().union(*results_folds)\n",
    "            summary = {}\n",
    "            for k in sorted(all_keys):\n",
    "                vals = np.array([fold.get(k, np.nan) for fold in results_folds], dtype=float)\n",
    "                mask = np.isfinite(vals); n = int(mask.sum())\n",
    "                if n == 0:\n",
    "                    m = low = high = np.nan\n",
    "                elif n == 1:\n",
    "                    m = float(vals[mask][0]); low = high = np.nan\n",
    "                else:\n",
    "                    m = float(np.nanmean(vals)); s = float(np.nanstd(vals, ddof=1)); se = s/np.sqrt(n)\n",
    "                    tcrit = float(student_t.ppf(0.975, df=n-1))\n",
    "                    low, high = m - tcrit*se, m + tcrit*se\n",
    "                summary[f\"{k}_mean\"] = m\n",
    "                summary[f\"{k}_95ci_low\"] = low\n",
    "                summary[f\"{k}_95ci_high\"] = high\n",
    "            return summary\n",
    "        cv_summary_050 = _summarize_cv_folds(fold_metrics_050)\n",
    "        cv_summary_opt = _summarize_cv_folds(fold_metrics_opt)\n",
    "        if verbose:\n",
    "            print(\"\\nCV averages (95% CI) @0.5:\")\n",
    "            for key in sorted(cv_summary_050.keys()):\n",
    "                if key.endswith(\"_mean\"):\n",
    "                    base = key[:-5]\n",
    "                    low = cv_summary_050.get(f\"{base}_95ci_low\", np.nan)\n",
    "                    high = cv_summary_050.get(f\"{base}_95ci_high\", np.nan)\n",
    "                    print(f\"{base:>20}: {cv_summary_050[key]:.4f}  (95% CI {low:.4f}, {high:.4f})\")\n",
    "            print(\"\\nCV averages (95% CI) @opt thresholds:\")\n",
    "            for key in sorted(cv_summary_opt.keys()):\n",
    "                if key.endswith(\"_mean\"):\n",
    "                    base = key[:-5]\n",
    "                    low = cv_summary_opt.get(f\"{base}_95ci_low\", np.nan)\n",
    "                    high = cv_summary_opt.get(f\"{base}_95ci_high\", np.nan)\n",
    "                    print(f\"{base:>20}: {cv_summary_opt[key]:.4f}  (95% CI {low:.4f}, {high:.4f})\")\n",
    "            print(\"Arch params (used in all folds):\", arch_defaults)\n",
    "            print(\"Train params (used in all folds):\", train_defaults)\n",
    "        return {\n",
    "            \"cv_folds_metrics@0.5\": fold_metrics_050,\n",
    "            \"cv_folds_metrics@optthr\": fold_metrics_opt,\n",
    "            \"cv_summary@0.5\": cv_summary_050,\n",
    "            \"cv_summary@optthr\": cv_summary_opt,\n",
    "            \"arch_params_final\": dict(arch_defaults),\n",
    "            \"train_params_final\": dict(train_defaults),\n",
    "        }\n",
    "    if mode == \"nested_cv\":\n",
    "        if not param_grid:\n",
    "            param_grid = {\n",
    "                \"d_fix_latent\": [128, 256],\n",
    "                \"d_rand_latent\": [128],\n",
    "                \"n_kernels\": [4],\n",
    "                \"dropout\": [0.0, 0.1],\n",
    "                \"lr\": [1e-3, 3e-4],\n",
    "                \"weight_decay\": [0.0, 1e-4],\n",
    "                \"batch_size\": [256],\n",
    "                \"max_epochs\": [100],\n",
    "                \"patience\": [10],\n",
    "            }\n",
    "        results_folds = []\n",
    "        best_score_global, best_params_global = -np.inf, None\n",
    "        if scenario == \"cases\":\n",
    "            outer = GroupKFold(n_splits=outer_folds)\n",
    "            outer_iter = outer.split(X_of, y[:, 0], groups=pid_idx)\n",
    "        else:\n",
    "            tss = TimeSeriesSplit(n_splits=outer_folds)\n",
    "            order = np.argsort(time_ix)\n",
    "            X_order = X_of[order]; y_order = y[order]\n",
    "            outer_iter = ((order[tr], order[te]) for tr, te in tss.split(X_order, y_order[:, 0]))\n",
    "        for fold_id, (tr_idx_all, te_idx) in enumerate(outer_iter, start=1):\n",
    "            if verbose:\n",
    "                print(f\"\\nOuter fold {fold_id}/{outer_folds} scenario={scenario}\")\n",
    "            def inner_iter():\n",
    "                if scenario == \"cases\":\n",
    "                    inner = GroupKFold(n_splits=inner_folds)\n",
    "                    return inner.split(X_of[tr_idx_all], y[tr_idx_all, 0], groups=pid_idx[tr_idx_all])\n",
    "                else:\n",
    "                    tr_order = np.argsort(time_ix[tr_idx_all])\n",
    "                    X_tr_order = X_of[tr_idx_all][tr_order]\n",
    "                    y_tr_order = y[tr_idx_all][tr_order]\n",
    "                    inner_tss = TimeSeriesSplit(n_splits=inner_folds)\n",
    "                    return ((tr_idx_all[tr_order][itr], tr_idx_all[tr_order][iva])\n",
    "                            for itr, iva in inner_tss.split(X_tr_order, y_tr_order[:, 0]))\n",
    "            best_inner_score, best_inner_params = -np.inf, None\n",
    "            for params in ParameterGrid(param_grid):\n",
    "                arch_params = dict(arch_defaults)\n",
    "                train_params = dict(train_defaults)\n",
    "                for k, v in params.items():\n",
    "                    if k in (\"d_fix_latent\", \"d_rand_latent\", \"n_kernels\", \"dropout\", \"use_attention\", \"use_random\"):\n",
    "                        arch_params[k] = v\n",
    "                    else:\n",
    "                        train_params[k] = v\n",
    "                inner_scores = []\n",
    "                for in_tr, in_va in inner_iter():\n",
    "                    tr_idx_inner, va_idx_inner = _make_train_val_split(np.asarray(in_tr), seed=rnd + fold_id)\n",
    "                    res_inner = _fit_eval_once_kan(\n",
    "                        build_model_fn, wrapper_cls,\n",
    "                        arch_params, train_params,\n",
    "                        X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                        tr_idx_inner, va_idx_inner, in_va,\n",
    "                        device=device, scenario=scenario,\n",
    "                        threshold_selection_source=\"train\",\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    score = res_inner[\"metrics@optthr\"].get(\"macro_F1_optthr\", np.nan)\n",
    "                    inner_scores.append(score)\n",
    "                avg_score = float(np.nanmean(inner_scores)) if len(inner_scores) else -np.inf\n",
    "                if avg_score > best_inner_score:\n",
    "                    best_inner_score = avg_score\n",
    "                    best_inner_params = (arch_params, train_params)\n",
    "            if best_inner_params is None:\n",
    "                if verbose: print(\"No viable inner config; skipping outer fold.\")\n",
    "                continue\n",
    "            arch_params, train_params = best_inner_params\n",
    "            tr_idx_outer, va_idx_outer = _make_train_val_split(np.asarray(tr_idx_all), seed=rnd + fold_id * 17)\n",
    "            res_outer = _fit_eval_once_kan(\n",
    "                build_model_fn, wrapper_cls,\n",
    "                arch_params, train_params,\n",
    "                X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                tr_idx_outer, va_idx_outer, te_idx,\n",
    "                device=device, scenario=scenario,\n",
    "                threshold_selection_source=\"train\",\n",
    "                verbose=False\n",
    "            )\n",
    "            results_folds.append(res_outer)\n",
    "            score_outer = res_outer[\"metrics@optthr\"].get(\"macro_F1_optthr\", -np.inf)\n",
    "            if score_outer > best_score_global:\n",
    "                best_score_global = score_outer\n",
    "                best_params_global = (arch_params, train_params)\n",
    "            if verbose:\n",
    "                macro_keys = [k for k in res_outer[\"metrics@optthr\"].keys() if k.startswith(\"macro_\")]\n",
    "                print(\"Outer fold macro (optthr): \" + \", \".join(f\"{k}={res_outer['metrics@optthr'][k]:.4f}\" for k in macro_keys))\n",
    "        def _summarize_block(results_list: List[Dict[str, Any]], which: str) -> Dict[str, float]:\n",
    "            keys = list(results_list[0][which].keys())\n",
    "            out = {}\n",
    "            for k in keys:\n",
    "                arr = np.array([res[which][k] for res in results_list], dtype=float)\n",
    "                m = float(np.nanmean(arr)); s = float(np.nanstd(arr, ddof=1)); n = len(arr)\n",
    "                se = s / np.sqrt(n) if n > 1 else np.nan\n",
    "                if n > 1:\n",
    "                    tcrit = float(student_t.ppf(0.975, df=n-1))\n",
    "                    ci = (m - tcrit * se, m + tcrit * se)\n",
    "                else:\n",
    "                    ci = (np.nan, np.nan)\n",
    "                out[k + \"_mean\"] = m\n",
    "                out[k + \"_95ci_low\"] = ci[0]\n",
    "                out[k + \"_95ci_high\"] = ci[1]\n",
    "            return out\n",
    "        cv_summary_050    = _summarize_block(results_folds, \"metrics@0.5\")\n",
    "        cv_summary_optthr = _summarize_block(results_folds, \"metrics@optthr\")\n",
    "        if verbose and best_params_global is not None:\n",
    "            print(\"\\nBest params (by outer macro_F1 at opt thresholds):\")\n",
    "            arch_p, train_p = best_params_global\n",
    "            print(\"[ARCH]:\");   [print(f\"  {k}: {v}\") for k, v in arch_p.items()]\n",
    "            print(\"[TRAIN]:\");  [print(f\"  {k}: {v}\") for k, v in train_p.items()]\n",
    "        print(\"\\nCross-validation results:\")\n",
    "        print(\"CV Summary @ 0.5:\")\n",
    "        print(cv_summary_050)\n",
    "        print(\"\\nCV Summary @ Optimal Threshold:\")\n",
    "        print(cv_summary_optthr)\n",
    "        return {\n",
    "            \"outer_folds\": results_folds,\n",
    "            \"cv_summary@0.5\": cv_summary_050,\n",
    "            \"cv_summary@optthr\": cv_summary_optthr,\n",
    "            \"best_params\": (best_params_global[0], best_params_global[1]) if best_params_global else None,\n",
    "        }\n",
    "    raise ValueError(\"mode must be one of {'single','cv_only','nested_cv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ef43a",
   "metadata": {},
   "source": [
    "## Model test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac406d",
   "metadata": {},
   "source": [
    "### Define variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0232db3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1031, 1) (1031, 130) (1031, 66) (1031,) (1031,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ---------- targets & aux ----------\n",
    "y_raw      = GHQ_cat_y.to_numpy(np.float32)\n",
    "y_np       = y_raw if y_raw.ndim == 2 else y_raw.reshape(-1, 1)\n",
    "\n",
    "y_lags_np  = GHQ_cat_outcomes_lags.to_numpy(np.float32)\n",
    "dt_lags_np = GHQ_cat_forecast_horizons.to_numpy(np.float32)\n",
    "\n",
    "# ---------- inputs ----------\n",
    "# keep the two blocks SEPARATE; evaluator will:\n",
    "#   - TC  <- only_fixed (after PCA)\n",
    "#   - X_fix <- concat(only_fixed_PCA, fixed_and_random_PCA)\n",
    "#   - Zrand <- fixed_and_random_PCA\n",
    "X_only_fixed_np        = GHQ_cat_only_fixed.to_numpy(np.float32)\n",
    "X_fixed_and_random_np  = GHQ_cat_fixed_and_random.to_numpy(np.float32)\n",
    "\n",
    "# ---------- ids & time ----------\n",
    "pid_raw    = GHQ_cat_participant_id.to_numpy().ravel()\n",
    "pid_uniqs, pid_encoded = np.unique(pid_raw, return_inverse=True)\n",
    "pid_np     = pid_encoded.astype(np.int64)\n",
    "n_ids      = int(len(pid_uniqs))\n",
    "\n",
    "time_ix_np = GHQ_cat_time.to_numpy().ravel()\n",
    "\n",
    "# ---------- dynamic builder ----------\n",
    "def build_model_fn(\n",
    "    *,\n",
    "    y_dim: int,\n",
    "    d_fix: int,\n",
    "    d_tc: int,        # this will be the PCAd only_fixed dim (activates FiLM)\n",
    "    d_zrand: int,     # this will be the PCAd fixed_and_random dim\n",
    "    n_ids: int,\n",
    "    **arch\n",
    "):\n",
    "    return KANAdditiveMixedEffects(\n",
    "        y_dim=y_dim,\n",
    "        d_fix=d_fix,       # = dim(concat(only_fixed_PCA, fixed_and_random_PCA))\n",
    "        d_tc=d_tc,         # = dim(only_fixed_PCA)  -> used by FiLM\n",
    "        d_zrand=d_zrand,   # = dim(fixed_and_random_PCA)\n",
    "        n_ids=n_ids,\n",
    "        **arch\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "arch_defaults = dict(d_fix_latent=256, d_rand_latent=256, n_kernels=8, dropout=0.0)\n",
    "train_defaults = dict(\n",
    "    lr=3e-4, weight_decay=3e-6, batch_size=256,\n",
    "    max_epochs=100, patience=20, threshold=0.5,\n",
    "    lambda_mean0=1e-3, lambda_ridge=3e-3,\n",
    "    lambda_orth_latent=3e-3,\n",
    "    lambda_film_identity=3e-3,\n",
    "    lambda_kan=0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(y_np.shape, X_only_fixed_np.shape, X_fixed_and_random_np.shape, pid_np.shape, time_ix_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ec280",
   "metadata": {},
   "source": [
    "### Simple cases split test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c8348913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[split] train | rows=  742 | unique_ids=  247\n",
      "[split] val   | rows=   82 | unique_ids=   72\n",
      "[split] test  | rows=  204 | unique_ids=   93\n",
      "Epoch 001 | train 0.655723 | val 0.654983\n",
      "Epoch 002 | train 0.647988 | val 0.648192\n",
      "Epoch 003 | train 0.638431 | val 0.637774\n",
      "Epoch 004 | train 0.627023 | val 0.624220\n",
      "Epoch 005 | train 0.611346 | val 0.606442\n",
      "Epoch 006 | train 0.590734 | val 0.584293\n",
      "Epoch 007 | train 0.566892 | val 0.558976\n",
      "Epoch 008 | train 0.540184 | val 0.534596\n",
      "Epoch 009 | train 0.518674 | val 0.517156\n",
      "Epoch 010 | train 0.506928 | val 0.510262\n",
      "Epoch 011 | train 0.498021 | val 0.510095\n",
      "Epoch 012 | train 0.489918 | val 0.507949\n",
      "Epoch 013 | train 0.479653 | val 0.504469\n",
      "Epoch 014 | train 0.471373 | val 0.501960\n",
      "Epoch 015 | train 0.463004 | val 0.503813\n",
      "Epoch 016 | train 0.455794 | val 0.506980\n",
      "Epoch 017 | train 0.447606 | val 0.514464\n",
      "Epoch 018 | train 0.437956 | val 0.524415\n",
      "Epoch 019 | train 0.429094 | val 0.538501\n",
      "Epoch 020 | train 0.417618 | val 0.554193\n",
      "Epoch 021 | train 0.406892 | val 0.573836\n",
      "Epoch 022 | train 0.394014 | val 0.596951\n",
      "Epoch 023 | train 0.380319 | val 0.618403\n",
      "Epoch 024 | train 0.365530 | val 0.639565\n",
      "Epoch 025 | train 0.348421 | val 0.653722\n",
      "Epoch 026 | train 0.330630 | val 0.674640\n",
      "Epoch 027 | train 0.309483 | val 0.693340\n",
      "Epoch 028 | train 0.286562 | val 0.717135\n",
      "Epoch 029 | train 0.264821 | val 0.744173\n",
      "Epoch 030 | train 0.239128 | val 0.781006\n",
      "Epoch 031 | train 0.212740 | val 0.829108\n",
      "Epoch 032 | train 0.185141 | val 0.879474\n",
      "Epoch 033 | train 0.158231 | val 0.925981\n",
      "Epoch 034 | train 0.134986 | val 0.971247\n",
      "Early stopping at epoch 034 (best val 0.501960)\n",
      "\n",
      "Single-fit test metrics @0.5:\n",
      "        task_1_AUC: 0.8500\n",
      "      task_1_AUPRC: 0.9127\n",
      "      task_1_Brier: 0.1589\n",
      "        task_1_ACC: 0.7647\n",
      "         task_1_F1: 0.7949\n",
      "  task_1_Precision: 0.8611\n",
      "     task_1_Recall: 0.7381\n",
      "task_1_Sensitivity: 0.7381\n",
      "task_1_Specificity: 0.8077\n",
      "         macro_AUC: 0.8500\n",
      "       macro_AUPRC: 0.9127\n",
      "       macro_Brier: 0.1589\n",
      "         macro_ACC: 0.7647\n",
      "          macro_F1: 0.7949\n",
      "   macro_Precision: 0.8611\n",
      "      macro_Recall: 0.7381\n",
      " macro_Sensitivity: 0.7381\n",
      " macro_Specificity: 0.8077\n",
      "\n",
      "Single-fit test metrics @F1-opt per task:\n",
      " task_1_AUC_optthr: 0.8500\n",
      "task_1_AUPRC_optthr: 0.9127\n",
      "task_1_Brier_optthr: 0.1589\n",
      " task_1_ACC_optthr: 0.7843\n",
      "  task_1_F1_optthr: 0.8321\n",
      "task_1_Precision_optthr: 0.8015\n",
      "task_1_Recall_optthr: 0.8651\n",
      "task_1_Sensitivity_optthr: 0.8651\n",
      "task_1_Specificity_optthr: 0.6538\n",
      "  macro_AUC_optthr: 0.8500\n",
      "macro_AUPRC_optthr: 0.9127\n",
      "macro_Brier_optthr: 0.1589\n",
      "  macro_ACC_optthr: 0.7843\n",
      "   macro_F1_optthr: 0.8321\n",
      "macro_Precision_optthr: 0.8015\n",
      "macro_Recall_optthr: 0.8651\n",
      "macro_Sensitivity_optthr: 0.8651\n",
      "macro_Specificity_optthr: 0.6538\n",
      "\n",
      "Model: KANAdditiveMixedEffects | params=5076880\n",
      "Final arch params: {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0}\n",
      "Final train params: {'lr': 0.0003, 'weight_decay': 3e-06, 'batch_size': 256, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 0.001, 'lambda_ridge': 0.003, 'lambda_orth_latent': 0.003, 'lambda_film_identity': 0.003, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"single\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"time\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd54bc3",
   "metadata": {},
   "source": [
    "### Simple time split test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9edf5e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[split] train | rows=  378 | unique_ids=   27\n",
      "[split] val   | rows=   41 | unique_ids=   17\n",
      "[split] test  | rows=   90 | unique_ids=    6\n",
      "Epoch 001 | train 0.526697 | val 0.534687\n",
      "Epoch 002 | train 0.507956 | val 0.517704\n",
      "Epoch 003 | train 0.480146 | val 0.495666\n",
      "Epoch 004 | train 0.444803 | val 0.474551\n",
      "Epoch 005 | train 0.409969 | val 0.444050\n",
      "Epoch 006 | train 0.370282 | val 0.421624\n",
      "Epoch 007 | train 0.342564 | val 0.434675\n",
      "Epoch 008 | train 0.331219 | val 0.459849\n",
      "Epoch 009 | train 0.320072 | val 0.464080\n",
      "Epoch 010 | train 0.311151 | val 0.475698\n",
      "Epoch 011 | train 0.298172 | val 0.468206\n",
      "Epoch 012 | train 0.289716 | val 0.470808\n",
      "Epoch 013 | train 0.282897 | val 0.473719\n",
      "Epoch 014 | train 0.275560 | val 0.494934\n",
      "Epoch 015 | train 0.266722 | val 0.495954\n",
      "Epoch 016 | train 0.262501 | val 0.510397\n",
      "Epoch 017 | train 0.253086 | val 0.525794\n",
      "Epoch 018 | train 0.242124 | val 0.528608\n",
      "Epoch 019 | train 0.229399 | val 0.558364\n",
      "Epoch 020 | train 0.221350 | val 0.576381\n",
      "Epoch 021 | train 0.207503 | val 0.555646\n",
      "Epoch 022 | train 0.193228 | val 0.684533\n",
      "Epoch 023 | train 0.182454 | val 0.679074\n",
      "Epoch 024 | train 0.164409 | val 0.747811\n",
      "Epoch 025 | train 0.168723 | val 0.742047\n",
      "Epoch 026 | train 0.147951 | val 0.717102\n",
      "Early stopping at epoch 026 (best val 0.421624)\n",
      "\n",
      "Single-fit test metrics @0.5:\n",
      "        task_1_AUC: 0.7049\n",
      "      task_1_AUPRC: 0.7203\n",
      "      task_1_Brier: 0.2183\n",
      "        task_1_ACC: 0.6889\n",
      "         task_1_F1: 0.6744\n",
      "  task_1_Precision: 0.7632\n",
      "     task_1_Recall: 0.6042\n",
      "task_1_Sensitivity: 0.6042\n",
      "task_1_Specificity: 0.7857\n",
      "         macro_AUC: 0.7049\n",
      "       macro_AUPRC: 0.7203\n",
      "       macro_Brier: 0.2183\n",
      "         macro_ACC: 0.6889\n",
      "          macro_F1: 0.6744\n",
      "   macro_Precision: 0.7632\n",
      "      macro_Recall: 0.6042\n",
      " macro_Sensitivity: 0.6042\n",
      " macro_Specificity: 0.7857\n",
      "\n",
      "Single-fit test metrics @F1-opt per task:\n",
      " task_1_AUC_optthr: 0.7049\n",
      "task_1_AUPRC_optthr: 0.7203\n",
      "task_1_Brier_optthr: 0.2183\n",
      " task_1_ACC_optthr: 0.5333\n",
      "  task_1_F1_optthr: 0.6912\n",
      "task_1_Precision_optthr: 0.5341\n",
      "task_1_Recall_optthr: 0.9792\n",
      "task_1_Sensitivity_optthr: 0.9792\n",
      "task_1_Specificity_optthr: 0.0238\n",
      "  macro_AUC_optthr: 0.7049\n",
      "macro_AUPRC_optthr: 0.7203\n",
      "macro_Brier_optthr: 0.2183\n",
      "  macro_ACC_optthr: 0.5333\n",
      "   macro_F1_optthr: 0.6912\n",
      "macro_Precision_optthr: 0.5341\n",
      "macro_Recall_optthr: 0.9792\n",
      "macro_Sensitivity_optthr: 0.9792\n",
      "macro_Specificity_optthr: 0.0238\n",
      "\n",
      "Model: KANAdditiveMixedEffects | params=2288012\n",
      "Final arch params: {'d_fix_latent': 32, 'd_rand_latent': 32, 'n_kernels': 6, 'dropout': 0.0}\n",
      "Final train params: {'lr': 0.0005, 'weight_decay': 3e-05, 'batch_size': 64, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 1e-06, 'lambda_ridge': 1e-05, 'lambda_orth_latent': 3e-05, 'lambda_film_identity': 3e-05, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"single\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"cases\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54510d6a",
   "metadata": {},
   "source": [
    "### Cases split test CV without parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "11b8a291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV fold 1/3] scenario=cases\n",
      "[split] train | rows=  619 | unique_ids=  162\n",
      "[split] val   | rows=   68 | unique_ids=   51\n",
      "[split] test  | rows=  344 | unique_ids=   85\n",
      "Epoch 001 | train 0.626386 | val 0.631180\n",
      "Epoch 002 | train 0.618664 | val 0.626191\n",
      "Epoch 003 | train 0.609833 | val 0.619980\n",
      "Epoch 004 | train 0.598692 | val 0.613087\n",
      "Epoch 005 | train 0.584390 | val 0.604130\n",
      "Epoch 006 | train 0.565252 | val 0.593680\n",
      "Epoch 007 | train 0.543033 | val 0.582972\n",
      "Epoch 008 | train 0.516012 | val 0.574479\n",
      "Epoch 009 | train 0.490675 | val 0.571622\n",
      "Epoch 010 | train 0.468066 | val 0.571354\n",
      "Epoch 011 | train 0.451320 | val 0.577060\n",
      "Epoch 012 | train 0.438476 | val 0.582741\n",
      "Epoch 013 | train 0.429051 | val 0.587713\n",
      "Epoch 014 | train 0.419968 | val 0.590652\n",
      "Epoch 015 | train 0.412148 | val 0.592757\n",
      "Epoch 016 | train 0.403862 | val 0.595539\n",
      "Epoch 017 | train 0.395921 | val 0.599732\n",
      "Epoch 018 | train 0.388455 | val 0.602804\n",
      "Epoch 019 | train 0.380055 | val 0.607732\n",
      "Epoch 020 | train 0.370436 | val 0.615369\n",
      "Epoch 021 | train 0.359895 | val 0.622548\n",
      "Epoch 022 | train 0.350625 | val 0.627206\n",
      "Epoch 023 | train 0.339694 | val 0.636672\n",
      "Epoch 024 | train 0.329189 | val 0.644003\n",
      "Epoch 025 | train 0.319108 | val 0.627174\n",
      "Epoch 026 | train 0.308044 | val 0.618544\n",
      "Epoch 027 | train 0.296591 | val 0.622041\n",
      "Epoch 028 | train 0.284025 | val 0.630265\n",
      "Epoch 029 | train 0.271330 | val 0.634711\n",
      "Epoch 030 | train 0.257403 | val 0.630807\n",
      "Early stopping at epoch 030 (best val 0.571354)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7966, macro_AUPRC_optthr=0.8132, macro_Brier_optthr=0.1827, macro_ACC_optthr=0.6657, macro_F1_optthr=0.7368, macro_Precision_optthr=0.6364, macro_Recall_optthr=0.8750, macro_Sensitivity_optthr=0.8750, macro_Specificity_optthr=0.4250\n",
      "\n",
      "[CV fold 2/3] scenario=cases\n",
      "[split] train | rows=  619 | unique_ids=  169\n",
      "[split] val   | rows=   68 | unique_ids=   55\n",
      "[split] test  | rows=  344 | unique_ids=   86\n",
      "Epoch 001 | train 0.660857 | val 0.645835\n",
      "Epoch 002 | train 0.651860 | val 0.630871\n",
      "Epoch 003 | train 0.642154 | val 0.613207\n",
      "Epoch 004 | train 0.629538 | val 0.591743\n",
      "Epoch 005 | train 0.613475 | val 0.564961\n",
      "Epoch 006 | train 0.592906 | val 0.531286\n",
      "Epoch 007 | train 0.568658 | val 0.492928\n",
      "Epoch 008 | train 0.543594 | val 0.455310\n",
      "Epoch 009 | train 0.522298 | val 0.423343\n",
      "Epoch 010 | train 0.507463 | val 0.401743\n",
      "Epoch 011 | train 0.496953 | val 0.389232\n",
      "Epoch 012 | train 0.490109 | val 0.380080\n",
      "Epoch 013 | train 0.482716 | val 0.373467\n",
      "Epoch 014 | train 0.474933 | val 0.370124\n",
      "Epoch 015 | train 0.467713 | val 0.370000\n",
      "Epoch 016 | train 0.458655 | val 0.376109\n",
      "Epoch 017 | train 0.449775 | val 0.382554\n",
      "Epoch 018 | train 0.441603 | val 0.387444\n",
      "Epoch 019 | train 0.430897 | val 0.386763\n",
      "Epoch 020 | train 0.422209 | val 0.386939\n",
      "Epoch 021 | train 0.409102 | val 0.394077\n",
      "Epoch 022 | train 0.396864 | val 0.410044\n",
      "Epoch 023 | train 0.384572 | val 0.415266\n",
      "Epoch 024 | train 0.369792 | val 0.416957\n",
      "Epoch 025 | train 0.356364 | val 0.418756\n",
      "Epoch 026 | train 0.341720 | val 0.427604\n",
      "Epoch 027 | train 0.327231 | val 0.445949\n",
      "Epoch 028 | train 0.313240 | val 0.431434\n",
      "Epoch 029 | train 0.293698 | val 0.427276\n",
      "Epoch 030 | train 0.276846 | val 0.431350\n",
      "Epoch 031 | train 0.258019 | val 0.438973\n",
      "Epoch 032 | train 0.238006 | val 0.431821\n",
      "Epoch 033 | train 0.214922 | val 0.445688\n",
      "Epoch 034 | train 0.190645 | val 0.467798\n",
      "Epoch 035 | train 0.165884 | val 0.486423\n",
      "Early stopping at epoch 035 (best val 0.370000)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7916, macro_AUPRC_optthr=0.8024, macro_Brier_optthr=0.1850, macro_ACC_optthr=0.7616, macro_F1_optthr=0.7940, macro_Precision_optthr=0.7633, macro_Recall_optthr=0.8272, macro_Sensitivity_optthr=0.8272, macro_Specificity_optthr=0.6797\n",
      "\n",
      "[CV fold 3/3] scenario=cases\n",
      "[split] train | rows=  620 | unique_ids=  166\n",
      "[split] val   | rows=   68 | unique_ids=   54\n",
      "[split] test  | rows=  343 | unique_ids=   86\n",
      "Epoch 001 | train 0.621835 | val 0.626838\n",
      "Epoch 002 | train 0.613957 | val 0.620770\n",
      "Epoch 003 | train 0.605711 | val 0.612877\n",
      "Epoch 004 | train 0.595394 | val 0.601050\n",
      "Epoch 005 | train 0.581667 | val 0.585049\n",
      "Epoch 006 | train 0.564011 | val 0.565899\n",
      "Epoch 007 | train 0.541871 | val 0.542764\n",
      "Epoch 008 | train 0.517190 | val 0.520884\n",
      "Epoch 009 | train 0.490770 | val 0.500758\n",
      "Epoch 010 | train 0.465837 | val 0.485471\n",
      "Epoch 011 | train 0.450527 | val 0.478388\n",
      "Epoch 012 | train 0.441242 | val 0.478941\n",
      "Epoch 013 | train 0.432069 | val 0.485404\n",
      "Epoch 014 | train 0.421846 | val 0.501641\n",
      "Epoch 015 | train 0.411243 | val 0.516952\n",
      "Epoch 016 | train 0.402908 | val 0.526471\n",
      "Epoch 017 | train 0.392534 | val 0.518227\n",
      "Epoch 018 | train 0.382516 | val 0.520372\n",
      "Epoch 019 | train 0.373168 | val 0.538032\n",
      "Epoch 020 | train 0.362327 | val 0.557099\n",
      "Epoch 021 | train 0.351675 | val 0.577997\n",
      "Epoch 022 | train 0.339772 | val 0.593107\n",
      "Epoch 023 | train 0.327895 | val 0.597686\n",
      "Epoch 024 | train 0.315149 | val 0.612466\n",
      "Epoch 025 | train 0.303492 | val 0.634842\n",
      "Epoch 026 | train 0.288981 | val 0.659809\n",
      "Epoch 027 | train 0.274306 | val 0.665679\n",
      "Epoch 028 | train 0.260040 | val 0.654784\n",
      "Epoch 029 | train 0.244529 | val 0.670838\n",
      "Epoch 030 | train 0.227161 | val 0.710495\n",
      "Epoch 031 | train 0.214613 | val 0.756623\n",
      "Early stopping at epoch 031 (best val 0.478388)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7711, macro_AUPRC_optthr=0.7792, macro_Brier_optthr=0.1953, macro_ACC_optthr=0.7085, macro_F1_optthr=0.7549, macro_Precision_optthr=0.6784, macro_Recall_optthr=0.8508, macro_Sensitivity_optthr=0.8508, macro_Specificity_optthr=0.5494\n",
      "\n",
      "CV averages (95% CI) @0.5:\n",
      "           macro_ACC: 0.7313  (95% CI 0.6479, 0.8146)\n",
      "           macro_AUC: 0.7864  (95% CI 0.7529, 0.8199)\n",
      "         macro_AUPRC: 0.7983  (95% CI 0.7552, 0.8414)\n",
      "         macro_Brier: 0.1877  (95% CI 0.1710, 0.2043)\n",
      "            macro_F1: 0.7476  (95% CI 0.6738, 0.8214)\n",
      "     macro_Precision: 0.7587  (95% CI 0.6416, 0.8758)\n",
      "        macro_Recall: 0.7374  (95% CI 0.6969, 0.7779)\n",
      "   macro_Sensitivity: 0.7374  (95% CI 0.6969, 0.7779)\n",
      "   macro_Specificity: 0.7249  (95% CI 0.5857, 0.8640)\n",
      "          task_1_ACC: 0.7313  (95% CI 0.6479, 0.8146)\n",
      "          task_1_AUC: 0.7864  (95% CI 0.7529, 0.8199)\n",
      "        task_1_AUPRC: 0.7983  (95% CI 0.7552, 0.8414)\n",
      "        task_1_Brier: 0.1877  (95% CI 0.1710, 0.2043)\n",
      "           task_1_F1: 0.7476  (95% CI 0.6738, 0.8214)\n",
      "    task_1_Precision: 0.7587  (95% CI 0.6416, 0.8758)\n",
      "       task_1_Recall: 0.7374  (95% CI 0.6969, 0.7779)\n",
      "  task_1_Sensitivity: 0.7374  (95% CI 0.6969, 0.7779)\n",
      "  task_1_Specificity: 0.7249  (95% CI 0.5857, 0.8640)\n",
      "\n",
      "CV averages (95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.7119  (95% CI 0.5925, 0.8313)\n",
      "    macro_AUC_optthr: 0.7864  (95% CI 0.7529, 0.8199)\n",
      "  macro_AUPRC_optthr: 0.7983  (95% CI 0.7552, 0.8414)\n",
      "  macro_Brier_optthr: 0.1877  (95% CI 0.1710, 0.2043)\n",
      "     macro_F1_optthr: 0.7619  (95% CI 0.6894, 0.8344)\n",
      "macro_Precision_optthr: 0.6927  (95% CI 0.5321, 0.8533)\n",
      " macro_Recall_optthr: 0.8510  (95% CI 0.7917, 0.9104)\n",
      "macro_Sensitivity_optthr: 0.8510  (95% CI 0.7917, 0.9104)\n",
      "macro_Specificity_optthr: 0.5514  (95% CI 0.2349, 0.8678)\n",
      "   task_1_ACC_optthr: 0.7119  (95% CI 0.5925, 0.8313)\n",
      "   task_1_AUC_optthr: 0.7864  (95% CI 0.7529, 0.8199)\n",
      " task_1_AUPRC_optthr: 0.7983  (95% CI 0.7552, 0.8414)\n",
      " task_1_Brier_optthr: 0.1877  (95% CI 0.1710, 0.2043)\n",
      "    task_1_F1_optthr: 0.7619  (95% CI 0.6894, 0.8344)\n",
      "task_1_Precision_optthr: 0.6927  (95% CI 0.5321, 0.8533)\n",
      "task_1_Recall_optthr: 0.8510  (95% CI 0.7917, 0.9104)\n",
      "task_1_Sensitivity_optthr: 0.8510  (95% CI 0.7917, 0.9104)\n",
      "task_1_Specificity_optthr: 0.5514  (95% CI 0.2349, 0.8678)\n",
      "Arch params (used in all folds): {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0}\n",
      "Train params (used in all folds): {'lr': 0.0003, 'weight_decay': 3e-06, 'batch_size': 256, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 0.001, 'lambda_ridge': 0.003, 'lambda_orth_latent': 0.003, 'lambda_film_identity': 0.003, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"cv_only\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"cases\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5c06e",
   "metadata": {},
   "source": [
    "### Time split test CV without parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8c1e00e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV fold 1/5] scenario=time\n",
      "[split] train | rows=  159 | unique_ids=  159\n",
      "[split] val   | rows=   17 | unique_ids=   17\n",
      "[split] test  | rows=  171 | unique_ids=  142\n",
      "Epoch 001 | train 0.664827 | val 0.666266\n",
      "Epoch 002 | train 0.662167 | val 0.664337\n",
      "Epoch 003 | train 0.659476 | val 0.662439\n",
      "Epoch 004 | train 0.656692 | val 0.660507\n",
      "Epoch 005 | train 0.653762 | val 0.658480\n",
      "Epoch 006 | train 0.650630 | val 0.656320\n",
      "Epoch 007 | train 0.647244 | val 0.654000\n",
      "Epoch 008 | train 0.643552 | val 0.651501\n",
      "Epoch 009 | train 0.639502 | val 0.648804\n",
      "Epoch 010 | train 0.635043 | val 0.645892\n",
      "Epoch 011 | train 0.630124 | val 0.642749\n",
      "Epoch 012 | train 0.624696 | val 0.639365\n",
      "Epoch 013 | train 0.618707 | val 0.635734\n",
      "Epoch 014 | train 0.612110 | val 0.631858\n",
      "Epoch 015 | train 0.604863 | val 0.627749\n",
      "Epoch 016 | train 0.596929 | val 0.623428\n",
      "Epoch 017 | train 0.588287 | val 0.618934\n",
      "Epoch 018 | train 0.578925 | val 0.614318\n",
      "Epoch 019 | train 0.568854 | val 0.609648\n",
      "Epoch 020 | train 0.558104 | val 0.605014\n",
      "Epoch 021 | train 0.546728 | val 0.600524\n",
      "Epoch 022 | train 0.534801 | val 0.596311\n",
      "Epoch 023 | train 0.522413 | val 0.592537\n",
      "Epoch 024 | train 0.509666 | val 0.589391\n",
      "Epoch 025 | train 0.496655 | val 0.587088\n",
      "Epoch 026 | train 0.483458 | val 0.585846\n",
      "Epoch 027 | train 0.470122 | val 0.585867\n",
      "Epoch 028 | train 0.456655 | val 0.587313\n",
      "Epoch 029 | train 0.443028 | val 0.590296\n",
      "Epoch 030 | train 0.429183 | val 0.594865\n",
      "Epoch 031 | train 0.415033 | val 0.601011\n",
      "Epoch 032 | train 0.400480 | val 0.608662\n",
      "Epoch 033 | train 0.385422 | val 0.617715\n",
      "Epoch 034 | train 0.369774 | val 0.628066\n",
      "Epoch 035 | train 0.353459 | val 0.639632\n",
      "Epoch 036 | train 0.336431 | val 0.652373\n",
      "Epoch 037 | train 0.318714 | val 0.666297\n",
      "Epoch 038 | train 0.300453 | val 0.681457\n",
      "Epoch 039 | train 0.281969 | val 0.697955\n",
      "Epoch 040 | train 0.263662 | val 0.715994\n",
      "Epoch 041 | train 0.245872 | val 0.735925\n",
      "Epoch 042 | train 0.228862 | val 0.758175\n",
      "Epoch 043 | train 0.212816 | val 0.782967\n",
      "Epoch 044 | train 0.197922 | val 0.810032\n",
      "Epoch 045 | train 0.184386 | val 0.838674\n",
      "Epoch 046 | train 0.172029 | val 0.868221\n",
      "Early stopping at epoch 046 (best val 0.585846)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7556, macro_AUPRC_optthr=0.7587, macro_Brier_optthr=0.1990, macro_ACC_optthr=0.6959, macro_F1_optthr=0.7636, macro_Precision_optthr=0.6774, macro_Recall_optthr=0.8750, macro_Sensitivity_optthr=0.8750, macro_Specificity_optthr=0.4667\n",
      "\n",
      "[CV fold 2/5] scenario=time\n",
      "[split] train | rows=  313 | unique_ids=  242\n",
      "[split] val   | rows=   34 | unique_ids=   33\n",
      "[split] test  | rows=  171 | unique_ids=  132\n",
      "Epoch 001 | train 0.650104 | val 0.642007\n",
      "Epoch 002 | train 0.645776 | val 0.638507\n",
      "Epoch 003 | train 0.641297 | val 0.635183\n",
      "Epoch 004 | train 0.636480 | val 0.631212\n",
      "Epoch 005 | train 0.630595 | val 0.627078\n",
      "Epoch 006 | train 0.623884 | val 0.622530\n",
      "Epoch 007 | train 0.615569 | val 0.617291\n",
      "Epoch 008 | train 0.606631 | val 0.612010\n",
      "Epoch 009 | train 0.594937 | val 0.607070\n",
      "Epoch 010 | train 0.582976 | val 0.602351\n",
      "Epoch 011 | train 0.569122 | val 0.599420\n",
      "Epoch 012 | train 0.553406 | val 0.598619\n",
      "Epoch 013 | train 0.538726 | val 0.601099\n",
      "Epoch 014 | train 0.524875 | val 0.609291\n",
      "Epoch 015 | train 0.511232 | val 0.624661\n",
      "Epoch 016 | train 0.500199 | val 0.646601\n",
      "Epoch 017 | train 0.491602 | val 0.672777\n",
      "Epoch 018 | train 0.485938 | val 0.698055\n",
      "Epoch 019 | train 0.479197 | val 0.713994\n",
      "Epoch 020 | train 0.469964 | val 0.723410\n",
      "Epoch 021 | train 0.458004 | val 0.730964\n",
      "Epoch 022 | train 0.446007 | val 0.737334\n",
      "Epoch 023 | train 0.433911 | val 0.741254\n",
      "Epoch 024 | train 0.422816 | val 0.743267\n",
      "Epoch 025 | train 0.410350 | val 0.749909\n",
      "Epoch 026 | train 0.398879 | val 0.758096\n",
      "Epoch 027 | train 0.386222 | val 0.771464\n",
      "Epoch 028 | train 0.371813 | val 0.793454\n",
      "Epoch 029 | train 0.356740 | val 0.829940\n",
      "Epoch 030 | train 0.341167 | val 0.873566\n",
      "Epoch 031 | train 0.325653 | val 0.912118\n",
      "Epoch 032 | train 0.308632 | val 0.942045\n",
      "Early stopping at epoch 032 (best val 0.598619)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7960, macro_AUPRC_optthr=0.7774, macro_Brier_optthr=0.1959, macro_ACC_optthr=0.6725, macro_F1_optthr=0.7255, macro_Precision_optthr=0.6116, macro_Recall_optthr=0.8916, macro_Sensitivity_optthr=0.8916, macro_Specificity_optthr=0.4659\n",
      "\n",
      "[CV fold 3/5] scenario=time\n",
      "[split] train | rows=  467 | unique_ids=  252\n",
      "[split] val   | rows=   51 | unique_ids=   47\n",
      "[split] test  | rows=  171 | unique_ids=  119\n",
      "Epoch 001 | train 0.677925 | val 0.666740\n",
      "Epoch 002 | train 0.672592 | val 0.660073\n",
      "Epoch 003 | train 0.667001 | val 0.653354\n",
      "Epoch 004 | train 0.660547 | val 0.645513\n",
      "Epoch 005 | train 0.653324 | val 0.636261\n",
      "Epoch 006 | train 0.644583 | val 0.625275\n",
      "Epoch 007 | train 0.634003 | val 0.612440\n",
      "Epoch 008 | train 0.621759 | val 0.597794\n",
      "Epoch 009 | train 0.608549 | val 0.581784\n",
      "Epoch 010 | train 0.593649 | val 0.565418\n",
      "Epoch 011 | train 0.577343 | val 0.551282\n",
      "Epoch 012 | train 0.559500 | val 0.541206\n",
      "Epoch 013 | train 0.545896 | val 0.537155\n",
      "Epoch 014 | train 0.533931 | val 0.540336\n",
      "Epoch 015 | train 0.522887 | val 0.545466\n",
      "Epoch 016 | train 0.517351 | val 0.550690\n",
      "Epoch 017 | train 0.510022 | val 0.552487\n",
      "Epoch 018 | train 0.501133 | val 0.550154\n",
      "Epoch 019 | train 0.491523 | val 0.544971\n",
      "Epoch 020 | train 0.481928 | val 0.539005\n",
      "Epoch 021 | train 0.470540 | val 0.538005\n",
      "Epoch 022 | train 0.459756 | val 0.542258\n",
      "Epoch 023 | train 0.447335 | val 0.546981\n",
      "Epoch 024 | train 0.435168 | val 0.553533\n",
      "Epoch 025 | train 0.422468 | val 0.559493\n",
      "Epoch 026 | train 0.408294 | val 0.562854\n",
      "Epoch 027 | train 0.393542 | val 0.565369\n",
      "Epoch 028 | train 0.378959 | val 0.569385\n",
      "Epoch 029 | train 0.362822 | val 0.574490\n",
      "Epoch 030 | train 0.347010 | val 0.583258\n",
      "Epoch 031 | train 0.329794 | val 0.597988\n",
      "Epoch 032 | train 0.313301 | val 0.601090\n",
      "Epoch 033 | train 0.294545 | val 0.599924\n",
      "Early stopping at epoch 033 (best val 0.537155)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8189, macro_AUPRC_optthr=0.7981, macro_Brier_optthr=0.1805, macro_ACC_optthr=0.7427, macro_F1_optthr=0.7708, macro_Precision_optthr=0.6981, macro_Recall_optthr=0.8605, macro_Sensitivity_optthr=0.8605, macro_Specificity_optthr=0.6235\n",
      "\n",
      "[CV fold 4/5] scenario=time\n",
      "[split] train | rows=  621 | unique_ids=  252\n",
      "[split] val   | rows=   68 | unique_ids=   61\n",
      "[split] test  | rows=  171 | unique_ids=  110\n",
      "Epoch 001 | train 0.677680 | val 0.671697\n",
      "Epoch 002 | train 0.669484 | val 0.664093\n",
      "Epoch 003 | train 0.661017 | val 0.654669\n",
      "Epoch 004 | train 0.649445 | val 0.643531\n",
      "Epoch 005 | train 0.634791 | val 0.629664\n",
      "Epoch 006 | train 0.616568 | val 0.613905\n",
      "Epoch 007 | train 0.595855 | val 0.598317\n",
      "Epoch 008 | train 0.570195 | val 0.587180\n",
      "Epoch 009 | train 0.546792 | val 0.586374\n",
      "Epoch 010 | train 0.528197 | val 0.600639\n",
      "Epoch 011 | train 0.517677 | val 0.620348\n",
      "Epoch 012 | train 0.509992 | val 0.628653\n",
      "Epoch 013 | train 0.500902 | val 0.624242\n",
      "Epoch 014 | train 0.491515 | val 0.618905\n",
      "Epoch 015 | train 0.482691 | val 0.615227\n",
      "Epoch 016 | train 0.472914 | val 0.626804\n",
      "Epoch 017 | train 0.462816 | val 0.638703\n",
      "Epoch 018 | train 0.453460 | val 0.649019\n",
      "Epoch 019 | train 0.442013 | val 0.648196\n",
      "Epoch 020 | train 0.431475 | val 0.655282\n",
      "Epoch 021 | train 0.419301 | val 0.675934\n",
      "Epoch 022 | train 0.406759 | val 0.700098\n",
      "Epoch 023 | train 0.392767 | val 0.710105\n",
      "Epoch 024 | train 0.378100 | val 0.723406\n",
      "Epoch 025 | train 0.361862 | val 0.743920\n",
      "Epoch 026 | train 0.343578 | val 0.764786\n",
      "Epoch 027 | train 0.325701 | val 0.800057\n",
      "Epoch 028 | train 0.304392 | val 0.802260\n",
      "Epoch 029 | train 0.283640 | val 0.799805\n",
      "Early stopping at epoch 029 (best val 0.586374)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8359, macro_AUPRC_optthr=0.8873, macro_Brier_optthr=0.1673, macro_ACC_optthr=0.7953, macro_F1_optthr=0.8148, macro_Precision_optthr=0.8462, macro_Recall_optthr=0.7857, macro_Sensitivity_optthr=0.7857, macro_Specificity_optthr=0.8082\n",
      "\n",
      "[CV fold 5/5] scenario=time\n",
      "[split] train | rows=  774 | unique_ids=  250\n",
      "[split] val   | rows=   86 | unique_ids=   71\n",
      "[split] test  | rows=  171 | unique_ids=   82\n",
      "Epoch 001 | train 0.646570 | val 0.647350\n",
      "Epoch 002 | train 0.635784 | val 0.637768\n",
      "Epoch 003 | train 0.623871 | val 0.627214\n",
      "Epoch 004 | train 0.609559 | val 0.613169\n",
      "Epoch 005 | train 0.591645 | val 0.595352\n",
      "Epoch 006 | train 0.569668 | val 0.576412\n",
      "Epoch 007 | train 0.547294 | val 0.559081\n",
      "Epoch 008 | train 0.525481 | val 0.545606\n",
      "Epoch 009 | train 0.508824 | val 0.537711\n",
      "Epoch 010 | train 0.492907 | val 0.540124\n",
      "Epoch 011 | train 0.489005 | val 0.554912\n",
      "Epoch 012 | train 0.488454 | val 0.568589\n",
      "Epoch 013 | train 0.493595 | val 0.572674\n",
      "Epoch 014 | train 0.484173 | val 0.557612\n",
      "Epoch 015 | train 0.475826 | val 0.557598\n",
      "Epoch 016 | train 0.476184 | val 0.561474\n",
      "Epoch 017 | train 0.477228 | val 0.562792\n",
      "Epoch 018 | train 0.471887 | val 0.560517\n",
      "Epoch 019 | train 0.464721 | val 0.562622\n",
      "Epoch 020 | train 0.460041 | val 0.568492\n",
      "Epoch 021 | train 0.456807 | val 0.576999\n",
      "Epoch 022 | train 0.453648 | val 0.587408\n",
      "Epoch 023 | train 0.451995 | val 0.596449\n",
      "Epoch 024 | train 0.449522 | val 0.596658\n",
      "Epoch 025 | train 0.446186 | val 0.591810\n",
      "Epoch 026 | train 0.443060 | val 0.588464\n",
      "Epoch 027 | train 0.439522 | val 0.585494\n",
      "Epoch 028 | train 0.436100 | val 0.587660\n",
      "Epoch 029 | train 0.431853 | val 0.592669\n",
      "Early stopping at epoch 029 (best val 0.537711)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8609, macro_AUPRC_optthr=0.9082, macro_Brier_optthr=0.1568, macro_ACC_optthr=0.7602, macro_F1_optthr=0.8178, macro_Precision_optthr=0.7480, macro_Recall_optthr=0.9020, macro_Sensitivity_optthr=0.9020, macro_Specificity_optthr=0.5507\n",
      "\n",
      "CV averages (95% CI) @0.5:\n",
      "           macro_ACC: 0.7661  (95% CI 0.7247, 0.8075)\n",
      "           macro_AUC: 0.8134  (95% CI 0.7636, 0.8633)\n",
      "         macro_AUPRC: 0.8259  (95% CI 0.7422, 0.9097)\n",
      "         macro_Brier: 0.1799  (95% CI 0.1574, 0.2024)\n",
      "            macro_F1: 0.7871  (95% CI 0.7441, 0.8301)\n",
      "     macro_Precision: 0.7765  (95% CI 0.7041, 0.8490)\n",
      "        macro_Recall: 0.7997  (95% CI 0.7688, 0.8306)\n",
      "   macro_Sensitivity: 0.7997  (95% CI 0.7688, 0.8306)\n",
      "   macro_Specificity: 0.7272  (95% CI 0.6502, 0.8042)\n",
      "          task_1_ACC: 0.7661  (95% CI 0.7247, 0.8075)\n",
      "          task_1_AUC: 0.8134  (95% CI 0.7636, 0.8633)\n",
      "        task_1_AUPRC: 0.8259  (95% CI 0.7422, 0.9097)\n",
      "        task_1_Brier: 0.1799  (95% CI 0.1574, 0.2024)\n",
      "           task_1_F1: 0.7871  (95% CI 0.7441, 0.8301)\n",
      "    task_1_Precision: 0.7765  (95% CI 0.7041, 0.8490)\n",
      "       task_1_Recall: 0.7997  (95% CI 0.7688, 0.8306)\n",
      "  task_1_Sensitivity: 0.7997  (95% CI 0.7688, 0.8306)\n",
      "  task_1_Specificity: 0.7272  (95% CI 0.6502, 0.8042)\n",
      "\n",
      "CV averages (95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.7333  (95% CI 0.6720, 0.7946)\n",
      "    macro_AUC_optthr: 0.8134  (95% CI 0.7636, 0.8633)\n",
      "  macro_AUPRC_optthr: 0.8259  (95% CI 0.7422, 0.9097)\n",
      "  macro_Brier_optthr: 0.1799  (95% CI 0.1574, 0.2024)\n",
      "     macro_F1_optthr: 0.7785  (95% CI 0.7306, 0.8264)\n",
      "macro_Precision_optthr: 0.7162  (95% CI 0.6075, 0.8250)\n",
      " macro_Recall_optthr: 0.8629  (95% CI 0.8058, 0.9200)\n",
      "macro_Sensitivity_optthr: 0.8629  (95% CI 0.8058, 0.9200)\n",
      "macro_Specificity_optthr: 0.5830  (95% CI 0.4067, 0.7593)\n",
      "   task_1_ACC_optthr: 0.7333  (95% CI 0.6720, 0.7946)\n",
      "   task_1_AUC_optthr: 0.8134  (95% CI 0.7636, 0.8633)\n",
      " task_1_AUPRC_optthr: 0.8259  (95% CI 0.7422, 0.9097)\n",
      " task_1_Brier_optthr: 0.1799  (95% CI 0.1574, 0.2024)\n",
      "    task_1_F1_optthr: 0.7785  (95% CI 0.7306, 0.8264)\n",
      "task_1_Precision_optthr: 0.7162  (95% CI 0.6075, 0.8250)\n",
      "task_1_Recall_optthr: 0.8629  (95% CI 0.8058, 0.9200)\n",
      "task_1_Sensitivity_optthr: 0.8629  (95% CI 0.8058, 0.9200)\n",
      "task_1_Specificity_optthr: 0.5830  (95% CI 0.4067, 0.7593)\n",
      "Arch params (used in all folds): {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0}\n",
      "Train params (used in all folds): {'lr': 0.0003, 'weight_decay': 3e-06, 'batch_size': 256, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 0.001, 'lambda_ridge': 0.003, 'lambda_orth_latent': 0.003, 'lambda_film_identity': 0.003, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"cv_only\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"time\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25babf89",
   "metadata": {},
   "source": [
    "### Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "051f906d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV fold 1/5] scenario=cases\n",
      "[split] train | rows=  742 | unique_ids=  201\n",
      "[split] val   | rows=   82 | unique_ids=   65\n",
      "[split] test  | rows=  207 | unique_ids=   51\n",
      "Epoch 001 | train 0.662666 | val 0.657978\n",
      "Epoch 002 | train 0.660247 | val 0.655375\n",
      "Epoch 003 | train 0.657050 | val 0.651790\n",
      "Epoch 004 | train 0.652186 | val 0.646179\n",
      "Epoch 005 | train 0.644552 | val 0.637915\n",
      "Epoch 006 | train 0.634448 | val 0.626129\n",
      "Epoch 007 | train 0.619174 | val 0.611002\n",
      "Epoch 008 | train 0.598446 | val 0.592786\n",
      "Epoch 009 | train 0.575270 | val 0.573702\n",
      "Epoch 010 | train 0.549078 | val 0.557326\n",
      "Epoch 011 | train 0.523538 | val 0.547020\n",
      "Epoch 012 | train 0.502395 | val 0.547514\n",
      "Epoch 013 | train 0.486301 | val 0.553952\n",
      "Epoch 014 | train 0.472944 | val 0.562537\n",
      "Epoch 015 | train 0.461520 | val 0.567270\n",
      "Epoch 016 | train 0.448859 | val 0.566741\n",
      "Epoch 017 | train 0.436466 | val 0.563500\n",
      "Epoch 018 | train 0.423948 | val 0.566173\n",
      "Epoch 019 | train 0.413287 | val 0.566165\n",
      "Epoch 020 | train 0.402715 | val 0.564648\n",
      "Epoch 021 | train 0.392523 | val 0.572284\n",
      "Epoch 022 | train 0.382623 | val 0.579868\n",
      "Epoch 023 | train 0.372968 | val 0.584773\n",
      "Epoch 024 | train 0.362388 | val 0.588883\n",
      "Epoch 025 | train 0.351706 | val 0.598656\n",
      "Epoch 026 | train 0.340408 | val 0.611052\n",
      "Epoch 027 | train 0.328888 | val 0.630906\n",
      "Epoch 028 | train 0.316234 | val 0.643488\n",
      "Epoch 029 | train 0.303152 | val 0.654654\n",
      "Epoch 030 | train 0.289429 | val 0.673298\n",
      "Epoch 031 | train 0.275942 | val 0.702073\n",
      "Early stopping at epoch 031 (best val 0.547020)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7058, macro_AUPRC_optthr=0.7412, macro_Brier_optthr=0.2178, macro_ACC_optthr=0.6377, macro_F1_optthr=0.6753, macro_Precision_optthr=0.7091, macro_Recall_optthr=0.6446, macro_Sensitivity_optthr=0.6446, macro_Specificity_optthr=0.6279\n",
      "\n",
      "[CV fold 2/5] scenario=cases\n",
      "[split] train | rows=  743 | unique_ids=  198\n",
      "[split] val   | rows=   82 | unique_ids=   63\n",
      "[split] test  | rows=  206 | unique_ids=   51\n",
      "Epoch 001 | train 0.652404 | val 0.642151\n",
      "Epoch 002 | train 0.649885 | val 0.637594\n",
      "Epoch 003 | train 0.646443 | val 0.631975\n",
      "Epoch 004 | train 0.641325 | val 0.623264\n",
      "Epoch 005 | train 0.633422 | val 0.610388\n",
      "Epoch 006 | train 0.622047 | val 0.592617\n",
      "Epoch 007 | train 0.606690 | val 0.570729\n",
      "Epoch 008 | train 0.587089 | val 0.546340\n",
      "Epoch 009 | train 0.563999 | val 0.523318\n",
      "Epoch 010 | train 0.539331 | val 0.505854\n",
      "Epoch 011 | train 0.516051 | val 0.498847\n",
      "Epoch 012 | train 0.497462 | val 0.502068\n",
      "Epoch 013 | train 0.484153 | val 0.511867\n",
      "Epoch 014 | train 0.475875 | val 0.517773\n",
      "Epoch 015 | train 0.467560 | val 0.516283\n",
      "Epoch 016 | train 0.456529 | val 0.507935\n",
      "Epoch 017 | train 0.444365 | val 0.497672\n",
      "Epoch 018 | train 0.433685 | val 0.491885\n",
      "Epoch 019 | train 0.424432 | val 0.494590\n",
      "Epoch 020 | train 0.414414 | val 0.499300\n",
      "Epoch 021 | train 0.405480 | val 0.499785\n",
      "Epoch 022 | train 0.396207 | val 0.505571\n",
      "Epoch 023 | train 0.386859 | val 0.510146\n",
      "Epoch 024 | train 0.376809 | val 0.508879\n",
      "Epoch 025 | train 0.367320 | val 0.515718\n",
      "Epoch 026 | train 0.356902 | val 0.530581\n",
      "Epoch 027 | train 0.345144 | val 0.537838\n",
      "Epoch 028 | train 0.332981 | val 0.546188\n",
      "Epoch 029 | train 0.320567 | val 0.556765\n",
      "Epoch 030 | train 0.306678 | val 0.563678\n",
      "Epoch 031 | train 0.292653 | val 0.573543\n",
      "Epoch 032 | train 0.276418 | val 0.582311\n",
      "Epoch 033 | train 0.260547 | val 0.608962\n",
      "Epoch 034 | train 0.242572 | val 0.625357\n",
      "Epoch 035 | train 0.225479 | val 0.631602\n",
      "Epoch 036 | train 0.206091 | val 0.681878\n",
      "Epoch 037 | train 0.186401 | val 0.702760\n",
      "Epoch 038 | train 0.167789 | val 0.744061\n",
      "Early stopping at epoch 038 (best val 0.491885)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.6502, macro_AUPRC_optthr=0.7091, macro_Brier_optthr=0.2546, macro_ACC_optthr=0.5680, macro_F1_optthr=0.6009, macro_Precision_optthr=0.5982, macro_Recall_optthr=0.6036, macro_Sensitivity_optthr=0.6036, macro_Specificity_optthr=0.5263\n",
      "\n",
      "[CV fold 3/5] scenario=cases\n",
      "[split] train | rows=  743 | unique_ids=  202\n",
      "[split] val   | rows=   82 | unique_ids=   62\n",
      "[split] test  | rows=  206 | unique_ids=   51\n",
      "Epoch 001 | train 0.630105 | val 0.625343\n",
      "Epoch 002 | train 0.627933 | val 0.623448\n",
      "Epoch 003 | train 0.625254 | val 0.620568\n",
      "Epoch 004 | train 0.620552 | val 0.616437\n",
      "Epoch 005 | train 0.613972 | val 0.610152\n",
      "Epoch 006 | train 0.604533 | val 0.601299\n",
      "Epoch 007 | train 0.591027 | val 0.589698\n",
      "Epoch 008 | train 0.573501 | val 0.576015\n",
      "Epoch 009 | train 0.554398 | val 0.561881\n",
      "Epoch 010 | train 0.531612 | val 0.550099\n",
      "Epoch 011 | train 0.509744 | val 0.542602\n",
      "Epoch 012 | train 0.489868 | val 0.540831\n",
      "Epoch 013 | train 0.475105 | val 0.541826\n",
      "Epoch 014 | train 0.462356 | val 0.540523\n",
      "Epoch 015 | train 0.449600 | val 0.537151\n",
      "Epoch 016 | train 0.436370 | val 0.529970\n",
      "Epoch 017 | train 0.423751 | val 0.525513\n",
      "Epoch 018 | train 0.412369 | val 0.522594\n",
      "Epoch 019 | train 0.401332 | val 0.517851\n",
      "Epoch 020 | train 0.391460 | val 0.513729\n",
      "Epoch 021 | train 0.381211 | val 0.509051\n",
      "Epoch 022 | train 0.371289 | val 0.508077\n",
      "Epoch 023 | train 0.361276 | val 0.502931\n",
      "Epoch 024 | train 0.350095 | val 0.500732\n",
      "Epoch 025 | train 0.339188 | val 0.495996\n",
      "Epoch 026 | train 0.327847 | val 0.493703\n",
      "Epoch 027 | train 0.314591 | val 0.492695\n",
      "Epoch 028 | train 0.302989 | val 0.491032\n",
      "Epoch 029 | train 0.288724 | val 0.488003\n",
      "Epoch 030 | train 0.273728 | val 0.488054\n",
      "Epoch 031 | train 0.259262 | val 0.487951\n",
      "Epoch 032 | train 0.241336 | val 0.487988\n",
      "Epoch 033 | train 0.223889 | val 0.487561\n",
      "Epoch 034 | train 0.203433 | val 0.492148\n",
      "Epoch 035 | train 0.184376 | val 0.499857\n",
      "Epoch 036 | train 0.163180 | val 0.509679\n",
      "Epoch 037 | train 0.141670 | val 0.527420\n",
      "Epoch 038 | train 0.120224 | val 0.552638\n",
      "Epoch 039 | train 0.096817 | val 0.575409\n",
      "Epoch 040 | train 0.076493 | val 0.603541\n",
      "Epoch 041 | train 0.059415 | val 0.630840\n",
      "Epoch 042 | train 0.046047 | val 0.661362\n",
      "Epoch 043 | train 0.034514 | val 0.690223\n",
      "Epoch 044 | train 0.027432 | val 0.716572\n",
      "Epoch 045 | train 0.023228 | val 0.746040\n",
      "Epoch 046 | train 0.018942 | val 0.776050\n",
      "Epoch 047 | train 0.014982 | val 0.804659\n",
      "Epoch 048 | train 0.012804 | val 0.832185\n",
      "Epoch 049 | train 0.010698 | val 0.854321\n",
      "Epoch 050 | train 0.008035 | val 0.871866\n",
      "Epoch 051 | train 0.006592 | val 0.877728\n",
      "Epoch 052 | train 0.003417 | val 0.888043\n",
      "Epoch 053 | train 0.002992 | val 0.901582\n",
      "Early stopping at epoch 053 (best val 0.487561)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.6976, macro_AUPRC_optthr=0.7368, macro_Brier_optthr=0.2803, macro_ACC_optthr=0.6262, macro_F1_optthr=0.6723, macro_Precision_optthr=0.6031, macro_Recall_optthr=0.7596, macro_Sensitivity_optthr=0.7596, macro_Specificity_optthr=0.4902\n",
      "\n",
      "[CV fold 4/5] scenario=cases\n",
      "[split] train | rows=  743 | unique_ids=  205\n",
      "[split] val   | rows=   82 | unique_ids=   66\n",
      "[split] test  | rows=  206 | unique_ids=   52\n",
      "Epoch 001 | train 0.661628 | val 0.656051\n",
      "Epoch 002 | train 0.659235 | val 0.653412\n",
      "Epoch 003 | train 0.656275 | val 0.649667\n",
      "Epoch 004 | train 0.651664 | val 0.644468\n",
      "Epoch 005 | train 0.645242 | val 0.637246\n",
      "Epoch 006 | train 0.635818 | val 0.627669\n",
      "Epoch 007 | train 0.622231 | val 0.616400\n",
      "Epoch 008 | train 0.605665 | val 0.603854\n",
      "Epoch 009 | train 0.586387 | val 0.592020\n",
      "Epoch 010 | train 0.564844 | val 0.583817\n",
      "Epoch 011 | train 0.540781 | val 0.581872\n",
      "Epoch 012 | train 0.521616 | val 0.588721\n",
      "Epoch 013 | train 0.504949 | val 0.601677\n",
      "Epoch 014 | train 0.491540 | val 0.610784\n",
      "Epoch 015 | train 0.479869 | val 0.612574\n",
      "Epoch 016 | train 0.466021 | val 0.606370\n",
      "Epoch 017 | train 0.452939 | val 0.599033\n",
      "Epoch 018 | train 0.439034 | val 0.596904\n",
      "Epoch 019 | train 0.426619 | val 0.600442\n",
      "Epoch 020 | train 0.414256 | val 0.604271\n",
      "Epoch 021 | train 0.405202 | val 0.597875\n",
      "Epoch 022 | train 0.393507 | val 0.598909\n",
      "Epoch 023 | train 0.383440 | val 0.605178\n",
      "Epoch 024 | train 0.372299 | val 0.611305\n",
      "Epoch 025 | train 0.361509 | val 0.613794\n",
      "Epoch 026 | train 0.351052 | val 0.611397\n",
      "Epoch 027 | train 0.338755 | val 0.631712\n",
      "Epoch 028 | train 0.326538 | val 0.633868\n",
      "Epoch 029 | train 0.313847 | val 0.635097\n",
      "Epoch 030 | train 0.302300 | val 0.625073\n",
      "Epoch 031 | train 0.288437 | val 0.650419\n",
      "Early stopping at epoch 031 (best val 0.581872)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8249, macro_AUPRC_optthr=0.8695, macro_Brier_optthr=0.1781, macro_ACC_optthr=0.6650, macro_F1_optthr=0.7612, macro_Precision_optthr=0.6433, macro_Recall_optthr=0.9322, macro_Sensitivity_optthr=0.9322, macro_Specificity_optthr=0.3068\n",
      "\n",
      "[CV fold 5/5] scenario=cases\n",
      "[split] train | rows=  743 | unique_ids=  201\n",
      "[split] val   | rows=   82 | unique_ids=   66\n",
      "[split] test  | rows=  206 | unique_ids=   52\n",
      "Epoch 001 | train 0.622797 | val 0.621966\n",
      "Epoch 002 | train 0.620647 | val 0.620052\n",
      "Epoch 003 | train 0.617810 | val 0.617265\n",
      "Epoch 004 | train 0.613572 | val 0.613080\n",
      "Epoch 005 | train 0.607029 | val 0.607007\n",
      "Epoch 006 | train 0.597412 | val 0.598723\n",
      "Epoch 007 | train 0.583976 | val 0.588294\n",
      "Epoch 008 | train 0.566306 | val 0.576868\n",
      "Epoch 009 | train 0.545697 | val 0.565714\n",
      "Epoch 010 | train 0.521852 | val 0.557116\n",
      "Epoch 011 | train 0.497600 | val 0.553688\n",
      "Epoch 012 | train 0.477999 | val 0.558814\n",
      "Epoch 013 | train 0.460035 | val 0.568987\n",
      "Epoch 014 | train 0.447296 | val 0.583842\n",
      "Epoch 015 | train 0.434059 | val 0.597883\n",
      "Epoch 016 | train 0.421347 | val 0.610829\n",
      "Epoch 017 | train 0.410114 | val 0.618806\n",
      "Epoch 018 | train 0.399269 | val 0.613164\n",
      "Epoch 019 | train 0.388991 | val 0.607360\n",
      "Epoch 020 | train 0.379592 | val 0.605932\n",
      "Epoch 021 | train 0.371988 | val 0.605588\n",
      "Epoch 022 | train 0.362309 | val 0.610868\n",
      "Epoch 023 | train 0.353223 | val 0.615332\n",
      "Epoch 024 | train 0.343790 | val 0.621492\n",
      "Epoch 025 | train 0.333709 | val 0.627661\n",
      "Epoch 026 | train 0.322056 | val 0.638110\n",
      "Epoch 027 | train 0.310418 | val 0.653319\n",
      "Epoch 028 | train 0.297807 | val 0.663768\n",
      "Epoch 029 | train 0.283792 | val 0.673915\n",
      "Epoch 030 | train 0.269433 | val 0.686931\n",
      "Epoch 031 | train 0.253989 | val 0.702484\n",
      "Early stopping at epoch 031 (best val 0.553688)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7202, macro_AUPRC_optthr=0.7352, macro_Brier_optthr=0.2107, macro_ACC_optthr=0.6408, macro_F1_optthr=0.6754, macro_Precision_optthr=0.6111, macro_Recall_optthr=0.7549, macro_Sensitivity_optthr=0.7549, macro_Specificity_optthr=0.5288\n",
      "\n",
      "CV averages (95% CI) @0.5:\n",
      "           macro_ACC: 0.6596  (95% CI 0.5770, 0.7421)\n",
      "           macro_AUC: 0.7197  (95% CI 0.6398, 0.7997)\n",
      "         macro_AUPRC: 0.7584  (95% CI 0.6797, 0.8371)\n",
      "         macro_Brier: 0.2283  (95% CI 0.1789, 0.2777)\n",
      "            macro_F1: 0.6627  (95% CI 0.5666, 0.7588)\n",
      "     macro_Precision: 0.7109  (95% CI 0.6062, 0.8156)\n",
      "        macro_Recall: 0.6319  (95% CI 0.4835, 0.7803)\n",
      "   macro_Sensitivity: 0.6319  (95% CI 0.4835, 0.7803)\n",
      "   macro_Specificity: 0.6954  (95% CI 0.5535, 0.8373)\n",
      "          task_1_ACC: 0.6596  (95% CI 0.5770, 0.7421)\n",
      "          task_1_AUC: 0.7197  (95% CI 0.6398, 0.7997)\n",
      "        task_1_AUPRC: 0.7584  (95% CI 0.6797, 0.8371)\n",
      "        task_1_Brier: 0.2283  (95% CI 0.1789, 0.2777)\n",
      "           task_1_F1: 0.6627  (95% CI 0.5666, 0.7588)\n",
      "    task_1_Precision: 0.7109  (95% CI 0.6062, 0.8156)\n",
      "       task_1_Recall: 0.6319  (95% CI 0.4835, 0.7803)\n",
      "  task_1_Sensitivity: 0.6319  (95% CI 0.4835, 0.7803)\n",
      "  task_1_Specificity: 0.6954  (95% CI 0.5535, 0.8373)\n",
      "\n",
      "CV averages (95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.6275  (95% CI 0.5826, 0.6725)\n",
      "    macro_AUC_optthr: 0.7197  (95% CI 0.6398, 0.7997)\n",
      "  macro_AUPRC_optthr: 0.7584  (95% CI 0.6797, 0.8371)\n",
      "  macro_Brier_optthr: 0.2283  (95% CI 0.1789, 0.2777)\n",
      "     macro_F1_optthr: 0.6770  (95% CI 0.6065, 0.7476)\n",
      "macro_Precision_optthr: 0.6329  (95% CI 0.5758, 0.6901)\n",
      " macro_Recall_optthr: 0.7390  (95% CI 0.5804, 0.8976)\n",
      "macro_Sensitivity_optthr: 0.7390  (95% CI 0.5804, 0.8976)\n",
      "macro_Specificity_optthr: 0.4960  (95% CI 0.3501, 0.6419)\n",
      "   task_1_ACC_optthr: 0.6275  (95% CI 0.5826, 0.6725)\n",
      "   task_1_AUC_optthr: 0.7197  (95% CI 0.6398, 0.7997)\n",
      " task_1_AUPRC_optthr: 0.7584  (95% CI 0.6797, 0.8371)\n",
      " task_1_Brier_optthr: 0.2283  (95% CI 0.1789, 0.2777)\n",
      "    task_1_F1_optthr: 0.6770  (95% CI 0.6065, 0.7476)\n",
      "task_1_Precision_optthr: 0.6329  (95% CI 0.5758, 0.6901)\n",
      "task_1_Recall_optthr: 0.7390  (95% CI 0.5804, 0.8976)\n",
      "task_1_Sensitivity_optthr: 0.7390  (95% CI 0.5804, 0.8976)\n",
      "task_1_Specificity_optthr: 0.4960  (95% CI 0.3501, 0.6419)\n",
      "Arch params (used in all folds): {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0, 'use_attention': False, 'use_random': False}\n",
      "Train params (used in all folds): {'lr': 0.0003, 'weight_decay': 3e-06, 'batch_size': 256, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 0.001, 'lambda_ridge': 0.003, 'lambda_orth_latent': 0.003, 'lambda_film_identity': 0.003, 'lambda_kan': 0, 'random_state': 42}\n",
      "\n",
      "[CV fold 1/5] scenario=time\n",
      "[split] train | rows=  159 | unique_ids=  159\n",
      "[split] val   | rows=   17 | unique_ids=   17\n",
      "[split] test  | rows=  171 | unique_ids=  142\n",
      "Epoch 001 | train 0.662580 | val 0.664853\n",
      "Epoch 002 | train 0.661693 | val 0.664018\n",
      "Epoch 003 | train 0.660766 | val 0.663157\n",
      "Epoch 004 | train 0.659754 | val 0.662245\n",
      "Epoch 005 | train 0.658613 | val 0.661253\n",
      "Epoch 006 | train 0.657296 | val 0.660160\n",
      "Epoch 007 | train 0.655762 | val 0.658943\n",
      "Epoch 008 | train 0.653972 | val 0.657583\n",
      "Epoch 009 | train 0.651886 | val 0.656064\n",
      "Epoch 010 | train 0.649464 | val 0.654372\n",
      "Epoch 011 | train 0.646663 | val 0.652495\n",
      "Epoch 012 | train 0.643440 | val 0.650421\n",
      "Epoch 013 | train 0.639750 | val 0.648143\n",
      "Epoch 014 | train 0.635549 | val 0.645658\n",
      "Epoch 015 | train 0.630793 | val 0.642965\n",
      "Epoch 016 | train 0.625439 | val 0.640070\n",
      "Epoch 017 | train 0.619450 | val 0.636987\n",
      "Epoch 018 | train 0.612796 | val 0.633737\n",
      "Epoch 019 | train 0.605453 | val 0.630352\n",
      "Epoch 020 | train 0.597411 | val 0.626877\n",
      "Epoch 021 | train 0.588667 | val 0.623363\n",
      "Epoch 022 | train 0.579233 | val 0.619876\n",
      "Epoch 023 | train 0.569129 | val 0.616491\n",
      "Epoch 024 | train 0.558388 | val 0.613299\n",
      "Epoch 025 | train 0.547054 | val 0.610409\n",
      "Epoch 026 | train 0.535184 | val 0.607958\n",
      "Epoch 027 | train 0.522847 | val 0.606098\n",
      "Epoch 028 | train 0.510116 | val 0.604996\n",
      "Epoch 029 | train 0.497062 | val 0.604822\n",
      "Epoch 030 | train 0.483751 | val 0.605744\n",
      "Epoch 031 | train 0.470233 | val 0.607920\n",
      "Epoch 032 | train 0.456538 | val 0.611491\n",
      "Epoch 033 | train 0.442662 | val 0.616585\n",
      "Epoch 034 | train 0.428575 | val 0.623311\n",
      "Epoch 035 | train 0.414231 | val 0.631764\n",
      "Epoch 036 | train 0.399579 | val 0.642040\n",
      "Epoch 037 | train 0.384578 | val 0.654246\n",
      "Epoch 038 | train 0.369203 | val 0.668521\n",
      "Epoch 039 | train 0.353444 | val 0.685028\n",
      "Epoch 040 | train 0.337293 | val 0.703944\n",
      "Epoch 041 | train 0.320737 | val 0.725433\n",
      "Epoch 042 | train 0.303751 | val 0.749622\n",
      "Epoch 043 | train 0.286289 | val 0.776588\n",
      "Epoch 044 | train 0.268300 | val 0.806369\n",
      "Epoch 045 | train 0.249776 | val 0.838982\n",
      "Epoch 046 | train 0.230832 | val 0.874432\n",
      "Epoch 047 | train 0.211705 | val 0.912676\n",
      "Epoch 048 | train 0.192639 | val 0.953575\n",
      "Epoch 049 | train 0.173969 | val 0.996885\n",
      "Early stopping at epoch 049 (best val 0.604822)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7301, macro_AUPRC_optthr=0.7496, macro_Brier_optthr=0.2079, macro_ACC_optthr=0.6550, macro_F1_optthr=0.7094, macro_Precision_optthr=0.6729, macro_Recall_optthr=0.7500, macro_Sensitivity_optthr=0.7500, macro_Specificity_optthr=0.5333\n",
      "\n",
      "[CV fold 2/5] scenario=time\n",
      "[split] train | rows=  313 | unique_ids=  242\n",
      "[split] val   | rows=   34 | unique_ids=   33\n",
      "[split] test  | rows=  171 | unique_ids=  132\n",
      "Epoch 001 | train 0.646299 | val 0.640797\n",
      "Epoch 002 | train 0.644736 | val 0.640539\n",
      "Epoch 003 | train 0.643276 | val 0.639890\n",
      "Epoch 004 | train 0.641452 | val 0.638893\n",
      "Epoch 005 | train 0.639109 | val 0.637544\n",
      "Epoch 006 | train 0.635983 | val 0.635560\n",
      "Epoch 007 | train 0.632155 | val 0.632856\n",
      "Epoch 008 | train 0.627043 | val 0.629434\n",
      "Epoch 009 | train 0.620848 | val 0.625106\n",
      "Epoch 010 | train 0.613277 | val 0.620028\n",
      "Epoch 011 | train 0.603725 | val 0.614102\n",
      "Epoch 012 | train 0.593191 | val 0.607185\n",
      "Epoch 013 | train 0.582007 | val 0.600553\n",
      "Epoch 014 | train 0.569565 | val 0.595773\n",
      "Epoch 015 | train 0.555037 | val 0.592917\n",
      "Epoch 016 | train 0.541830 | val 0.592154\n",
      "Epoch 017 | train 0.528200 | val 0.593548\n",
      "Epoch 018 | train 0.514374 | val 0.597175\n",
      "Epoch 019 | train 0.501282 | val 0.604034\n",
      "Epoch 020 | train 0.488601 | val 0.612401\n",
      "Epoch 021 | train 0.475486 | val 0.623822\n",
      "Epoch 022 | train 0.462714 | val 0.639257\n",
      "Epoch 023 | train 0.449349 | val 0.657003\n",
      "Epoch 024 | train 0.435474 | val 0.676807\n",
      "Epoch 025 | train 0.421947 | val 0.701003\n",
      "Epoch 026 | train 0.407398 | val 0.726629\n",
      "Epoch 027 | train 0.392395 | val 0.751553\n",
      "Epoch 028 | train 0.377466 | val 0.776437\n",
      "Epoch 029 | train 0.361593 | val 0.805542\n",
      "Epoch 030 | train 0.344936 | val 0.844305\n",
      "Epoch 031 | train 0.329294 | val 0.887300\n",
      "Epoch 032 | train 0.314948 | val 0.913354\n",
      "Epoch 033 | train 0.299406 | val 0.923053\n",
      "Epoch 034 | train 0.285190 | val 0.927678\n",
      "Epoch 035 | train 0.273051 | val 0.942375\n",
      "Epoch 036 | train 0.263520 | val 0.985589\n",
      "Early stopping at epoch 036 (best val 0.592154)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7318, macro_AUPRC_optthr=0.7244, macro_Brier_optthr=0.2082, macro_ACC_optthr=0.6784, macro_F1_optthr=0.6995, macro_Precision_optthr=0.6400, macro_Recall_optthr=0.7711, macro_Sensitivity_optthr=0.7711, macro_Specificity_optthr=0.5909\n",
      "\n",
      "[CV fold 3/5] scenario=time\n",
      "[split] train | rows=  467 | unique_ids=  252\n",
      "[split] val   | rows=   51 | unique_ids=   47\n",
      "[split] test  | rows=  171 | unique_ids=  119\n",
      "Epoch 001 | train 0.676470 | val 0.672296\n",
      "Epoch 002 | train 0.674953 | val 0.670739\n",
      "Epoch 003 | train 0.673435 | val 0.668401\n",
      "Epoch 004 | train 0.671198 | val 0.665879\n",
      "Epoch 005 | train 0.668708 | val 0.662400\n",
      "Epoch 006 | train 0.665335 | val 0.658079\n",
      "Epoch 007 | train 0.660952 | val 0.652517\n",
      "Epoch 008 | train 0.655405 | val 0.645623\n",
      "Epoch 009 | train 0.648187 | val 0.637348\n",
      "Epoch 010 | train 0.639195 | val 0.627572\n",
      "Epoch 011 | train 0.628178 | val 0.616908\n",
      "Epoch 012 | train 0.616453 | val 0.605738\n",
      "Epoch 013 | train 0.603116 | val 0.594773\n",
      "Epoch 014 | train 0.587932 | val 0.585565\n",
      "Epoch 015 | train 0.571786 | val 0.578180\n",
      "Epoch 016 | train 0.558493 | val 0.573673\n",
      "Epoch 017 | train 0.542976 | val 0.573695\n",
      "Epoch 018 | train 0.530603 | val 0.576790\n",
      "Epoch 019 | train 0.517382 | val 0.581751\n",
      "Epoch 020 | train 0.507272 | val 0.586158\n",
      "Epoch 021 | train 0.496024 | val 0.590512\n",
      "Epoch 022 | train 0.483217 | val 0.591519\n",
      "Epoch 023 | train 0.470750 | val 0.592418\n",
      "Epoch 024 | train 0.456542 | val 0.591595\n",
      "Epoch 025 | train 0.443893 | val 0.592654\n",
      "Epoch 026 | train 0.431276 | val 0.594491\n",
      "Epoch 027 | train 0.417905 | val 0.599021\n",
      "Epoch 028 | train 0.406012 | val 0.611410\n",
      "Epoch 029 | train 0.393532 | val 0.617906\n",
      "Epoch 030 | train 0.380883 | val 0.619636\n",
      "Epoch 031 | train 0.367747 | val 0.616917\n",
      "Epoch 032 | train 0.355079 | val 0.616489\n",
      "Epoch 033 | train 0.341781 | val 0.623710\n",
      "Epoch 034 | train 0.328642 | val 0.629532\n",
      "Epoch 035 | train 0.314175 | val 0.639645\n",
      "Epoch 036 | train 0.299921 | val 0.649098\n",
      "Early stopping at epoch 036 (best val 0.573673)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7752, macro_AUPRC_optthr=0.7626, macro_Brier_optthr=0.1993, macro_ACC_optthr=0.7018, macro_F1_optthr=0.6909, macro_Precision_optthr=0.7215, macro_Recall_optthr=0.6628, macro_Sensitivity_optthr=0.6628, macro_Specificity_optthr=0.7412\n",
      "\n",
      "[CV fold 4/5] scenario=time\n",
      "[split] train | rows=  621 | unique_ids=  252\n",
      "[split] val   | rows=   68 | unique_ids=   61\n",
      "[split] test  | rows=  171 | unique_ids=  110\n",
      "Epoch 001 | train 0.676190 | val 0.673211\n",
      "Epoch 002 | train 0.674064 | val 0.671450\n",
      "Epoch 003 | train 0.671114 | val 0.669213\n",
      "Epoch 004 | train 0.667016 | val 0.665634\n",
      "Epoch 005 | train 0.660972 | val 0.660375\n",
      "Epoch 006 | train 0.651378 | val 0.652939\n",
      "Epoch 007 | train 0.639000 | val 0.643316\n",
      "Epoch 008 | train 0.623452 | val 0.632472\n",
      "Epoch 009 | train 0.603310 | val 0.622555\n",
      "Epoch 010 | train 0.582620 | val 0.615786\n",
      "Epoch 011 | train 0.561006 | val 0.615657\n",
      "Epoch 012 | train 0.540064 | val 0.624748\n",
      "Epoch 013 | train 0.523981 | val 0.643250\n",
      "Epoch 014 | train 0.509309 | val 0.663987\n",
      "Epoch 015 | train 0.499625 | val 0.680558\n",
      "Epoch 016 | train 0.486505 | val 0.682049\n",
      "Epoch 017 | train 0.473482 | val 0.681112\n",
      "Epoch 018 | train 0.462354 | val 0.682897\n",
      "Epoch 019 | train 0.450530 | val 0.694749\n",
      "Epoch 020 | train 0.437250 | val 0.712762\n",
      "Epoch 021 | train 0.424883 | val 0.731664\n",
      "Epoch 022 | train 0.414132 | val 0.741843\n",
      "Epoch 023 | train 0.400582 | val 0.746425\n",
      "Epoch 024 | train 0.387865 | val 0.754764\n",
      "Epoch 025 | train 0.373484 | val 0.770886\n",
      "Epoch 026 | train 0.359683 | val 0.797087\n",
      "Epoch 027 | train 0.345928 | val 0.811236\n",
      "Epoch 028 | train 0.329922 | val 0.806379\n",
      "Epoch 029 | train 0.314903 | val 0.825269\n",
      "Epoch 030 | train 0.300127 | val 0.862224\n",
      "Epoch 031 | train 0.282510 | val 0.912050\n",
      "Early stopping at epoch 031 (best val 0.615657)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8202, macro_AUPRC_optthr=0.8731, macro_Brier_optthr=0.1805, macro_ACC_optthr=0.7661, macro_F1_optthr=0.7938, macro_Precision_optthr=0.8021, macro_Recall_optthr=0.7857, macro_Sensitivity_optthr=0.7857, macro_Specificity_optthr=0.7397\n",
      "\n",
      "[CV fold 5/5] scenario=time\n",
      "[split] train | rows=  774 | unique_ids=  250\n",
      "[split] val   | rows=   86 | unique_ids=   71\n",
      "[split] test  | rows=  171 | unique_ids=   82\n",
      "Epoch 001 | train 0.645546 | val 0.648667\n",
      "Epoch 002 | train 0.641927 | val 0.645689\n",
      "Epoch 003 | train 0.637618 | val 0.642270\n",
      "Epoch 004 | train 0.630796 | val 0.636472\n",
      "Epoch 005 | train 0.621713 | val 0.628183\n",
      "Epoch 006 | train 0.610371 | val 0.617765\n",
      "Epoch 007 | train 0.597119 | val 0.606235\n",
      "Epoch 008 | train 0.582995 | val 0.593137\n",
      "Epoch 009 | train 0.565202 | val 0.580086\n",
      "Epoch 010 | train 0.547176 | val 0.570032\n",
      "Epoch 011 | train 0.531522 | val 0.564918\n",
      "Epoch 012 | train 0.523780 | val 0.566380\n",
      "Epoch 013 | train 0.519511 | val 0.570804\n",
      "Epoch 014 | train 0.514687 | val 0.575458\n",
      "Epoch 015 | train 0.508921 | val 0.576692\n",
      "Epoch 016 | train 0.504304 | val 0.576991\n",
      "Epoch 017 | train 0.499432 | val 0.577642\n",
      "Epoch 018 | train 0.494421 | val 0.578809\n",
      "Epoch 019 | train 0.490184 | val 0.579242\n",
      "Epoch 020 | train 0.483676 | val 0.578110\n",
      "Epoch 021 | train 0.475269 | val 0.576008\n",
      "Epoch 022 | train 0.464993 | val 0.574185\n",
      "Epoch 023 | train 0.458230 | val 0.576657\n",
      "Epoch 024 | train 0.455787 | val 0.577677\n",
      "Epoch 025 | train 0.449258 | val 0.583400\n",
      "Epoch 026 | train 0.443275 | val 0.591783\n",
      "Epoch 027 | train 0.438562 | val 0.587630\n",
      "Epoch 028 | train 0.435078 | val 0.586872\n",
      "Epoch 029 | train 0.431986 | val 0.585707\n",
      "Epoch 030 | train 0.425702 | val 0.587627\n",
      "Epoch 031 | train 0.420272 | val 0.576562\n",
      "Early stopping at epoch 031 (best val 0.564918)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7933, macro_AUPRC_optthr=0.8703, macro_Brier_optthr=0.1836, macro_ACC_optthr=0.7018, macro_F1_optthr=0.7811, macro_Precision_optthr=0.6947, macro_Recall_optthr=0.8922, macro_Sensitivity_optthr=0.8922, macro_Specificity_optthr=0.4203\n",
      "\n",
      "CV averages (95% CI) @0.5:\n",
      "           macro_ACC: 0.6819  (95% CI 0.6432, 0.7205)\n",
      "           macro_AUC: 0.7701  (95% CI 0.7215, 0.8188)\n",
      "         macro_AUPRC: 0.7960  (95% CI 0.7085, 0.8835)\n",
      "         macro_Brier: 0.1959  (95% CI 0.1795, 0.2123)\n",
      "            macro_F1: 0.6789  (95% CI 0.6416, 0.7162)\n",
      "     macro_Precision: 0.7527  (95% CI 0.6787, 0.8268)\n",
      "        macro_Recall: 0.6206  (95% CI 0.5766, 0.6645)\n",
      "   macro_Sensitivity: 0.6206  (95% CI 0.5766, 0.6645)\n",
      "   macro_Specificity: 0.7523  (95% CI 0.6490, 0.8556)\n",
      "          task_1_ACC: 0.6819  (95% CI 0.6432, 0.7205)\n",
      "          task_1_AUC: 0.7701  (95% CI 0.7215, 0.8188)\n",
      "        task_1_AUPRC: 0.7960  (95% CI 0.7085, 0.8835)\n",
      "        task_1_Brier: 0.1959  (95% CI 0.1795, 0.2123)\n",
      "           task_1_F1: 0.6789  (95% CI 0.6416, 0.7162)\n",
      "    task_1_Precision: 0.7527  (95% CI 0.6787, 0.8268)\n",
      "       task_1_Recall: 0.6206  (95% CI 0.5766, 0.6645)\n",
      "  task_1_Sensitivity: 0.6206  (95% CI 0.5766, 0.6645)\n",
      "  task_1_Specificity: 0.7523  (95% CI 0.6490, 0.8556)\n",
      "\n",
      "CV averages (95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.7006  (95% CI 0.6491, 0.7520)\n",
      "    macro_AUC_optthr: 0.7701  (95% CI 0.7215, 0.8188)\n",
      "  macro_AUPRC_optthr: 0.7960  (95% CI 0.7085, 0.8835)\n",
      "  macro_Brier_optthr: 0.1959  (95% CI 0.1795, 0.2123)\n",
      "     macro_F1_optthr: 0.7349  (95% CI 0.6746, 0.7953)\n",
      "macro_Precision_optthr: 0.7062  (95% CI 0.6301, 0.7824)\n",
      " macro_Recall_optthr: 0.7723  (95% CI 0.6703, 0.8744)\n",
      "macro_Sensitivity_optthr: 0.7723  (95% CI 0.6703, 0.8744)\n",
      "macro_Specificity_optthr: 0.6051  (95% CI 0.4338, 0.7764)\n",
      "   task_1_ACC_optthr: 0.7006  (95% CI 0.6491, 0.7520)\n",
      "   task_1_AUC_optthr: 0.7701  (95% CI 0.7215, 0.8188)\n",
      " task_1_AUPRC_optthr: 0.7960  (95% CI 0.7085, 0.8835)\n",
      " task_1_Brier_optthr: 0.1959  (95% CI 0.1795, 0.2123)\n",
      "    task_1_F1_optthr: 0.7349  (95% CI 0.6746, 0.7953)\n",
      "task_1_Precision_optthr: 0.7062  (95% CI 0.6301, 0.7824)\n",
      "task_1_Recall_optthr: 0.7723  (95% CI 0.6703, 0.8744)\n",
      "task_1_Sensitivity_optthr: 0.7723  (95% CI 0.6703, 0.8744)\n",
      "task_1_Specificity_optthr: 0.6051  (95% CI 0.4338, 0.7764)\n",
      "Arch params (used in all folds): {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0, 'use_attention': False, 'use_random': False}\n",
      "Train params (used in all folds): {'lr': 0.0003, 'weight_decay': 3e-06, 'batch_size': 256, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 0.001, 'lambda_ridge': 0.003, 'lambda_orth_latent': 0.003, 'lambda_film_identity': 0.003, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "arch_defaults = dict(d_fix_latent=256, d_rand_latent=256, n_kernels=8, dropout=0.0, use_attention = False, use_random = False)\n",
    "\n",
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"cv_only\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"both\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9138297",
   "metadata": {},
   "source": [
    "### Cases split CV with parameter search test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outer fold 1/3\n",
      "Outer fold macro (optthr): macro_AUC_optthr=0.7936, macro_AUPRC_optthr=0.8187, macro_Brier_optthr=0.1857, macro_ACC_optthr=0.6860, macro_F1_optthr=0.7465, macro_Precision_optthr=0.6570, macro_Recall_optthr=0.8641, macro_Sensitivity_optthr=0.8641, macro_Specificity_optthr=0.4813\n",
      "\n",
      "Outer fold 2/3\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"d_fix_latent\": [128],\n",
    "    \"d_rand_latent\": [128],\n",
    "    \"n_kernels\": [6],\n",
    "    \"dropout\": [0.1],\n",
    "\n",
    "    \"lr\": [1e-4, 3e-4],\n",
    "    \"weight_decay\": [1e-4, 1e-3],\n",
    "    \"batch_size\": [128, 64],\n",
    "    \"max_epochs\": [100],\n",
    "    \"patience\": [10],\n",
    "\n",
    "    \"lambda_mean0\": [1e-4, 1e-3],\n",
    "    \"lambda_ridge\": [1e-4, 3e-4],\n",
    "    \"lambda_orth_latent\": [1e-3, 1e-4],\n",
    "    \"lambda_film_identity\": [1e-4, 1e-3],\n",
    "    \"lambda_kan\": [0.0, 1e-4],\n",
    "}\n",
    "\n",
    "res_cases = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # used for BOTH X_fix and TC\n",
    "    X_fixed_and_random=X_fixed_and_random_np,           # used for Zrand\n",
    "    y=y_np,\n",
    "    y_lags=y_lags_np,\n",
    "    dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np,\n",
    "    time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,         # the dynamic-dims builder we just fixed\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"nested_cv\",                         # or \"cv_only\" / \"nested_cv\"\n",
    "    scenario=\"cases\",                       # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=3,\n",
    "    inner_folds=2,\n",
    "    param_grid=param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec27e2",
   "metadata": {},
   "source": [
    "### Time split CV with parameter search test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cases = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_fix_np,                 # used for BOTH X_fix and TC\n",
    "    X_fixed_and_random=Zrand_np,           # used for Zrand\n",
    "    y=y_np,\n",
    "    y_lags=y_lags_np,\n",
    "    dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np,\n",
    "    time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,         # the dynamic-dims builder we just fixed\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"nested_cv\",                         # or \"cv_only\" / \"nested_cv\"\n",
    "    scenario=\"time\",                       # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=3,\n",
    "    inner_folds=2,\n",
    "    param_grid=param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23d56b",
   "metadata": {},
   "source": [
    "## New outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a226172d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_rows': 509,\n",
       " 'n_unique_participants': 33,\n",
       " 'avg_rows_per_participant': np.float64(15.424242424242424),\n",
       " 'min_rows_per_participant': 1,\n",
       " 'max_rows_per_participant': 31,\n",
       " 'avg_age_per_participant': np.float64(28.96969696969697),\n",
       " 'avg_age_row_weighted': np.float64(28.398821218074655),\n",
       " 'num_ones_next_outcome': np.int64(304),\n",
       " 'pct_positive_next_outcome': np.float64(59.72495088408645)}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Body_cat_df = pd.read_csv(os.path.join(data_dir, \"Body_data_for_categorical_forecast.csv\"))\n",
    "columns_Body_cat_df = pd.read_csv(os.path.join(data_dir, \"columns_Body_data_for_categorical_forecast.csv\"))\n",
    "\n",
    "Body_cat_df.head()\n",
    "\n",
    "n_rows = len(Body_cat_df)\n",
    "n_unique_participants = Body_cat_df[\"id\"].nunique()\n",
    "\n",
    "\n",
    "group_sizes = Body_cat_df.groupby(\"id\").size()\n",
    "avg_rows_per_id = group_sizes.mean()\n",
    "min_rows_per_id = group_sizes.min()\n",
    "max_rows_per_id = group_sizes.max()\n",
    "\n",
    "\n",
    "avg_age_per_participant = (\n",
    "    Body_cat_df.groupby(\"id\")[\"age\"].first().mean()\n",
    ")\n",
    "avg_age_row_weighted = Body_cat_df[\"age\"].mean()\n",
    "\n",
    "# Next-outcome positives\n",
    "num_ones_next_outcome = (Body_cat_df[\"critical_event_next\"] == 1).sum()\n",
    "pct_positive_next_outcome = 100.0 * (\n",
    "    Body_cat_df[\"critical_event_next\"] == 1\n",
    ").mean()  # % over all rows\n",
    "\n",
    "summary = {\n",
    "    \"n_rows\": n_rows,\n",
    "    \"n_unique_participants\": n_unique_participants,\n",
    "    \"avg_rows_per_participant\": avg_rows_per_id,\n",
    "    \"min_rows_per_participant\": min_rows_per_id,\n",
    "    \"max_rows_per_participant\": max_rows_per_id,\n",
    "    \"avg_age_per_participant\": avg_age_per_participant,\n",
    "    \"avg_age_row_weighted\": avg_age_row_weighted,\n",
    "    \"num_ones_next_outcome\": num_ones_next_outcome,\n",
    "    \"pct_positive_next_outcome\": pct_positive_next_outcome,\n",
    "}\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "395aa2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the outcome column(s) marked with 1 in the \"outcomes\" column of columns_Body_cat_df\n",
    "Body_cat_outcome_cols = columns_Body_cat_df.loc[columns_Body_cat_df['outcomes'] == 1, 'column_name'].tolist()\n",
    "Body_cat_y = Body_cat_df[Body_cat_outcome_cols]\n",
    "Body_cat_y.head()\n",
    "\n",
    "# Same for outcomes lags column(s)\n",
    "Body_cat_outcomes_lags_cols = columns_Body_cat_df.loc[columns_Body_cat_df['outcomes_lags'] == 1, 'column_name'].tolist()\n",
    "Body_cat_outcomes_lags = Body_cat_df[Body_cat_outcomes_lags_cols]\n",
    "\n",
    "# Same for participant column(s)\n",
    "Body_cat_participant_cols = columns_Body_cat_df.loc[columns_Body_cat_df['participant_id'] == 1, 'column_name'].tolist()\n",
    "Body_cat_participant_id = Body_cat_df[Body_cat_participant_cols]\n",
    "\n",
    "# Same for time column(s)\n",
    "Body_cat_time_cols = columns_Body_cat_df.loc[columns_Body_cat_df['time'] == 1, 'column_name'].tolist()\n",
    "Body_cat_time = Body_cat_df[Body_cat_time_cols]\n",
    "\n",
    "# Same for forecast horizons column(s)\n",
    "Body_cat_forecast_horizons_cols = columns_Body_cat_df.loc[columns_Body_cat_df['forecast_horizons'] == 1, 'column_name'].tolist()\n",
    "Body_cat_forecast_horizons = Body_cat_df[Body_cat_forecast_horizons_cols]\n",
    "\n",
    "# Same for fixed effects column(s)\n",
    "Body_cat_only_fixed_cols = columns_Body_cat_df.loc[columns_Body_cat_df['only_fixed'] == 1, 'column_name'].tolist()\n",
    "Body_cat_only_fixed = Body_cat_df[Body_cat_only_fixed_cols]\n",
    "\n",
    "# Same for random effects column(s)\n",
    "Body_cat_fixed_and_random_cols = columns_Body_cat_df.loc[columns_Body_cat_df['fixed_and_random'] == 1, 'column_name'].tolist()\n",
    "Body_cat_fixed_and_random = Body_cat_df[Body_cat_fixed_and_random_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ea09b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ---------- targets & aux ----------\n",
    "y_raw      = Body_cat_y.to_numpy(np.float32)\n",
    "y_np       = y_raw if y_raw.ndim == 2 else y_raw.reshape(-1, 1)\n",
    "\n",
    "y_lags_np  = Body_cat_outcomes_lags.to_numpy(np.float32)\n",
    "dt_lags_np = Body_cat_forecast_horizons.to_numpy(np.float32)\n",
    "\n",
    "# ---------- inputs ----------\n",
    "# keep the two blocks SEPARATE; evaluator will:\n",
    "#   - TC  <- only_fixed (after PCA)\n",
    "#   - X_fix <- concat(only_fixed_PCA, fixed_and_random_PCA)\n",
    "#   - Zrand <- fixed_and_random_PCA\n",
    "X_only_fixed_np        = Body_cat_only_fixed.to_numpy(np.float32)\n",
    "X_fixed_and_random_np  = Body_cat_fixed_and_random.to_numpy(np.float32)\n",
    "\n",
    "# ---------- ids & time ----------\n",
    "pid_raw    = Body_cat_participant_id.to_numpy().ravel()\n",
    "pid_uniqs, pid_encoded = np.unique(pid_raw, return_inverse=True)\n",
    "pid_np     = pid_encoded.astype(np.int64)\n",
    "n_ids      = int(len(pid_uniqs))\n",
    "\n",
    "time_ix_np = Body_cat_time.to_numpy().ravel()\n",
    "\n",
    "# ---------- dynamic builder ----------\n",
    "def build_model_fn(\n",
    "    *,\n",
    "    y_dim: int,\n",
    "    d_fix: int,\n",
    "    d_tc: int,        # this will be the PCAd only_fixed dim (activates FiLM)\n",
    "    d_zrand: int,     # this will be the PCAd fixed_and_random dim\n",
    "    n_ids: int,\n",
    "    **arch\n",
    "):\n",
    "    return KANAdditiveMixedEffects(\n",
    "        y_dim=y_dim,\n",
    "        d_fix=d_fix,       # = dim(concat(only_fixed_PCA, fixed_and_random_PCA))\n",
    "        d_tc=d_tc,         # = dim(only_fixed_PCA)  -> used by FiLM\n",
    "        d_zrand=d_zrand,   # = dim(fixed_and_random_PCA)\n",
    "        n_ids=n_ids,\n",
    "        **arch\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "arch_defaults = dict(d_fix_latent=32, d_rand_latent=32, n_kernels=6, dropout=0.00)\n",
    "train_defaults = dict(\n",
    "    lr=5e-4, weight_decay=3e-5, batch_size=64,\n",
    "    max_epochs=100, patience=20, threshold=0.5,\n",
    "    lambda_mean0=1e-6, lambda_ridge=1e-5,\n",
    "    lambda_orth_latent=3e-5,\n",
    "    lambda_film_identity=3e-5,\n",
    "    lambda_kan=0,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2b504de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train 0.526399 | val 0.536068\n",
      "Epoch 002 | train 0.504631 | val 0.514953\n",
      "Epoch 003 | train 0.473995 | val 0.491043\n",
      "Epoch 004 | train 0.437780 | val 0.471919\n",
      "Epoch 005 | train 0.404375 | val 0.438891\n",
      "Epoch 006 | train 0.365621 | val 0.424810\n",
      "Epoch 007 | train 0.342439 | val 0.437593\n",
      "Epoch 008 | train 0.331724 | val 0.469246\n",
      "Epoch 009 | train 0.324498 | val 0.485574\n",
      "Epoch 010 | train 0.315082 | val 0.449413\n",
      "Epoch 011 | train 0.302206 | val 0.451521\n",
      "Epoch 012 | train 0.296104 | val 0.458941\n",
      "Epoch 013 | train 0.287374 | val 0.455781\n",
      "Epoch 014 | train 0.280117 | val 0.467125\n",
      "Epoch 015 | train 0.272721 | val 0.477199\n",
      "Epoch 016 | train 0.263935 | val 0.496088\n",
      "Epoch 017 | train 0.259578 | val 0.487866\n",
      "Epoch 018 | train 0.249786 | val 0.514506\n",
      "Epoch 019 | train 0.240466 | val 0.541955\n",
      "Epoch 020 | train 0.230078 | val 0.528945\n",
      "Epoch 021 | train 0.217564 | val 0.551798\n",
      "Epoch 022 | train 0.201423 | val 0.574612\n",
      "Epoch 023 | train 0.190832 | val 0.623385\n",
      "Epoch 024 | train 0.171922 | val 0.654402\n",
      "Epoch 025 | train 0.151953 | val 0.672535\n",
      "Epoch 026 | train 0.137309 | val 0.753851\n",
      "Early stopping at epoch 026 (best val 0.424810)\n",
      "\n",
      "Single-fit test metrics @0.5:\n",
      "        task_1_AUC: 0.7480\n",
      "      task_1_AUPRC: 0.7791\n",
      "      task_1_Brier: 0.2125\n",
      "        task_1_ACC: 0.6889\n",
      "         task_1_F1: 0.6744\n",
      "  task_1_Precision: 0.7632\n",
      "     task_1_Recall: 0.6042\n",
      "task_1_Sensitivity: 0.6042\n",
      "task_1_Specificity: 0.7857\n",
      "         macro_AUC: 0.7480\n",
      "       macro_AUPRC: 0.7791\n",
      "       macro_Brier: 0.2125\n",
      "         macro_ACC: 0.6889\n",
      "          macro_F1: 0.6744\n",
      "   macro_Precision: 0.7632\n",
      "      macro_Recall: 0.6042\n",
      " macro_Sensitivity: 0.6042\n",
      " macro_Specificity: 0.7857\n",
      "\n",
      "Single-fit test metrics @F1-opt per task:\n",
      " task_1_AUC_optthr: 0.7480\n",
      "task_1_AUPRC_optthr: 0.7791\n",
      "task_1_Brier_optthr: 0.2125\n",
      " task_1_ACC_optthr: 0.5778\n",
      "  task_1_F1_optthr: 0.7077\n",
      "task_1_Precision_optthr: 0.5610\n",
      "task_1_Recall_optthr: 0.9583\n",
      "task_1_Sensitivity_optthr: 0.9583\n",
      "task_1_Specificity_optthr: 0.1429\n",
      "  macro_AUC_optthr: 0.7480\n",
      "macro_AUPRC_optthr: 0.7791\n",
      "macro_Brier_optthr: 0.2125\n",
      "  macro_ACC_optthr: 0.5778\n",
      "   macro_F1_optthr: 0.7077\n",
      "macro_Precision_optthr: 0.5610\n",
      "macro_Recall_optthr: 0.9583\n",
      "macro_Sensitivity_optthr: 0.9583\n",
      "macro_Specificity_optthr: 0.1429\n"
     ]
    }
   ],
   "source": [
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"single\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"cases\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e61b5511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train 0.691174 | val 0.674552\n",
      "Epoch 002 | train 0.673155 | val 0.657005\n",
      "Epoch 003 | train 0.657176 | val 0.638801\n",
      "Epoch 004 | train 0.640712 | val 0.618207\n",
      "Epoch 005 | train 0.626919 | val 0.593741\n",
      "Epoch 006 | train 0.609616 | val 0.562917\n",
      "Epoch 007 | train 0.593699 | val 0.524263\n",
      "Epoch 008 | train 0.571390 | val 0.477930\n",
      "Epoch 009 | train 0.546989 | val 0.425699\n",
      "Epoch 010 | train 0.520621 | val 0.375775\n",
      "Epoch 011 | train 0.492534 | val 0.341928\n",
      "Epoch 012 | train 0.476113 | val 0.328653\n",
      "Epoch 013 | train 0.466694 | val 0.323414\n",
      "Epoch 014 | train 0.459762 | val 0.317265\n",
      "Epoch 015 | train 0.450997 | val 0.313831\n",
      "Epoch 016 | train 0.437306 | val 0.325763\n",
      "Epoch 017 | train 0.423853 | val 0.345655\n",
      "Epoch 018 | train 0.414891 | val 0.364518\n",
      "Epoch 019 | train 0.408511 | val 0.371982\n",
      "Epoch 020 | train 0.401708 | val 0.369975\n",
      "Epoch 021 | train 0.395335 | val 0.371225\n",
      "Epoch 022 | train 0.388389 | val 0.373633\n",
      "Epoch 023 | train 0.381316 | val 0.377838\n",
      "Epoch 024 | train 0.373431 | val 0.374006\n",
      "Epoch 025 | train 0.365426 | val 0.366699\n",
      "Epoch 026 | train 0.358890 | val 0.364335\n",
      "Epoch 027 | train 0.350746 | val 0.377296\n",
      "Epoch 028 | train 0.342167 | val 0.388085\n",
      "Epoch 029 | train 0.332647 | val 0.391066\n",
      "Epoch 030 | train 0.323727 | val 0.405465\n",
      "Epoch 031 | train 0.315724 | val 0.423647\n",
      "Epoch 032 | train 0.305099 | val 0.421575\n",
      "Epoch 033 | train 0.295568 | val 0.419013\n",
      "Epoch 034 | train 0.283167 | val 0.422846\n",
      "Epoch 035 | train 0.271008 | val 0.421379\n",
      "Early stopping at epoch 035 (best val 0.313831)\n",
      "\n",
      "Single-fit test metrics @0.5:\n",
      "        task_1_AUC: 0.8381\n",
      "      task_1_AUPRC: 0.8716\n",
      "      task_1_Brier: 0.1695\n",
      "        task_1_ACC: 0.7647\n",
      "         task_1_F1: 0.7447\n",
      "  task_1_Precision: 0.8333\n",
      "     task_1_Recall: 0.6731\n",
      "task_1_Sensitivity: 0.6731\n",
      "task_1_Specificity: 0.8600\n",
      "         macro_AUC: 0.8381\n",
      "       macro_AUPRC: 0.8716\n",
      "       macro_Brier: 0.1695\n",
      "         macro_ACC: 0.7647\n",
      "          macro_F1: 0.7447\n",
      "   macro_Precision: 0.8333\n",
      "      macro_Recall: 0.6731\n",
      " macro_Sensitivity: 0.6731\n",
      " macro_Specificity: 0.8600\n",
      "\n",
      "Single-fit test metrics @F1-opt per task:\n",
      " task_1_AUC_optthr: 0.8381\n",
      "task_1_AUPRC_optthr: 0.8716\n",
      "task_1_Brier_optthr: 0.1695\n",
      " task_1_ACC_optthr: 0.7745\n",
      "  task_1_F1_optthr: 0.7629\n",
      "task_1_Precision_optthr: 0.8222\n",
      "task_1_Recall_optthr: 0.7115\n",
      "task_1_Sensitivity_optthr: 0.7115\n",
      "task_1_Specificity_optthr: 0.8400\n",
      "  macro_AUC_optthr: 0.8381\n",
      "macro_AUPRC_optthr: 0.8716\n",
      "macro_Brier_optthr: 0.1695\n",
      "  macro_ACC_optthr: 0.7745\n",
      "   macro_F1_optthr: 0.7629\n",
      "macro_Precision_optthr: 0.8222\n",
      "macro_Recall_optthr: 0.7115\n",
      "macro_Sensitivity_optthr: 0.7115\n",
      "macro_Specificity_optthr: 0.8400\n"
     ]
    }
   ],
   "source": [
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"single\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"time\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f907af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV fold 1/3] scenario=cases\n",
      "[split] train | rows=  306 | unique_ids=   23\n",
      "[split] val   | rows=   33 | unique_ids=   15\n",
      "[split] test  | rows=  170 | unique_ids=   10\n",
      "Epoch 001 | train 0.603116 | val 0.573488\n",
      "Epoch 002 | train 0.577285 | val 0.532629\n",
      "Epoch 003 | train 0.537541 | val 0.476628\n",
      "Epoch 004 | train 0.482722 | val 0.430720\n",
      "Epoch 005 | train 0.440747 | val 0.402311\n",
      "Epoch 006 | train 0.414711 | val 0.410173\n",
      "Epoch 007 | train 0.410812 | val 0.419564\n",
      "Epoch 008 | train 0.380462 | val 0.439714\n",
      "Epoch 009 | train 0.364851 | val 0.460059\n",
      "Epoch 010 | train 0.349953 | val 0.464421\n",
      "Epoch 011 | train 0.336671 | val 0.474444\n",
      "Epoch 012 | train 0.327609 | val 0.486951\n",
      "Epoch 013 | train 0.317418 | val 0.465810\n",
      "Epoch 014 | train 0.307021 | val 0.456966\n",
      "Epoch 015 | train 0.294061 | val 0.458056\n",
      "Epoch 016 | train 0.284687 | val 0.471107\n",
      "Epoch 017 | train 0.273174 | val 0.464081\n",
      "Epoch 018 | train 0.255641 | val 0.451216\n",
      "Epoch 019 | train 0.244101 | val 0.458896\n",
      "Epoch 020 | train 0.229345 | val 0.489723\n",
      "Epoch 021 | train 0.215381 | val 0.516016\n",
      "Epoch 022 | train 0.195019 | val 0.522005\n",
      "Epoch 023 | train 0.173416 | val 0.554448\n",
      "Epoch 024 | train 0.150504 | val 0.621650\n",
      "Epoch 025 | train 0.122850 | val 0.663204\n",
      "Early stopping at epoch 025 (best val 0.402311)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8203, macro_AUPRC_optthr=0.8762, macro_Brier_optthr=0.1622, macro_ACC_optthr=0.7294, macro_F1_optthr=0.7723, macro_Precision_optthr=0.8571, macro_Recall_optthr=0.7027, macro_Sensitivity_optthr=0.7027, macro_Specificity_optthr=0.7797\n",
      "\n",
      "[CV fold 2/3] scenario=cases\n",
      "[split] train | rows=  306 | unique_ids=   20\n",
      "[split] val   | rows=   33 | unique_ids=   18\n",
      "[split] test  | rows=  170 | unique_ids=   12\n",
      "Epoch 001 | train 0.519232 | val 0.468802\n",
      "Epoch 002 | train 0.499646 | val 0.437922\n",
      "Epoch 003 | train 0.470247 | val 0.401256\n",
      "Epoch 004 | train 0.426567 | val 0.384102\n",
      "Epoch 005 | train 0.384923 | val 0.421610\n",
      "Epoch 006 | train 0.354887 | val 0.471209\n",
      "Epoch 007 | train 0.353871 | val 0.461790\n",
      "Epoch 008 | train 0.334899 | val 0.411607\n",
      "Epoch 009 | train 0.320649 | val 0.382406\n",
      "Epoch 010 | train 0.315740 | val 0.377752\n",
      "Epoch 011 | train 0.304289 | val 0.392751\n",
      "Epoch 012 | train 0.292170 | val 0.400566\n",
      "Epoch 013 | train 0.286155 | val 0.412276\n",
      "Epoch 014 | train 0.278757 | val 0.425349\n",
      "Epoch 015 | train 0.267702 | val 0.455157\n",
      "Epoch 016 | train 0.256339 | val 0.488381\n",
      "Epoch 017 | train 0.246815 | val 0.531108\n",
      "Epoch 018 | train 0.240398 | val 0.578553\n",
      "Epoch 019 | train 0.232761 | val 0.579395\n",
      "Epoch 020 | train 0.219576 | val 0.571443\n",
      "Epoch 021 | train 0.211497 | val 0.600616\n",
      "Epoch 022 | train 0.201586 | val 0.615979\n",
      "Epoch 023 | train 0.190067 | val 0.613319\n",
      "Epoch 024 | train 0.178425 | val 0.597241\n",
      "Epoch 025 | train 0.169053 | val 0.663799\n",
      "Epoch 026 | train 0.154871 | val 0.752786\n",
      "Epoch 027 | train 0.147091 | val 0.782773\n",
      "Epoch 028 | train 0.143451 | val 0.755030\n",
      "Epoch 029 | train 0.132069 | val 0.759663\n",
      "Epoch 030 | train 0.121660 | val 0.827245\n",
      "Early stopping at epoch 030 (best val 0.377752)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7310, macro_AUPRC_optthr=0.7087, macro_Brier_optthr=0.2680, macro_ACC_optthr=0.6235, macro_F1_optthr=0.7265, macro_Precision_optthr=0.5822, macro_Recall_optthr=0.9659, macro_Sensitivity_optthr=0.9659, macro_Specificity_optthr=0.2561\n",
      "\n",
      "[CV fold 3/3] scenario=cases\n",
      "[split] train | rows=  306 | unique_ids=   22\n",
      "[split] val   | rows=   34 | unique_ids=   14\n",
      "[split] test  | rows=  169 | unique_ids=   11\n",
      "Epoch 001 | train 0.572309 | val 0.554772\n",
      "Epoch 002 | train 0.545955 | val 0.525066\n",
      "Epoch 003 | train 0.506778 | val 0.480594\n",
      "Epoch 004 | train 0.443188 | val 0.441218\n",
      "Epoch 005 | train 0.386381 | val 0.447855\n",
      "Epoch 006 | train 0.358774 | val 0.500638\n",
      "Epoch 007 | train 0.353364 | val 0.534950\n",
      "Epoch 008 | train 0.344325 | val 0.504350\n",
      "Epoch 009 | train 0.334289 | val 0.471620\n",
      "Epoch 010 | train 0.320151 | val 0.447752\n",
      "Epoch 011 | train 0.308455 | val 0.443824\n",
      "Epoch 012 | train 0.299786 | val 0.432086\n",
      "Epoch 013 | train 0.293844 | val 0.420458\n",
      "Epoch 014 | train 0.288130 | val 0.412816\n",
      "Epoch 015 | train 0.275739 | val 0.426058\n",
      "Epoch 016 | train 0.265798 | val 0.435079\n",
      "Epoch 017 | train 0.257247 | val 0.433581\n",
      "Epoch 018 | train 0.243823 | val 0.470928\n",
      "Epoch 019 | train 0.231010 | val 0.510541\n",
      "Epoch 020 | train 0.215061 | val 0.548713\n",
      "Epoch 021 | train 0.195406 | val 0.573259\n",
      "Epoch 022 | train 0.183584 | val 0.593860\n",
      "Epoch 023 | train 0.165663 | val 0.654109\n",
      "Epoch 024 | train 0.157685 | val 0.703007\n",
      "Epoch 025 | train 0.145923 | val 0.726648\n",
      "Epoch 026 | train 0.129814 | val 0.824595\n",
      "Epoch 027 | train 0.114265 | val 0.828268\n",
      "Epoch 028 | train 0.091400 | val 0.919611\n",
      "Epoch 029 | train 0.074432 | val 0.998588\n",
      "Epoch 030 | train 0.056538 | val 1.054306\n",
      "Epoch 031 | train 0.035847 | val 1.137390\n",
      "Epoch 032 | train 0.045556 | val 1.262739\n",
      "Epoch 033 | train 0.035217 | val 1.411282\n",
      "Epoch 034 | train 0.036173 | val 1.506595\n",
      "Early stopping at epoch 034 (best val 0.412816)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.6231, macro_AUPRC_optthr=0.7567, macro_Brier_optthr=0.2607, macro_ACC_optthr=0.5444, macro_F1_optthr=0.5333, macro_Precision_optthr=0.7333, macro_Recall_optthr=0.4190, macro_Sensitivity_optthr=0.4190, macro_Specificity_optthr=0.7500\n",
      "\n",
      "CV averages (95% CI) @0.5:\n",
      "           macro_ACC: 0.6639  (95% CI 0.3935, 0.9343)\n",
      "           macro_AUC: 0.7248  (95% CI 0.4795, 0.9701)\n",
      "         macro_AUPRC: 0.7805  (95% CI 0.5662, 0.9948)\n",
      "         macro_Brier: 0.2303  (95% CI 0.0835, 0.3771)\n",
      "            macro_F1: 0.7225  (95% CI 0.4408, 1.0043)\n",
      "     macro_Precision: 0.7172  (95% CI 0.3956, 1.0388)\n",
      "        macro_Recall: 0.7683  (95% CI 0.2263, 1.3102)\n",
      "   macro_Sensitivity: 0.7683  (95% CI 0.2263, 1.3102)\n",
      "   macro_Specificity: 0.5502  (95% CI -0.0564, 1.1569)\n",
      "          task_1_ACC: 0.6639  (95% CI 0.3935, 0.9343)\n",
      "          task_1_AUC: 0.7248  (95% CI 0.4795, 0.9701)\n",
      "        task_1_AUPRC: 0.7805  (95% CI 0.5662, 0.9948)\n",
      "        task_1_Brier: 0.2303  (95% CI 0.0835, 0.3771)\n",
      "           task_1_F1: 0.7225  (95% CI 0.4408, 1.0043)\n",
      "    task_1_Precision: 0.7172  (95% CI 0.3956, 1.0388)\n",
      "       task_1_Recall: 0.7683  (95% CI 0.2263, 1.3102)\n",
      "  task_1_Sensitivity: 0.7683  (95% CI 0.2263, 1.3102)\n",
      "  task_1_Specificity: 0.5502  (95% CI -0.0564, 1.1569)\n",
      "\n",
      "CV averages (95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.6324  (95% CI 0.4018, 0.8631)\n",
      "    macro_AUC_optthr: 0.7248  (95% CI 0.4795, 0.9701)\n",
      "  macro_AUPRC_optthr: 0.7805  (95% CI 0.5662, 0.9948)\n",
      "  macro_Brier_optthr: 0.2303  (95% CI 0.0835, 0.3771)\n",
      "     macro_F1_optthr: 0.6774  (95% CI 0.3623, 0.9924)\n",
      "macro_Precision_optthr: 0.7242  (95% CI 0.3822, 1.0663)\n",
      " macro_Recall_optthr: 0.6959  (95% CI 0.0165, 1.3753)\n",
      "macro_Sensitivity_optthr: 0.6959  (95% CI 0.0165, 1.3753)\n",
      "macro_Specificity_optthr: 0.5953  (95% CI -0.1353, 1.3258)\n",
      "   task_1_ACC_optthr: 0.6324  (95% CI 0.4018, 0.8631)\n",
      "   task_1_AUC_optthr: 0.7248  (95% CI 0.4795, 0.9701)\n",
      " task_1_AUPRC_optthr: 0.7805  (95% CI 0.5662, 0.9948)\n",
      " task_1_Brier_optthr: 0.2303  (95% CI 0.0835, 0.3771)\n",
      "    task_1_F1_optthr: 0.6774  (95% CI 0.3623, 0.9924)\n",
      "task_1_Precision_optthr: 0.7242  (95% CI 0.3822, 1.0663)\n",
      "task_1_Recall_optthr: 0.6959  (95% CI 0.0165, 1.3753)\n",
      "task_1_Sensitivity_optthr: 0.6959  (95% CI 0.0165, 1.3753)\n",
      "task_1_Specificity_optthr: 0.5953  (95% CI -0.1353, 1.3258)\n",
      "Arch params (used in all folds): {'d_fix_latent': 128, 'd_rand_latent': 128, 'n_kernels': 6, 'dropout': 0.0}\n",
      "Train params (used in all folds): {'lr': 0.001, 'weight_decay': 3e-05, 'batch_size': 128, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 1e-06, 'lambda_ridge': 1e-05, 'lambda_orth_latent': 3e-05, 'lambda_film_identity': 3e-05, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "arch_defaults = dict(d_fix_latent=128, d_rand_latent=128, n_kernels=6, dropout=0.00)\n",
    "train_defaults = dict(\n",
    "    lr=1e-3, weight_decay=3e-5, batch_size=128,\n",
    "    max_epochs=100, patience=20, threshold=0.5,\n",
    "    lambda_mean0=1e-6, lambda_ridge=1e-5,\n",
    "    lambda_orth_latent=3e-5,\n",
    "    lambda_film_identity=3e-5,\n",
    "    lambda_kan=0,\n",
    "    random_state=42,\n",
    ")\n",
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"cv_only\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"cases\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "140ec1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV fold 1/3] scenario=time\n",
      "[split] train | rows=  116 | unique_ids=   32\n",
      "[split] val   | rows=   12 | unique_ids=   10\n",
      "[split] test  | rows=  127 | unique_ids=   26\n",
      "Epoch 001 | train 0.382819 | val 0.403235\n",
      "Epoch 002 | train 0.378945 | val 0.397648\n",
      "Epoch 003 | train 0.374878 | val 0.390692\n",
      "Epoch 004 | train 0.369992 | val 0.382003\n",
      "Epoch 005 | train 0.363868 | val 0.371213\n",
      "Epoch 006 | train 0.356208 | val 0.358061\n",
      "Epoch 007 | train 0.346906 | val 0.342494\n",
      "Epoch 008 | train 0.336147 | val 0.324724\n",
      "Epoch 009 | train 0.324459 | val 0.305057\n",
      "Epoch 010 | train 0.312642 | val 0.283329\n",
      "Epoch 011 | train 0.301320 | val 0.258588\n",
      "Epoch 012 | train 0.290313 | val 0.231205\n",
      "Epoch 013 | train 0.279294 | val 0.204106\n",
      "Epoch 014 | train 0.268557 | val 0.181045\n",
      "Epoch 015 | train 0.258548 | val 0.165415\n",
      "Epoch 016 | train 0.249183 | val 0.160576\n",
      "Epoch 017 | train 0.239571 | val 0.170358\n",
      "Epoch 018 | train 0.229093 | val 0.199408\n",
      "Epoch 019 | train 0.218898 | val 0.242596\n",
      "Epoch 020 | train 0.211643 | val 0.249671\n",
      "Epoch 021 | train 0.203563 | val 0.214827\n",
      "Epoch 022 | train 0.191607 | val 0.166994\n",
      "Epoch 023 | train 0.179276 | val 0.132912\n",
      "Epoch 024 | train 0.167687 | val 0.114957\n",
      "Epoch 025 | train 0.155396 | val 0.108450\n",
      "Epoch 026 | train 0.143095 | val 0.108905\n",
      "Epoch 027 | train 0.131836 | val 0.109074\n",
      "Epoch 028 | train 0.120853 | val 0.102568\n",
      "Epoch 029 | train 0.109745 | val 0.091290\n",
      "Epoch 030 | train 0.099218 | val 0.081811\n",
      "Epoch 031 | train 0.088122 | val 0.076202\n",
      "Epoch 032 | train 0.076216 | val 0.069865\n",
      "Epoch 033 | train 0.065135 | val 0.060294\n",
      "Epoch 034 | train 0.055550 | val 0.054895\n",
      "Epoch 035 | train 0.046882 | val 0.064991\n",
      "Epoch 036 | train 0.038895 | val 0.095513\n",
      "Epoch 037 | train 0.031854 | val 0.092459\n",
      "Epoch 038 | train 0.024873 | val 0.078230\n",
      "Epoch 039 | train 0.019450 | val 0.100913\n",
      "Epoch 040 | train 0.014673 | val 0.141738\n",
      "Epoch 041 | train 0.011233 | val 0.121877\n",
      "Epoch 042 | train 0.007876 | val 0.104175\n",
      "Epoch 043 | train 0.005590 | val 0.123459\n",
      "Epoch 044 | train 0.003775 | val 0.150692\n",
      "Epoch 045 | train 0.002678 | val 0.151478\n",
      "Epoch 046 | train 0.001877 | val 0.127947\n",
      "Epoch 047 | train 0.001287 | val 0.101979\n",
      "Epoch 048 | train 0.000948 | val 0.089202\n",
      "Epoch 049 | train 0.000732 | val 0.092963\n",
      "Epoch 050 | train 0.000561 | val 0.110089\n",
      "Epoch 051 | train 0.000433 | val 0.135490\n",
      "Epoch 052 | train 0.000346 | val 0.163817\n",
      "Epoch 053 | train 0.000289 | val 0.190633\n",
      "Epoch 054 | train 0.000250 | val 0.213100\n",
      "Early stopping at epoch 054 (best val 0.054895)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8114, macro_AUPRC_optthr=0.8655, macro_Brier_optthr=0.2122, macro_ACC_optthr=0.7323, macro_F1_optthr=0.7703, macro_Precision_optthr=0.8028, macro_Recall_optthr=0.7403, macro_Sensitivity_optthr=0.7403, macro_Specificity_optthr=0.7200\n",
      "\n",
      "[CV fold 2/3] scenario=time\n",
      "[split] train | rows=  230 | unique_ids=   31\n",
      "[split] val   | rows=   25 | unique_ids=   17\n",
      "[split] test  | rows=  127 | unique_ids=   23\n",
      "Epoch 001 | train 0.461429 | val 0.461267\n",
      "Epoch 002 | train 0.449751 | val 0.447235\n",
      "Epoch 003 | train 0.434472 | val 0.430195\n",
      "Epoch 004 | train 0.413235 | val 0.413405\n",
      "Epoch 005 | train 0.384734 | val 0.407537\n",
      "Epoch 006 | train 0.362363 | val 0.421613\n",
      "Epoch 007 | train 0.336929 | val 0.455690\n",
      "Epoch 008 | train 0.318569 | val 0.509672\n",
      "Epoch 009 | train 0.304914 | val 0.544407\n",
      "Epoch 010 | train 0.294209 | val 0.556054\n",
      "Epoch 011 | train 0.273488 | val 0.558576\n",
      "Epoch 012 | train 0.258789 | val 0.546466\n",
      "Epoch 013 | train 0.241166 | val 0.540534\n",
      "Epoch 014 | train 0.225373 | val 0.565070\n",
      "Epoch 015 | train 0.215061 | val 0.609206\n",
      "Epoch 016 | train 0.200839 | val 0.682855\n",
      "Epoch 017 | train 0.189991 | val 0.761388\n",
      "Epoch 018 | train 0.179413 | val 0.826153\n",
      "Epoch 019 | train 0.165546 | val 0.947422\n",
      "Epoch 020 | train 0.154552 | val 1.062919\n",
      "Epoch 021 | train 0.146574 | val 1.169417\n",
      "Epoch 022 | train 0.134569 | val 1.233840\n",
      "Epoch 023 | train 0.125556 | val 1.339479\n",
      "Epoch 024 | train 0.107532 | val 1.447436\n",
      "Epoch 025 | train 0.097199 | val 1.587979\n",
      "Early stopping at epoch 025 (best val 0.407537)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7985, macro_AUPRC_optthr=0.7855, macro_Brier_optthr=0.1877, macro_ACC_optthr=0.7165, macro_F1_optthr=0.7353, macro_Precision_optthr=0.7692, macro_Recall_optthr=0.7042, macro_Sensitivity_optthr=0.7042, macro_Specificity_optthr=0.7321\n",
      "\n",
      "[CV fold 3/3] scenario=time\n",
      "[split] train | rows=  344 | unique_ids=   33\n",
      "[split] val   | rows=   38 | unique_ids=   18\n",
      "[split] test  | rows=  127 | unique_ids=   18\n",
      "Epoch 001 | train 0.496447 | val 0.524977\n",
      "Epoch 002 | train 0.475798 | val 0.493476\n",
      "Epoch 003 | train 0.442858 | val 0.450585\n",
      "Epoch 004 | train 0.400284 | val 0.409794\n",
      "Epoch 005 | train 0.367566 | val 0.384856\n",
      "Epoch 006 | train 0.350710 | val 0.382116\n",
      "Epoch 007 | train 0.339727 | val 0.354152\n",
      "Epoch 008 | train 0.319067 | val 0.330717\n",
      "Epoch 009 | train 0.304200 | val 0.321352\n",
      "Epoch 010 | train 0.292341 | val 0.330300\n",
      "Epoch 011 | train 0.278421 | val 0.361079\n",
      "Epoch 012 | train 0.270590 | val 0.374999\n",
      "Epoch 013 | train 0.253237 | val 0.363599\n",
      "Epoch 014 | train 0.242391 | val 0.380398\n",
      "Epoch 015 | train 0.226363 | val 0.452316\n",
      "Epoch 016 | train 0.216060 | val 0.470911\n",
      "Epoch 017 | train 0.196806 | val 0.472358\n",
      "Epoch 018 | train 0.179198 | val 0.481040\n",
      "Epoch 019 | train 0.166529 | val 0.558098\n",
      "Epoch 020 | train 0.143784 | val 0.684934\n",
      "Epoch 021 | train 0.129765 | val 0.712266\n",
      "Epoch 022 | train 0.114892 | val 0.806910\n",
      "Epoch 023 | train 0.102760 | val 0.909778\n",
      "Epoch 024 | train 0.081371 | val 0.999612\n",
      "Epoch 025 | train 0.063868 | val 1.183476\n",
      "Epoch 026 | train 0.057413 | val 1.101067\n",
      "Epoch 027 | train 0.041123 | val 1.268508\n",
      "Epoch 028 | train 0.049212 | val 1.280899\n",
      "Epoch 029 | train 0.036231 | val 1.372505\n",
      "Early stopping at epoch 029 (best val 0.321352)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8534, macro_AUPRC_optthr=0.8846, macro_Brier_optthr=0.1673, macro_ACC_optthr=0.7717, macro_F1_optthr=0.7603, macro_Precision_optthr=0.8070, macro_Recall_optthr=0.7188, macro_Sensitivity_optthr=0.7188, macro_Specificity_optthr=0.8254\n",
      "\n",
      "CV averages (95% CI) @0.5:\n",
      "           macro_ACC: 0.7559  (95% CI 0.7168, 0.7950)\n",
      "           macro_AUC: 0.8211  (95% CI 0.7498, 0.8924)\n",
      "         macro_AUPRC: 0.8452  (95% CI 0.7146, 0.9759)\n",
      "         macro_Brier: 0.1891  (95% CI 0.1333, 0.2448)\n",
      "            macro_F1: 0.7566  (95% CI 0.6860, 0.8273)\n",
      "     macro_Precision: 0.8556  (95% CI 0.6362, 1.0751)\n",
      "        macro_Recall: 0.6903  (95% CI 0.4480, 0.9325)\n",
      "   macro_Sensitivity: 0.6903  (95% CI 0.4480, 0.9325)\n",
      "   macro_Specificity: 0.8303  (95% CI 0.4843, 1.1764)\n",
      "          task_1_ACC: 0.7559  (95% CI 0.7168, 0.7950)\n",
      "          task_1_AUC: 0.8211  (95% CI 0.7498, 0.8924)\n",
      "        task_1_AUPRC: 0.8452  (95% CI 0.7146, 0.9759)\n",
      "        task_1_Brier: 0.1891  (95% CI 0.1333, 0.2448)\n",
      "           task_1_F1: 0.7566  (95% CI 0.6860, 0.8273)\n",
      "    task_1_Precision: 0.8556  (95% CI 0.6362, 1.0751)\n",
      "       task_1_Recall: 0.6903  (95% CI 0.4480, 0.9325)\n",
      "  task_1_Sensitivity: 0.6903  (95% CI 0.4480, 0.9325)\n",
      "  task_1_Specificity: 0.8303  (95% CI 0.4843, 1.1764)\n",
      "\n",
      "CV averages (95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.7402  (95% CI 0.6696, 0.8107)\n",
      "    macro_AUC_optthr: 0.8211  (95% CI 0.7498, 0.8924)\n",
      "  macro_AUPRC_optthr: 0.8452  (95% CI 0.7146, 0.9759)\n",
      "  macro_Brier_optthr: 0.1891  (95% CI 0.1333, 0.2448)\n",
      "     macro_F1_optthr: 0.7553  (95% CI 0.7105, 0.8001)\n",
      "macro_Precision_optthr: 0.7930  (95% CI 0.7416, 0.8445)\n",
      " macro_Recall_optthr: 0.7211  (95% CI 0.6760, 0.7661)\n",
      "macro_Sensitivity_optthr: 0.7211  (95% CI 0.6760, 0.7661)\n",
      "macro_Specificity_optthr: 0.7592  (95% CI 0.6159, 0.9024)\n",
      "   task_1_ACC_optthr: 0.7402  (95% CI 0.6696, 0.8107)\n",
      "   task_1_AUC_optthr: 0.8211  (95% CI 0.7498, 0.8924)\n",
      " task_1_AUPRC_optthr: 0.8452  (95% CI 0.7146, 0.9759)\n",
      " task_1_Brier_optthr: 0.1891  (95% CI 0.1333, 0.2448)\n",
      "    task_1_F1_optthr: 0.7553  (95% CI 0.7105, 0.8001)\n",
      "task_1_Precision_optthr: 0.7930  (95% CI 0.7416, 0.8445)\n",
      "task_1_Recall_optthr: 0.7211  (95% CI 0.6760, 0.7661)\n",
      "task_1_Sensitivity_optthr: 0.7211  (95% CI 0.6760, 0.7661)\n",
      "task_1_Specificity_optthr: 0.7592  (95% CI 0.6159, 0.9024)\n",
      "Arch params (used in all folds): {'d_fix_latent': 128, 'd_rand_latent': 128, 'n_kernels': 6, 'dropout': 0.0}\n",
      "Train params (used in all folds): {'lr': 0.001, 'weight_decay': 3e-05, 'batch_size': 128, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 1e-06, 'lambda_ridge': 1e-05, 'lambda_orth_latent': 3e-05, 'lambda_film_identity': 3e-05, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"cv_only\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"time\",        # or \"cases\"\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b9c99cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV fold 1/5] scenario=cases\n",
      "[split] train | rows=  366 | unique_ids=   27\n",
      "[split] val   | rows=   40 | unique_ids=   15\n",
      "[split] test  | rows=  103 | unique_ids=    6\n",
      "Epoch 001 | train 0.532014 | val 0.567292\n",
      "Epoch 002 | train 0.518756 | val 0.555196\n",
      "Epoch 003 | train 0.486833 | val 0.524333\n",
      "Epoch 004 | train 0.432576 | val 0.476921\n",
      "Epoch 005 | train 0.374326 | val 0.394476\n",
      "Epoch 006 | train 0.334793 | val 0.330582\n",
      "Epoch 007 | train 0.314220 | val 0.334133\n",
      "Epoch 008 | train 0.307585 | val 0.357299\n",
      "Epoch 009 | train 0.301506 | val 0.392293\n",
      "Epoch 010 | train 0.283565 | val 0.434847\n",
      "Epoch 011 | train 0.281098 | val 0.424777\n",
      "Epoch 012 | train 0.274181 | val 0.396069\n",
      "Epoch 013 | train 0.267647 | val 0.400164\n",
      "Epoch 014 | train 0.259456 | val 0.404594\n",
      "Epoch 015 | train 0.254048 | val 0.400163\n",
      "Epoch 016 | train 0.246458 | val 0.405989\n",
      "Epoch 017 | train 0.237165 | val 0.426798\n",
      "Epoch 018 | train 0.228234 | val 0.441263\n",
      "Epoch 019 | train 0.220082 | val 0.439643\n",
      "Epoch 020 | train 0.207932 | val 0.455120\n",
      "Epoch 021 | train 0.194164 | val 0.481099\n",
      "Epoch 022 | train 0.180730 | val 0.510244\n",
      "Epoch 023 | train 0.167049 | val 0.552530\n",
      "Epoch 024 | train 0.153780 | val 0.581774\n",
      "Epoch 025 | train 0.143290 | val 0.643454\n",
      "Epoch 026 | train 0.127559 | val 0.648446\n",
      "Early stopping at epoch 026 (best val 0.330582)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8564, macro_AUPRC_optthr=0.9265, macro_Brier_optthr=0.1821, macro_ACC_optthr=0.8058, macro_F1_optthr=0.8214, macro_Precision_optthr=0.9020, macro_Recall_optthr=0.7541, macro_Sensitivity_optthr=0.7541, macro_Specificity_optthr=0.8810\n",
      "\n",
      "[CV fold 2/5] scenario=cases\n",
      "[split] train | rows=  367 | unique_ids=   26\n",
      "[split] val   | rows=   40 | unique_ids=   18\n",
      "[split] test  | rows=  102 | unique_ids=    7\n",
      "Epoch 001 | train 0.489396 | val 0.563537\n",
      "Epoch 002 | train 0.476278 | val 0.561110\n",
      "Epoch 003 | train 0.443847 | val 0.562119\n",
      "Epoch 004 | train 0.403191 | val 0.563212\n",
      "Epoch 005 | train 0.350987 | val 0.487828\n",
      "Epoch 006 | train 0.303803 | val 0.369422\n",
      "Epoch 007 | train 0.294166 | val 0.384325\n",
      "Epoch 008 | train 0.273485 | val 0.442835\n",
      "Epoch 009 | train 0.259537 | val 0.469077\n",
      "Epoch 010 | train 0.253019 | val 0.414794\n",
      "Epoch 011 | train 0.238885 | val 0.325470\n",
      "Epoch 012 | train 0.240259 | val 0.291116\n",
      "Epoch 013 | train 0.229590 | val 0.325801\n",
      "Epoch 014 | train 0.222620 | val 0.341849\n",
      "Epoch 015 | train 0.215899 | val 0.332174\n",
      "Epoch 016 | train 0.208816 | val 0.373599\n",
      "Epoch 017 | train 0.202726 | val 0.402219\n",
      "Epoch 018 | train 0.196570 | val 0.406538\n",
      "Epoch 019 | train 0.188503 | val 0.322264\n",
      "Epoch 020 | train 0.179851 | val 0.364916\n",
      "Epoch 021 | train 0.168332 | val 0.421704\n",
      "Epoch 022 | train 0.162292 | val 0.497232\n",
      "Epoch 023 | train 0.147071 | val 0.425859\n",
      "Epoch 024 | train 0.135286 | val 0.485642\n",
      "Epoch 025 | train 0.122091 | val 0.570397\n",
      "Epoch 026 | train 0.110674 | val 0.687528\n",
      "Epoch 027 | train 0.096682 | val 0.644661\n",
      "Epoch 028 | train 0.082417 | val 0.964880\n",
      "Epoch 029 | train 0.064387 | val 0.883650\n",
      "Epoch 030 | train 0.052737 | val 1.043967\n",
      "Epoch 031 | train 0.036462 | val 1.301712\n",
      "Epoch 032 | train 0.028821 | val 1.192683\n",
      "Early stopping at epoch 032 (best val 0.291116)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7619, macro_AUPRC_optthr=0.7445, macro_Brier_optthr=0.2169, macro_ACC_optthr=0.7157, macro_F1_optthr=0.7563, macro_Precision_optthr=0.6522, macro_Recall_optthr=0.9000, macro_Sensitivity_optthr=0.9000, macro_Specificity_optthr=0.5385\n",
      "\n",
      "[CV fold 3/5] scenario=cases\n",
      "[split] train | rows=  367 | unique_ids=   26\n",
      "[split] val   | rows=   40 | unique_ids=   20\n",
      "[split] test  | rows=  102 | unique_ids=    7\n",
      "Epoch 001 | train 0.628772 | val 0.614294\n",
      "Epoch 002 | train 0.613579 | val 0.581813\n",
      "Epoch 003 | train 0.577820 | val 0.514394\n",
      "Epoch 004 | train 0.514362 | val 0.422868\n",
      "Epoch 005 | train 0.443987 | val 0.352043\n",
      "Epoch 006 | train 0.416496 | val 0.322936\n",
      "Epoch 007 | train 0.403849 | val 0.318021\n",
      "Epoch 008 | train 0.388610 | val 0.323927\n",
      "Epoch 009 | train 0.372679 | val 0.336317\n",
      "Epoch 010 | train 0.365729 | val 0.346532\n",
      "Epoch 011 | train 0.355810 | val 0.340780\n",
      "Epoch 012 | train 0.347257 | val 0.334621\n",
      "Epoch 013 | train 0.337789 | val 0.321688\n",
      "Epoch 014 | train 0.326555 | val 0.317596\n",
      "Epoch 015 | train 0.317459 | val 0.312995\n",
      "Epoch 016 | train 0.306541 | val 0.295531\n",
      "Epoch 017 | train 0.295764 | val 0.295230\n",
      "Epoch 018 | train 0.281376 | val 0.306821\n",
      "Epoch 019 | train 0.265434 | val 0.291280\n",
      "Epoch 020 | train 0.248877 | val 0.279071\n",
      "Epoch 021 | train 0.234427 | val 0.275802\n",
      "Epoch 022 | train 0.211173 | val 0.256869\n",
      "Epoch 023 | train 0.196060 | val 0.245602\n",
      "Epoch 024 | train 0.172971 | val 0.228381\n",
      "Epoch 025 | train 0.149541 | val 0.242161\n",
      "Epoch 026 | train 0.124890 | val 0.224170\n",
      "Epoch 027 | train 0.104131 | val 0.235753\n",
      "Epoch 028 | train 0.071385 | val 0.286075\n",
      "Epoch 029 | train 0.049920 | val 0.296281\n",
      "Epoch 030 | train 0.033938 | val 0.266982\n",
      "Epoch 031 | train 0.019790 | val 0.328751\n",
      "Epoch 032 | train 0.014528 | val 0.361310\n",
      "Epoch 033 | train 0.006944 | val 0.373722\n",
      "Epoch 034 | train 0.009141 | val 0.499978\n",
      "Epoch 035 | train 0.029091 | val 0.305173\n",
      "Epoch 036 | train 0.097144 | val 0.399907\n",
      "Epoch 037 | train 0.038879 | val 0.713916\n",
      "Epoch 038 | train 0.032231 | val 0.527038\n",
      "Epoch 039 | train 0.028034 | val 0.428814\n",
      "Epoch 040 | train 0.007238 | val 0.524798\n",
      "Epoch 041 | train 0.020985 | val 0.483267\n",
      "Epoch 042 | train 0.004786 | val 0.420118\n",
      "Epoch 043 | train 0.002381 | val 0.351506\n",
      "Epoch 044 | train 0.006408 | val 0.382862\n",
      "Epoch 045 | train 0.004019 | val 0.426037\n",
      "Epoch 046 | train 0.001995 | val 0.404806\n",
      "Early stopping at epoch 046 (best val 0.224170)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.1733, macro_AUPRC_optthr=0.6422, macro_Brier_optthr=0.4896, macro_ACC_optthr=0.4412, macro_F1_optthr=0.6069, macro_Precision_optthr=0.6769, macro_Recall_optthr=0.5500, macro_Sensitivity_optthr=0.5500, macro_Specificity_optthr=0.0455\n",
      "\n",
      "[CV fold 4/5] scenario=cases\n",
      "[split] train | rows=  368 | unique_ids=   25\n",
      "[split] val   | rows=   40 | unique_ids=   20\n",
      "[split] test  | rows=  101 | unique_ids=    6\n",
      "Epoch 001 | train 0.498600 | val 0.498000\n",
      "Epoch 002 | train 0.484634 | val 0.481493\n",
      "Epoch 003 | train 0.449885 | val 0.447841\n",
      "Epoch 004 | train 0.394232 | val 0.416925\n",
      "Epoch 005 | train 0.336941 | val 0.438035\n",
      "Epoch 006 | train 0.311545 | val 0.482913\n",
      "Epoch 007 | train 0.300813 | val 0.510386\n",
      "Epoch 008 | train 0.283264 | val 0.491132\n",
      "Epoch 009 | train 0.274385 | val 0.471918\n",
      "Epoch 010 | train 0.261822 | val 0.443833\n",
      "Epoch 011 | train 0.254751 | val 0.429240\n",
      "Epoch 012 | train 0.247420 | val 0.434437\n",
      "Epoch 013 | train 0.237650 | val 0.452911\n",
      "Epoch 014 | train 0.228799 | val 0.481539\n",
      "Epoch 015 | train 0.220882 | val 0.494620\n",
      "Epoch 016 | train 0.210461 | val 0.511194\n",
      "Epoch 017 | train 0.197677 | val 0.522599\n",
      "Epoch 018 | train 0.190727 | val 0.563846\n",
      "Epoch 019 | train 0.180279 | val 0.649446\n",
      "Epoch 020 | train 0.164226 | val 0.658068\n",
      "Epoch 021 | train 0.151375 | val 0.707721\n",
      "Epoch 022 | train 0.131063 | val 0.801792\n",
      "Epoch 023 | train 0.118630 | val 0.865190\n",
      "Epoch 024 | train 0.102459 | val 0.924768\n",
      "Early stopping at epoch 024 (best val 0.416925)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.4107, macro_AUPRC_optthr=0.3881, macro_Brier_optthr=0.2831, macro_ACC_optthr=0.4158, macro_F1_optthr=0.5874, macro_Precision_optthr=0.4242, macro_Recall_optthr=0.9545, macro_Sensitivity_optthr=0.9545, macro_Specificity_optthr=0.0000\n",
      "\n",
      "[CV fold 5/5] scenario=cases\n",
      "[split] train | rows=  368 | unique_ids=   26\n",
      "[split] val   | rows=   40 | unique_ids=   17\n",
      "[split] test  | rows=  101 | unique_ids=    7\n",
      "Epoch 001 | train 0.582451 | val 0.583653\n",
      "Epoch 002 | train 0.571627 | val 0.567695\n",
      "Epoch 003 | train 0.545158 | val 0.533587\n",
      "Epoch 004 | train 0.490182 | val 0.488255\n",
      "Epoch 005 | train 0.431777 | val 0.468810\n",
      "Epoch 006 | train 0.389027 | val 0.504197\n",
      "Epoch 007 | train 0.363641 | val 0.549954\n",
      "Epoch 008 | train 0.356821 | val 0.565810\n",
      "Epoch 009 | train 0.342064 | val 0.567848\n",
      "Epoch 010 | train 0.330089 | val 0.559596\n",
      "Epoch 011 | train 0.325939 | val 0.546221\n",
      "Epoch 012 | train 0.314156 | val 0.564779\n",
      "Epoch 013 | train 0.301264 | val 0.576005\n",
      "Epoch 014 | train 0.290598 | val 0.599061\n",
      "Epoch 015 | train 0.277291 | val 0.643146\n",
      "Epoch 016 | train 0.261839 | val 0.707868\n",
      "Epoch 017 | train 0.251144 | val 0.758860\n",
      "Epoch 018 | train 0.236019 | val 0.738842\n",
      "Epoch 019 | train 0.226219 | val 0.838740\n",
      "Epoch 020 | train 0.214174 | val 0.904455\n",
      "Epoch 021 | train 0.187936 | val 1.037740\n",
      "Epoch 022 | train 0.177606 | val 1.053730\n",
      "Epoch 023 | train 0.156952 | val 1.237872\n",
      "Epoch 024 | train 0.141545 | val 1.380495\n",
      "Epoch 025 | train 0.130645 | val 1.482992\n",
      "Early stopping at epoch 025 (best val 0.468810)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8111, macro_AUPRC_optthr=0.9059, macro_Brier_optthr=0.1989, macro_ACC_optthr=0.6733, macro_F1_optthr=0.7481, macro_Precision_optthr=0.7903, macro_Recall_optthr=0.7101, macro_Sensitivity_optthr=0.7101, macro_Specificity_optthr=0.5938\n",
      "\n",
      "CV averages (95% CI) @0.5:\n",
      "           macro_ACC: 0.5909  (95% CI 0.3920, 0.7899)\n",
      "           macro_AUC: 0.6027  (95% CI 0.2331, 0.9723)\n",
      "         macro_AUPRC: 0.7214  (95% CI 0.4480, 0.9949)\n",
      "         macro_Brier: 0.2741  (95% CI 0.1172, 0.4311)\n",
      "            macro_F1: 0.5898  (95% CI 0.3362, 0.8435)\n",
      "     macro_Precision: 0.6913  (95% CI 0.3893, 0.9933)\n",
      "        macro_Recall: 0.5255  (95% CI 0.2717, 0.7793)\n",
      "   macro_Sensitivity: 0.5255  (95% CI 0.2717, 0.7793)\n",
      "   macro_Specificity: 0.6094  (95% CI 0.1778, 1.0410)\n",
      "          task_1_ACC: 0.5909  (95% CI 0.3920, 0.7899)\n",
      "          task_1_AUC: 0.6027  (95% CI 0.2331, 0.9723)\n",
      "        task_1_AUPRC: 0.7214  (95% CI 0.4480, 0.9949)\n",
      "        task_1_Brier: 0.2741  (95% CI 0.1172, 0.4311)\n",
      "           task_1_F1: 0.5898  (95% CI 0.3362, 0.8435)\n",
      "    task_1_Precision: 0.6913  (95% CI 0.3893, 0.9933)\n",
      "       task_1_Recall: 0.5255  (95% CI 0.2717, 0.7793)\n",
      "  task_1_Sensitivity: 0.5255  (95% CI 0.2717, 0.7793)\n",
      "  task_1_Specificity: 0.6094  (95% CI 0.1778, 1.0410)\n",
      "\n",
      "CV averages (95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.6104  (95% CI 0.3955, 0.8252)\n",
      "    macro_AUC_optthr: 0.6027  (95% CI 0.2331, 0.9723)\n",
      "  macro_AUPRC_optthr: 0.7214  (95% CI 0.4480, 0.9949)\n",
      "  macro_Brier_optthr: 0.2741  (95% CI 0.1172, 0.4311)\n",
      "     macro_F1_optthr: 0.7040  (95% CI 0.5776, 0.8305)\n",
      "macro_Precision_optthr: 0.6891  (95% CI 0.4677, 0.9106)\n",
      " macro_Recall_optthr: 0.7738  (95% CI 0.5744, 0.9731)\n",
      "macro_Sensitivity_optthr: 0.7738  (95% CI 0.5744, 0.9731)\n",
      "macro_Specificity_optthr: 0.4117  (95% CI -0.0582, 0.8817)\n",
      "   task_1_ACC_optthr: 0.6104  (95% CI 0.3955, 0.8252)\n",
      "   task_1_AUC_optthr: 0.6027  (95% CI 0.2331, 0.9723)\n",
      " task_1_AUPRC_optthr: 0.7214  (95% CI 0.4480, 0.9949)\n",
      " task_1_Brier_optthr: 0.2741  (95% CI 0.1172, 0.4311)\n",
      "    task_1_F1_optthr: 0.7040  (95% CI 0.5776, 0.8305)\n",
      "task_1_Precision_optthr: 0.6891  (95% CI 0.4677, 0.9106)\n",
      "task_1_Recall_optthr: 0.7738  (95% CI 0.5744, 0.9731)\n",
      "task_1_Sensitivity_optthr: 0.7738  (95% CI 0.5744, 0.9731)\n",
      "task_1_Specificity_optthr: 0.4117  (95% CI -0.0582, 0.8817)\n",
      "Arch params (used in all folds): {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0, 'use_attention': False, 'use_random': False}\n",
      "Train params (used in all folds): {'lr': 0.001, 'weight_decay': 3e-05, 'batch_size': 128, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 1e-06, 'lambda_ridge': 1e-05, 'lambda_orth_latent': 3e-05, 'lambda_film_identity': 3e-05, 'lambda_kan': 0, 'random_state': 42}\n",
      "\n",
      "[CV fold 1/5] scenario=time\n",
      "[split] train | rows=   81 | unique_ids=   31\n",
      "[split] val   | rows=    8 | unique_ids=    7\n",
      "[split] test  | rows=   84 | unique_ids=   28\n",
      "Epoch 001 | train 0.308081 | val 0.382413\n",
      "Epoch 002 | train 0.306390 | val 0.381254\n",
      "Epoch 003 | train 0.304284 | val 0.379412\n",
      "Epoch 004 | train 0.301198 | val 0.376841\n",
      "Epoch 005 | train 0.296716 | val 0.373697\n",
      "Epoch 006 | train 0.290455 | val 0.370481\n",
      "Epoch 007 | train 0.282076 | val 0.368199\n",
      "Epoch 008 | train 0.271363 | val 0.368421\n",
      "Epoch 009 | train 0.258283 | val 0.373108\n",
      "Epoch 010 | train 0.242986 | val 0.384624\n",
      "Epoch 011 | train 0.225804 | val 0.405774\n",
      "Epoch 012 | train 0.207284 | val 0.439276\n",
      "Epoch 013 | train 0.188222 | val 0.487197\n",
      "Epoch 014 | train 0.169585 | val 0.550287\n",
      "Epoch 015 | train 0.152193 | val 0.627642\n",
      "Epoch 016 | train 0.136428 | val 0.716520\n",
      "Epoch 017 | train 0.122255 | val 0.813132\n",
      "Epoch 018 | train 0.109570 | val 0.916693\n",
      "Epoch 019 | train 0.098561 | val 1.033785\n",
      "Epoch 020 | train 0.088971 | val 1.167743\n",
      "Epoch 021 | train 0.079763 | val 1.316743\n",
      "Epoch 022 | train 0.070732 | val 1.482981\n",
      "Epoch 023 | train 0.062937 | val 1.664145\n",
      "Epoch 024 | train 0.055392 | val 1.852845\n",
      "Epoch 025 | train 0.047435 | val 2.028636\n",
      "Epoch 026 | train 0.040306 | val 2.171114\n",
      "Epoch 027 | train 0.035165 | val 2.287463\n",
      "Early stopping at epoch 027 (best val 0.368199)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.7392, macro_AUPRC_optthr=0.7513, macro_Brier_optthr=0.2240, macro_ACC_optthr=0.7262, macro_F1_optthr=0.7965, macro_Precision_optthr=0.7258, macro_Recall_optthr=0.8824, macro_Sensitivity_optthr=0.8824, macro_Specificity_optthr=0.4848\n",
      "\n",
      "[CV fold 2/5] scenario=time\n",
      "[split] train | rows=  156 | unique_ids=   33\n",
      "[split] val   | rows=   17 | unique_ids=   14\n",
      "[split] test  | rows=   84 | unique_ids=   24\n",
      "Epoch 001 | train 0.390697 | val 0.524142\n",
      "Epoch 002 | train 0.386788 | val 0.529826\n",
      "Epoch 003 | train 0.379736 | val 0.547998\n",
      "Epoch 004 | train 0.366647 | val 0.577806\n",
      "Epoch 005 | train 0.347297 | val 0.621060\n",
      "Epoch 006 | train 0.318697 | val 0.687169\n",
      "Epoch 007 | train 0.290746 | val 0.794223\n",
      "Epoch 008 | train 0.262952 | val 0.879772\n",
      "Epoch 009 | train 0.241946 | val 0.940019\n",
      "Epoch 010 | train 0.226484 | val 1.035903\n",
      "Epoch 011 | train 0.216012 | val 1.129375\n",
      "Epoch 012 | train 0.202334 | val 1.197293\n",
      "Epoch 013 | train 0.187091 | val 1.219563\n",
      "Epoch 014 | train 0.175850 | val 1.156795\n",
      "Epoch 015 | train 0.167403 | val 1.024884\n",
      "Epoch 016 | train 0.166480 | val 1.038070\n",
      "Epoch 017 | train 0.156709 | val 1.169056\n",
      "Epoch 018 | train 0.141771 | val 1.321075\n",
      "Epoch 019 | train 0.131574 | val 1.524496\n",
      "Epoch 020 | train 0.129819 | val 1.656048\n",
      "Epoch 021 | train 0.124724 | val 1.595532\n",
      "Early stopping at epoch 021 (best val 0.524142)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8378, macro_AUPRC_optthr=0.9040, macro_Brier_optthr=0.2454, macro_ACC_optthr=0.6429, macro_F1_optthr=0.7619, macro_Precision_optthr=0.6400, macro_Recall_optthr=0.9412, macro_Sensitivity_optthr=0.9412, macro_Specificity_optthr=0.1818\n",
      "\n",
      "[CV fold 3/5] scenario=time\n",
      "[split] train | rows=  232 | unique_ids=   33\n",
      "[split] val   | rows=   25 | unique_ids=   16\n",
      "[split] test  | rows=   84 | unique_ids=   22\n",
      "Epoch 001 | train 0.441159 | val 0.514672\n",
      "Epoch 002 | train 0.435758 | val 0.512174\n",
      "Epoch 003 | train 0.425597 | val 0.509419\n",
      "Epoch 004 | train 0.408234 | val 0.505078\n",
      "Epoch 005 | train 0.379078 | val 0.497699\n",
      "Epoch 006 | train 0.341622 | val 0.495177\n",
      "Epoch 007 | train 0.301510 | val 0.487104\n",
      "Epoch 008 | train 0.272803 | val 0.495508\n",
      "Epoch 009 | train 0.257246 | val 0.514716\n",
      "Epoch 010 | train 0.250071 | val 0.513724\n",
      "Epoch 011 | train 0.241512 | val 0.490827\n",
      "Epoch 012 | train 0.228349 | val 0.472204\n",
      "Epoch 013 | train 0.221201 | val 0.456383\n",
      "Epoch 014 | train 0.210952 | val 0.442827\n",
      "Epoch 015 | train 0.206736 | val 0.426093\n",
      "Epoch 016 | train 0.197154 | val 0.421365\n",
      "Epoch 017 | train 0.191152 | val 0.424162\n",
      "Epoch 018 | train 0.183339 | val 0.440068\n",
      "Epoch 019 | train 0.176642 | val 0.455491\n",
      "Epoch 020 | train 0.170861 | val 0.462295\n",
      "Epoch 021 | train 0.163158 | val 0.464569\n",
      "Epoch 022 | train 0.154846 | val 0.473996\n",
      "Epoch 023 | train 0.144917 | val 0.463925\n",
      "Epoch 024 | train 0.133798 | val 0.454622\n",
      "Epoch 025 | train 0.123237 | val 0.450921\n",
      "Epoch 026 | train 0.114091 | val 0.478322\n",
      "Epoch 027 | train 0.098341 | val 0.525051\n",
      "Epoch 028 | train 0.084508 | val 0.561966\n",
      "Epoch 029 | train 0.074731 | val 0.594020\n",
      "Epoch 030 | train 0.063156 | val 0.609953\n",
      "Epoch 031 | train 0.052268 | val 0.631639\n",
      "Epoch 032 | train 0.041264 | val 0.653105\n",
      "Epoch 033 | train 0.036179 | val 0.663755\n",
      "Epoch 034 | train 0.025537 | val 0.701030\n",
      "Epoch 035 | train 0.016708 | val 0.729001\n",
      "Epoch 036 | train 0.017186 | val 0.750405\n",
      "Early stopping at epoch 036 (best val 0.421365)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8431, macro_AUPRC_optthr=0.9018, macro_Brier_optthr=0.1838, macro_ACC_optthr=0.7500, macro_F1_optthr=0.7961, macro_Precision_optthr=0.8039, macro_Recall_optthr=0.7885, macro_Sensitivity_optthr=0.7885, macro_Specificity_optthr=0.6875\n",
      "\n",
      "[CV fold 4/5] scenario=time\n",
      "[split] train | rows=  307 | unique_ids=   33\n",
      "[split] val   | rows=   34 | unique_ids=   23\n",
      "[split] test  | rows=   84 | unique_ids=   19\n",
      "Epoch 001 | train 0.500025 | val 0.454353\n",
      "Epoch 002 | train 0.490063 | val 0.433822\n",
      "Epoch 003 | train 0.466777 | val 0.384369\n",
      "Epoch 004 | train 0.423987 | val 0.308662\n",
      "Epoch 005 | train 0.371069 | val 0.231798\n",
      "Epoch 006 | train 0.338532 | val 0.188725\n",
      "Epoch 007 | train 0.328747 | val 0.166562\n",
      "Epoch 008 | train 0.315484 | val 0.162742\n",
      "Epoch 009 | train 0.302538 | val 0.182701\n",
      "Epoch 010 | train 0.292378 | val 0.204621\n",
      "Epoch 011 | train 0.289434 | val 0.221644\n",
      "Epoch 012 | train 0.285031 | val 0.205955\n",
      "Epoch 013 | train 0.271576 | val 0.186917\n",
      "Epoch 014 | train 0.261409 | val 0.172120\n",
      "Epoch 015 | train 0.250554 | val 0.163838\n",
      "Epoch 016 | train 0.241721 | val 0.166750\n",
      "Epoch 017 | train 0.231461 | val 0.165539\n",
      "Epoch 018 | train 0.215053 | val 0.165314\n",
      "Epoch 019 | train 0.197584 | val 0.170105\n",
      "Epoch 020 | train 0.178972 | val 0.170267\n",
      "Epoch 021 | train 0.167455 | val 0.180608\n",
      "Epoch 022 | train 0.145250 | val 0.194869\n",
      "Epoch 023 | train 0.131903 | val 0.222359\n",
      "Epoch 024 | train 0.116589 | val 0.323077\n",
      "Epoch 025 | train 0.087233 | val 0.300038\n",
      "Epoch 026 | train 0.077774 | val 0.265692\n",
      "Epoch 027 | train 0.058445 | val 0.240329\n",
      "Epoch 028 | train 0.055865 | val 0.268125\n",
      "Early stopping at epoch 028 (best val 0.162742)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8775, macro_AUPRC_optthr=0.8909, macro_Brier_optthr=0.1429, macro_ACC_optthr=0.8214, macro_F1_optthr=0.8148, macro_Precision_optthr=0.8250, macro_Recall_optthr=0.8049, macro_Sensitivity_optthr=0.8049, macro_Specificity_optthr=0.8372\n",
      "\n",
      "[CV fold 5/5] scenario=time\n",
      "[split] train | rows=  383 | unique_ids=   32\n",
      "[split] val   | rows=   42 | unique_ids=   23\n",
      "[split] test  | rows=   84 | unique_ids=   18\n",
      "Epoch 001 | train 0.549415 | val 0.499936\n",
      "Epoch 002 | train 0.537982 | val 0.481916\n",
      "Epoch 003 | train 0.509951 | val 0.436976\n",
      "Epoch 004 | train 0.461173 | val 0.364495\n",
      "Epoch 005 | train 0.402180 | val 0.311318\n",
      "Epoch 006 | train 0.372492 | val 0.303845\n",
      "Epoch 007 | train 0.359027 | val 0.293776\n",
      "Epoch 008 | train 0.342248 | val 0.278581\n",
      "Epoch 009 | train 0.326048 | val 0.266223\n",
      "Epoch 010 | train 0.316666 | val 0.256636\n",
      "Epoch 011 | train 0.306598 | val 0.254796\n",
      "Epoch 012 | train 0.297978 | val 0.263606\n",
      "Epoch 013 | train 0.286394 | val 0.250751\n",
      "Epoch 014 | train 0.274463 | val 0.246363\n",
      "Epoch 015 | train 0.263532 | val 0.255575\n",
      "Epoch 016 | train 0.248468 | val 0.262294\n",
      "Epoch 017 | train 0.233124 | val 0.259555\n",
      "Epoch 018 | train 0.220193 | val 0.259511\n",
      "Epoch 019 | train 0.194910 | val 0.267384\n",
      "Epoch 020 | train 0.179272 | val 0.259024\n",
      "Epoch 021 | train 0.158066 | val 0.280759\n",
      "Epoch 022 | train 0.142823 | val 0.294501\n",
      "Epoch 023 | train 0.114908 | val 0.317846\n",
      "Epoch 024 | train 0.102633 | val 0.351813\n",
      "Epoch 025 | train 0.072089 | val 0.377367\n",
      "Epoch 026 | train 0.072361 | val 0.433841\n",
      "Epoch 027 | train 0.064449 | val 0.445965\n",
      "Epoch 028 | train 0.041369 | val 0.478567\n",
      "Epoch 029 | train 0.032306 | val 0.557887\n",
      "Epoch 030 | train 0.028089 | val 0.552213\n",
      "Epoch 031 | train 0.024358 | val 0.525629\n",
      "Epoch 032 | train 0.014366 | val 0.571070\n",
      "Epoch 033 | train 0.005264 | val 0.641201\n",
      "Epoch 034 | train 0.005125 | val 0.718612\n",
      "Early stopping at epoch 034 (best val 0.246363)\n",
      "Fold macro (optthr): macro_AUC_optthr=0.8315, macro_AUPRC_optthr=0.8725, macro_Brier_optthr=0.1668, macro_ACC_optthr=0.7500, macro_F1_optthr=0.7470, macro_Precision_optthr=0.7381, macro_Recall_optthr=0.7561, macro_Sensitivity_optthr=0.7561, macro_Specificity_optthr=0.7442\n",
      "\n",
      "CV averages (95% CI) @0.5:\n",
      "           macro_ACC: 0.7643  (95% CI 0.7182, 0.8103)\n",
      "           macro_AUC: 0.8258  (95% CI 0.7617, 0.8899)\n",
      "         macro_AUPRC: 0.8641  (95% CI 0.7843, 0.9439)\n",
      "         macro_Brier: 0.1926  (95% CI 0.1407, 0.2445)\n",
      "            macro_F1: 0.7909  (95% CI 0.7568, 0.8250)\n",
      "     macro_Precision: 0.7979  (95% CI 0.6960, 0.8998)\n",
      "        macro_Recall: 0.7997  (95% CI 0.6679, 0.9315)\n",
      "   macro_Sensitivity: 0.7997  (95% CI 0.6679, 0.9315)\n",
      "   macro_Specificity: 0.6964  (95% CI 0.4190, 0.9737)\n",
      "          task_1_ACC: 0.7643  (95% CI 0.7182, 0.8103)\n",
      "          task_1_AUC: 0.8258  (95% CI 0.7617, 0.8899)\n",
      "        task_1_AUPRC: 0.8641  (95% CI 0.7843, 0.9439)\n",
      "        task_1_Brier: 0.1926  (95% CI 0.1407, 0.2445)\n",
      "           task_1_F1: 0.7909  (95% CI 0.7568, 0.8250)\n",
      "    task_1_Precision: 0.7979  (95% CI 0.6960, 0.8998)\n",
      "       task_1_Recall: 0.7997  (95% CI 0.6679, 0.9315)\n",
      "  task_1_Sensitivity: 0.7997  (95% CI 0.6679, 0.9315)\n",
      "  task_1_Specificity: 0.6964  (95% CI 0.4190, 0.9737)\n",
      "\n",
      "CV averages (95% CI) @opt thresholds:\n",
      "    macro_ACC_optthr: 0.7381  (95% CI 0.6585, 0.8177)\n",
      "    macro_AUC_optthr: 0.8258  (95% CI 0.7617, 0.8899)\n",
      "  macro_AUPRC_optthr: 0.8641  (95% CI 0.7843, 0.9439)\n",
      "  macro_Brier_optthr: 0.1926  (95% CI 0.1407, 0.2445)\n",
      "     macro_F1_optthr: 0.7833  (95% CI 0.7487, 0.8179)\n",
      "macro_Precision_optthr: 0.7466  (95% CI 0.6560, 0.8372)\n",
      " macro_Recall_optthr: 0.8346  (95% CI 0.7408, 0.9284)\n",
      "macro_Sensitivity_optthr: 0.8346  (95% CI 0.7408, 0.9284)\n",
      "macro_Specificity_optthr: 0.5871  (95% CI 0.2633, 0.9109)\n",
      "   task_1_ACC_optthr: 0.7381  (95% CI 0.6585, 0.8177)\n",
      "   task_1_AUC_optthr: 0.8258  (95% CI 0.7617, 0.8899)\n",
      " task_1_AUPRC_optthr: 0.8641  (95% CI 0.7843, 0.9439)\n",
      " task_1_Brier_optthr: 0.1926  (95% CI 0.1407, 0.2445)\n",
      "    task_1_F1_optthr: 0.7833  (95% CI 0.7487, 0.8179)\n",
      "task_1_Precision_optthr: 0.7466  (95% CI 0.6560, 0.8372)\n",
      "task_1_Recall_optthr: 0.8346  (95% CI 0.7408, 0.9284)\n",
      "task_1_Sensitivity_optthr: 0.8346  (95% CI 0.7408, 0.9284)\n",
      "task_1_Specificity_optthr: 0.5871  (95% CI 0.2633, 0.9109)\n",
      "Arch params (used in all folds): {'d_fix_latent': 256, 'd_rand_latent': 256, 'n_kernels': 8, 'dropout': 0.0, 'use_attention': False, 'use_random': False}\n",
      "Train params (used in all folds): {'lr': 0.001, 'weight_decay': 3e-05, 'batch_size': 128, 'max_epochs': 100, 'patience': 20, 'threshold': 0.5, 'lambda_mean0': 1e-06, 'lambda_ridge': 1e-05, 'lambda_orth_latent': 3e-05, 'lambda_film_identity': 3e-05, 'lambda_kan': 0, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "arch_ablations = dict(d_fix_latent=256, d_rand_latent=256, n_kernels=8, dropout=0.0, use_attention = False, use_random = False)\n",
    "\n",
    "res = run_training_and_eval_kan(\n",
    "    X_only_fixed=X_only_fixed_np,                 # -> TC and part of FIXED\n",
    "    X_fixed_and_random=X_fixed_and_random_np,     # -> RANDOM and part of FIXED\n",
    "    y=y_np, y_lags=y_lags_np, dt_lags=dt_lags_np,\n",
    "    pid_idx=pid_np, time_index=time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsWrapper,\n",
    "    mode=\"cv_only\",          # or \"cv_only\"/\"nested_cv\"\n",
    "    scenario=\"both\",        # or \"cases\"\n",
    "    arch_defaults=arch_ablations,\n",
    "    train_defaults=train_defaults,\n",
    "    verbose=True,\n",
    "    outer_folds=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
