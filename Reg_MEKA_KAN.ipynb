{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f278667b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1069b",
   "metadata": {},
   "source": [
    "## Root and data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "068e5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "root_dir = \"/Users/silviumatu/Desktop/Code/Python/Disertatie/Disertatie_Matu_Silviu_v1\"\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "data_dir = os.path.join(root_dir, \"Data\")\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1166a5e",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d6a9787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_rows': 4429,\n",
       " 'n_unique_participants': 137,\n",
       " 'avg_rows_per_participant': np.float64(32.32846715328467),\n",
       " 'min_rows_per_participant': 1,\n",
       " 'max_rows_per_participant': 55,\n",
       " 'avg_age_per_participant': np.float64(27.364963503649633)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_reg_df = pd.read_csv(os.path.join(data_dir, \"EXP_regression_data_forecast.csv\"))\n",
    "columns_EXP_reg_df = pd.read_csv(os.path.join(data_dir, \"columns_EXP_regression_data_forecast_S.csv\"))\n",
    "\n",
    "EXP_reg_df.head()\n",
    "\n",
    "\n",
    "\n",
    "n_rows = len(EXP_reg_df)\n",
    "n_unique_participants = EXP_reg_df[\"x_participant_id\"].nunique()\n",
    "\n",
    "\n",
    "group_sizes = EXP_reg_df.groupby(\"x_participant_id\").size()\n",
    "avg_rows_per_id = group_sizes.mean()\n",
    "min_rows_per_id = group_sizes.min()\n",
    "max_rows_per_id = group_sizes.max()\n",
    "\n",
    "\n",
    "avg_age_per_participant = (\n",
    "    EXP_reg_df.groupby(\"x_participant_id\")[\"x_age\"].first().mean()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "summary = {\n",
    "    \"n_rows\": n_rows,\n",
    "    \"n_unique_participants\": n_unique_participants,\n",
    "    \"avg_rows_per_participant\": avg_rows_per_id,\n",
    "    \"min_rows_per_participant\": min_rows_per_id,\n",
    "    \"max_rows_per_participant\": max_rows_per_id,\n",
    "    \"avg_age_per_participant\": avg_age_per_participant\n",
    "}\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c174fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the outcome column(s) marked with 1 in the \"outcomes\" column of columns_EXP_reg_df\n",
    "EXP_reg_outcome_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['outcomes'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_y = EXP_reg_df[EXP_reg_outcome_cols]\n",
    "EXP_reg_y.head()\n",
    "\n",
    "# Same for outcomes lags column(s)\n",
    "EXP_reg_outcomes_lags_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['outcomes_lags'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_outcomes_lags = EXP_reg_df[EXP_reg_outcomes_lags_cols]\n",
    "\n",
    "# Same for participant column(s)\n",
    "EXP_reg_participant_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['participant_id'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_participant_id = EXP_reg_df[EXP_reg_participant_cols]\n",
    "\n",
    "# Same for time column(s)\n",
    "EXP_reg_time_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['time'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_time = EXP_reg_df[EXP_reg_time_cols]\n",
    "\n",
    "# Same for forecast horizons column(s)\n",
    "EXP_reg_forecast_horizons_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['forecast_horizons'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_forecast_horizons = EXP_reg_df[EXP_reg_forecast_horizons_cols]\n",
    "\n",
    "# Same for fixed effects column(s)\n",
    "EXP_reg_only_fixed_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['only_fixed'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_only_fixed = EXP_reg_df[EXP_reg_only_fixed_cols]\n",
    "\n",
    "# Same for random effects column(s)\n",
    "EXP_reg_fixed_and_random_cols = columns_EXP_reg_df.loc[columns_EXP_reg_df['fixed_and_random'] == 1, 'column_name'].tolist()\n",
    "EXP_reg_fixed_and_random = EXP_reg_df[EXP_reg_fixed_and_random_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b382d0b",
   "metadata": {},
   "source": [
    "# Custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8845d70a",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e00bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from efficient_kan import KANLinear\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "\n",
    "def _make_kan(in_dim, out_dim,\n",
    "              grid_size=8, spline_order=3,\n",
    "              scale_noise=0.1, scale_base=1.0, scale_spline=1.0,\n",
    "              enable_standalone_scale_spline=True,\n",
    "              base_activation=torch.nn.SiLU,\n",
    "              grid_eps=0.02, grid_range=(-1.0, 1.0)):\n",
    "    return KANLinear(\n",
    "        in_features=in_dim,\n",
    "        out_features=out_dim,\n",
    "        grid_size=grid_size,\n",
    "        spline_order=spline_order,\n",
    "        scale_noise=scale_noise,\n",
    "        scale_base=scale_base,\n",
    "        scale_spline=scale_spline,\n",
    "        enable_standalone_scale_spline=enable_standalone_scale_spline,\n",
    "        base_activation=base_activation,\n",
    "        grid_eps=grid_eps,\n",
    "        grid_range=list(grid_range),\n",
    "    )\n",
    "\n",
    "class KANBlock(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims=(128, 64), out_dim=None,\n",
    "                 dropout=0.0,\n",
    "                 grid_size=8, spline_order=3,\n",
    "                 scale_noise=0.1, scale_base=1.0, scale_spline=1.0,\n",
    "                 enable_standalone_scale_spline=True,\n",
    "                 base_activation=torch.nn.SiLU,\n",
    "                 grid_eps=0.02, grid_range=(-1.0, 1.0)):\n",
    "        super().__init__()\n",
    "        dims = [in_dim] + list(hidden_dims)\n",
    "        layers = []\n",
    "        for d0, d1 in zip(dims[:-1], dims[1:]):\n",
    "            layers.append(_make_kan(\n",
    "                d0, d1,\n",
    "                grid_size=grid_size, spline_order=spline_order,\n",
    "                scale_noise=scale_noise, scale_base=scale_base, scale_spline=scale_spline,\n",
    "                enable_standalone_scale_spline=enable_standalone_scale_spline,\n",
    "                base_activation=base_activation,\n",
    "                grid_eps=grid_eps, grid_range=grid_range\n",
    "            ))\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "        if out_dim is not None:\n",
    "            layers.append(_make_kan(\n",
    "                dims[-1], out_dim,\n",
    "                grid_size=grid_size, spline_order=spline_order,\n",
    "                scale_noise=scale_noise, scale_base=scale_base, scale_spline=scale_spline,\n",
    "                enable_standalone_scale_spline=enable_standalone_scale_spline,\n",
    "                base_activation=base_activation,\n",
    "                grid_eps=grid_eps, grid_range=grid_range\n",
    "            ))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        reg = 0.0\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, KANLinear):\n",
    "                reg = reg + m.regularization_loss(\n",
    "                    regularize_activation=regularize_activation,\n",
    "                    regularize_entropy=regularize_entropy\n",
    "                )\n",
    "        return reg\n",
    "\n",
    "class TemporalKernelAttentionKAN(nn.Module):\n",
    "    def __init__(self, n_kernels: int = 4, d_att: int = 32,\n",
    "                 grid_size=8, spline_order=3, dropout=0.0,\n",
    "                 normalize_weights: bool = False):\n",
    "        super().__init__()\n",
    "        self.n_k = n_kernels\n",
    "        self.normalize = normalize_weights\n",
    "        self.pi_logits = nn.Parameter(torch.zeros(n_kernels))\n",
    "        self.lam_raw = nn.Parameter(torch.zeros(n_kernels))\n",
    "        self.summarize = KANBlock(1, hidden_dims=(d_att,), out_dim=d_att,\n",
    "                                  dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "        self.out_head = KANBlock(d_att, hidden_dims=(d_att,), out_dim=1,\n",
    "                                 dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, y_lags, dt_lags):\n",
    "        B, L = y_lags.shape\n",
    "        pi = F.softmax(self.pi_logits, dim=-1)\n",
    "        lam = F.softplus(self.lam_raw) + 1e-6\n",
    "        pi_exp = pi.view(1, 1, -1).expand(B, 1, -1)\n",
    "        lam_exp = lam.view(1, 1, -1).expand(B, 1, -1)\n",
    "        kernel = torch.exp(-lam_exp * dt_lags.unsqueeze(-1))\n",
    "        w_lags = torch.sum(pi_exp * kernel, dim=-1)\n",
    "        if self.normalize:\n",
    "            w_sum = w_lags.sum(dim=1, keepdim=True) + 1e-8\n",
    "            w_lags = w_lags / w_sum\n",
    "        s = torch.sum(w_lags * y_lags, dim=1, keepdim=True)\n",
    "        z_att = self.summarize(s)\n",
    "        e_att = self.out_head(z_att)\n",
    "        return e_att, w_lags, z_att\n",
    "\n",
    "class FixedBranchKAN(nn.Module):\n",
    "    def __init__(self, d_fix, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = KANBlock(d_fix, hidden_dims=(256, 128), out_dim=d_latent,\n",
    "                            dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "        self.head = KANBlock(d_latent, hidden_dims=(64,), out_dim=1,\n",
    "                             dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, X_fix):\n",
    "        z = self.enc(X_fix)\n",
    "        e = self.head(z)\n",
    "        return e, z\n",
    "\n",
    "class RandEncoderKAN(nn.Module):\n",
    "    def __init__(self, d_zrand, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = KANBlock(d_zrand, hidden_dims=(256, 128), out_dim=d_latent,\n",
    "                            dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, Zrand):\n",
    "        return self.enc(Zrand)\n",
    "\n",
    "class TCEncoderKAN(nn.Module):\n",
    "    def __init__(self, d_tc, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.enc = KANBlock(d_tc, hidden_dims=(256, 128), out_dim=d_latent,\n",
    "                            dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, TC):\n",
    "        return self.enc(TC)\n",
    "\n",
    "class RandomHeadKAN(nn.Module):\n",
    "    def __init__(self, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.head = KANBlock(d_latent, hidden_dims=(64,), out_dim=1,\n",
    "                             dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.head(z)\n",
    "\n",
    "class FiLMFromTC(nn.Module):\n",
    "    def __init__(self, d_latent=64, grid_size=8, spline_order=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.gamma = KANBlock(d_latent, hidden_dims=(64,), out_dim=d_latent,\n",
    "                              dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "        self.beta = KANBlock(d_latent, hidden_dims=(64,), out_dim=d_latent,\n",
    "                             dropout=dropout, grid_size=grid_size, spline_order=spline_order)\n",
    "\n",
    "    def forward(self, e_tc):\n",
    "        return self.gamma(e_tc), self.beta(e_tc)\n",
    "\n",
    "class KANAdditiveMixedEffects(nn.Module):\n",
    "    def __init__(self,\n",
    "                 y_dim: int,\n",
    "                 d_fix: int,\n",
    "                 d_tc: int,\n",
    "                 d_zrand: int,\n",
    "                 n_ids: int = None,\n",
    "                 use_id_intercept: bool = True,\n",
    "                 n_kernels: int = 4,\n",
    "                 d_att: int = 128,\n",
    "                 d_fix_latent: int = 128,\n",
    "                 d_rand_latent: int = 128,\n",
    "                 grid_size: int = 8,\n",
    "                 spline_order: int = 3,\n",
    "                 dropout: float = 0.0,\n",
    "                 normalize_att_weights: bool = True,\n",
    "                 # mixing\n",
    "                 mix_mode: str = \"softmax\",\n",
    "                 per_task_mixing: bool = True,\n",
    "                 init_mixture: Optional[Tuple[float, float, float]] = None,\n",
    "                 # --- NEW flags ---\n",
    "                 use_attention: bool = True,\n",
    "                 use_random: bool = True):\n",
    "        super().__init__()\n",
    "        self.y_dim = y_dim\n",
    "        self.d_fix = d_fix\n",
    "        self.d_tc  = d_tc\n",
    "        self.d_zr  = d_zrand\n",
    "        self.d_rand_latent = d_rand_latent\n",
    "        self.d_att = d_att\n",
    "        self.use_attention = use_attention\n",
    "        self.use_random = use_random\n",
    "\n",
    "        # ATTENTION BRANCH (optional)\n",
    "        self.att_branches = (\n",
    "            nn.ModuleList([\n",
    "                TemporalKernelAttentionKAN(n_kernels=n_kernels, d_att=d_att,\n",
    "                                           grid_size=grid_size, spline_order=spline_order,\n",
    "                                           dropout=dropout, normalize_weights=normalize_att_weights)\n",
    "                for _ in range(y_dim)\n",
    "            ]) if use_attention else None\n",
    "        )\n",
    "\n",
    "        # FIXED BRANCH (always on)\n",
    "        self.fix_branches = nn.ModuleList([\n",
    "            FixedBranchKAN(d_fix=d_fix, d_latent=d_fix_latent,\n",
    "                           grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "            for _ in range(y_dim)\n",
    "        ])\n",
    "\n",
    "        # RANDOM BRANCH (optional)\n",
    "        if use_random:\n",
    "            self.rand_encoders = (\n",
    "                nn.ModuleList([\n",
    "                    RandEncoderKAN(d_zrand=d_zrand, d_latent=d_rand_latent,\n",
    "                                   grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "                    for _ in range(y_dim)\n",
    "                ]) if d_zrand > 0 else None\n",
    "            )\n",
    "            self.tc_encoders = (\n",
    "                nn.ModuleList([\n",
    "                    TCEncoderKAN(d_tc=d_tc, d_latent=d_rand_latent,\n",
    "                                 grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "                    for _ in range(y_dim)\n",
    "                ]) if d_tc > 0 else None\n",
    "            )\n",
    "            self.film_from_tc = (\n",
    "                nn.ModuleList([\n",
    "                    FiLMFromTC(d_latent=d_rand_latent, grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "                    for _ in range(y_dim)\n",
    "                ]) if d_tc > 0 else None\n",
    "            )\n",
    "            self.rand_heads = nn.ModuleList([\n",
    "                RandomHeadKAN(d_latent=d_rand_latent, grid_size=grid_size, spline_order=spline_order, dropout=dropout)\n",
    "                for _ in range(y_dim)\n",
    "            ])\n",
    "        else:\n",
    "            self.rand_encoders = None\n",
    "            self.tc_encoders = None\n",
    "            self.film_from_tc = None\n",
    "            self.rand_heads = None\n",
    "\n",
    "        # mixing config\n",
    "        self.mix_mode = mix_mode\n",
    "        self.per_task_mixing = per_task_mixing\n",
    "        size = (y_dim, 3) if per_task_mixing else (1, 3)\n",
    "\n",
    "        mix_logits = torch.zeros(size)\n",
    "        if init_mixture is not None:\n",
    "            a, f, r = init_mixture\n",
    "            eps = 1e-8\n",
    "            probs = torch.tensor([a, f, r], dtype=mix_logits.dtype).clamp_min(eps)\n",
    "            probs = probs / probs.sum()\n",
    "            mix_logits = mix_logits + probs.log()\n",
    "        self.mix_logits = nn.Parameter(mix_logits)\n",
    "\n",
    "    def forward(self, X_fix, TC, Zrand, y_lags, dt_lags,\n",
    "                pid_idx: Optional[torch.Tensor] = None,\n",
    "                pid_seen_mask: Optional[torch.Tensor] = None):\n",
    "        device = X_fix.device\n",
    "        dtype  = X_fix.dtype\n",
    "        B = X_fix.size(0)\n",
    "\n",
    "        # make per-task lag tensors\n",
    "        if y_lags.dim() == 2:\n",
    "            y_lags_list = [y_lags for _ in range(self.y_dim)]\n",
    "        elif y_lags.dim() == 3:\n",
    "            assert y_lags.size(2) == self.y_dim\n",
    "            y_lags_list = [y_lags[:, :, j] for j in range(self.y_dim)]\n",
    "        else:\n",
    "            raise ValueError(\"y_lags must be [B, L] or [B, L, y_dim].\")\n",
    "\n",
    "        # determine L for zero-shapes if attention off\n",
    "        L = dt_lags.size(1) if (dt_lags is not None and dt_lags.dim() == 2) else 1\n",
    "\n",
    "        e_att_all, e_fix_all, e_rand_all = [], [], []\n",
    "        z_att_list, z_fix_list = [], []\n",
    "        z_rand_list, z_rand_film_list, z_tc_list = [], [], []\n",
    "        w_lags_list, e_rand_mod_list, gamma_list, beta_list = [], [], [], []\n",
    "\n",
    "        for j in range(self.y_dim):\n",
    "            # --- Attention branch (optional) ---\n",
    "            if self.use_attention and (self.att_branches is not None):\n",
    "                e_att_j, w_lags_j, z_att_j = self.att_branches[j](y_lags_list[j], dt_lags)\n",
    "            else:\n",
    "                e_att_j  = torch.zeros(B, 1, device=device, dtype=dtype)\n",
    "                w_lags_j = torch.zeros(B, L, device=device, dtype=dtype)\n",
    "                z_att_j  = torch.zeros(B, self.d_att, device=device, dtype=dtype)\n",
    "\n",
    "            # --- Fixed branch (always) ---\n",
    "            e_fix_j, z_fix_j = self.fix_branches[j](X_fix)\n",
    "\n",
    "            # --- Random branch (optional) ---\n",
    "            if self.use_random and (self.rand_heads is not None):\n",
    "                if (Zrand is not None) and (Zrand.size(1) > 0) and (self.rand_encoders is not None):\n",
    "                    z_rand_j = self.rand_encoders[j](Zrand)\n",
    "                else:\n",
    "                    z_rand_j = torch.zeros(B, self.d_rand_latent, device=device, dtype=dtype)\n",
    "\n",
    "                if self.d_tc > 0 and (TC is not None) and (TC.size(1) > 0) and (self.tc_encoders is not None):\n",
    "                    z_tc_j = self.tc_encoders[j](TC)\n",
    "                    if self.film_from_tc is not None:\n",
    "                        gamma_j, beta_j = self.film_from_tc[j](z_tc_j)\n",
    "                    else:\n",
    "                        gamma_j = torch.ones_like(z_rand_j)\n",
    "                        beta_j  = torch.zeros_like(z_rand_j)\n",
    "                else:\n",
    "                    z_tc_j = torch.zeros_like(z_rand_j)\n",
    "                    gamma_j = torch.ones_like(z_rand_j)\n",
    "                    beta_j  = torch.zeros_like(z_rand_j)\n",
    "\n",
    "                z_tilde_j = gamma_j * z_rand_j + beta_j\n",
    "                e_rand_j  = self.rand_heads[j](z_tilde_j)\n",
    "            else:\n",
    "                z_rand_j = torch.zeros(B, self.d_rand_latent, device=device, dtype=dtype)\n",
    "                z_tc_j   = torch.zeros_like(z_rand_j)\n",
    "                gamma_j  = torch.ones_like(z_rand_j)\n",
    "                beta_j   = torch.zeros_like(z_rand_j)\n",
    "                z_tilde_j= z_rand_j\n",
    "                e_rand_j = torch.zeros(B, 1, device=device, dtype=dtype)\n",
    "\n",
    "            # collect\n",
    "            e_att_all.append(e_att_j)\n",
    "            e_fix_all.append(e_fix_j)\n",
    "            e_rand_all.append(e_rand_j)\n",
    "\n",
    "            z_att_list.append(z_att_j)\n",
    "            z_fix_list.append(z_fix_j)\n",
    "            z_rand_list.append(z_rand_j)\n",
    "            z_rand_film_list.append(z_tilde_j)\n",
    "            z_tc_list.append(z_tc_j)\n",
    "\n",
    "            w_lags_list.append(w_lags_j)\n",
    "            e_rand_mod_list.append(e_rand_j)\n",
    "            gamma_list.append(gamma_j)\n",
    "            beta_list.append(beta_j)\n",
    "\n",
    "        # concat parts\n",
    "        e_att = torch.cat(e_att_all, dim=1) if e_att_all else torch.zeros(B, self.y_dim, device=device, dtype=dtype)\n",
    "        e_fix = torch.cat(e_fix_all, dim=1) if e_fix_all else torch.zeros(B, self.y_dim, device=device, dtype=dtype)\n",
    "        e_rand= torch.cat(e_rand_all,dim=1) if e_rand_all else torch.zeros(B, self.y_dim, device=device, dtype=dtype)\n",
    "\n",
    "        # mixing weights\n",
    "        W = self.mix_logits\n",
    "        if W.size(0) == 1:\n",
    "            W = W.expand(self.y_dim, -1)\n",
    "        if self.mix_mode == \"softmax\":\n",
    "            W = F.softmax(W, dim=-1)\n",
    "        elif self.mix_mode == \"softplus\":\n",
    "            W = F.softplus(W)\n",
    "        elif self.mix_mode == \"sigmoid\":\n",
    "            W = torch.sigmoid(W)\n",
    "        else:\n",
    "            raise ValueError(\"mix_mode must be one of {'softmax','softplus','sigmoid'}\")\n",
    "\n",
    "        e_stack = torch.stack([e_att, e_fix, e_rand], dim=-1)  # (B, y_dim, 3)\n",
    "        y_hat = (e_stack * W.unsqueeze(0)).sum(dim=-1)\n",
    "\n",
    "        parts = {\n",
    "            \"e_att\": e_att,\n",
    "            \"e_fix\": e_fix,\n",
    "            \"e_rand\": e_rand,\n",
    "            \"z_att_list\": z_att_list,\n",
    "            \"z_fix_list\": z_fix_list,\n",
    "            \"z_rand_list\": z_rand_list,\n",
    "            \"z_rand_film_list\": z_rand_film_list,\n",
    "            \"z_tc_list\": z_tc_list,\n",
    "            \"w_lags_list\": w_lags_list,\n",
    "            \"e_rand_mod_list\": e_rand_mod_list,\n",
    "            \"film_gamma_list\": gamma_list,\n",
    "            \"film_beta_list\": beta_list,\n",
    "            \"mix_weights\": W,   # (y_dim, 3) columns: [att, fix, rand]\n",
    "        }\n",
    "        return y_hat, parts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba99af",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b19c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "# ---- helpers (no metrics) ----\n",
    "def _regression_loss(\n",
    "    y_hat: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    loss: str = \"mse\",\n",
    "    huber_delta: float = 1.0\n",
    ") -> torch.Tensor:\n",
    "    # allow [B] vs [B,1]\n",
    "    if y_true.shape != y_hat.shape:\n",
    "        if y_true.dim() == 2 and y_true.size(1) == 1 and y_hat.dim() == 1:\n",
    "            y_hat = y_hat.unsqueeze(1)\n",
    "        elif y_true.dim() == 1 and y_hat.dim() == 2 and y_hat.size(1) == 1:\n",
    "            y_true = y_true.unsqueeze(1)\n",
    "        elif y_true.shape != y_hat.shape:\n",
    "            raise ValueError(f\"y_true {y_true.shape} must equal y_hat {y_hat.shape}\")\n",
    "    if loss == \"mse\":   return F.mse_loss(y_hat, y_true)\n",
    "    if loss == \"mae\":   return F.l1_loss(y_hat, y_true)\n",
    "    if loss == \"huber\": return F.huber_loss(y_hat, y_true, delta=huber_delta)\n",
    "    raise ValueError(\"loss must be one of {'mse','mae','huber'}\")\n",
    "\n",
    "def _orthogonality_penalty_latents(z_fix: torch.Tensor, z_rand_film: torch.Tensor) -> torch.Tensor:\n",
    "    B = z_fix.size(0)\n",
    "    if B <= 1:\n",
    "        return z_fix.new_zeros(())\n",
    "    zf = z_fix - z_fix.mean(dim=0, keepdim=True)\n",
    "    zr = z_rand_film - z_rand_film.mean(dim=0, keepdim=True)\n",
    "    M = (zf.T @ zr) / float(B)\n",
    "    return (M ** 2).mean()\n",
    "\n",
    "class KANMixedEffectsRegWrapper:\n",
    "    \"\"\"\n",
    "    Thin training/inference wrapper for regression KAN/MLP AdditiveMixedEffects.\n",
    "    Adds gating for attention/random branches via flags, and zeros the\n",
    "    random-branch regularizers when that branch is disabled.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 cfg: Optional[Dict[str, Any]] = None,\n",
    "                 device: Optional[torch.device] = None):\n",
    "        self.model = model\n",
    "        self.cfg = {\n",
    "            # branch flags (optional; if None we infer from model or default True)\n",
    "            \"use_attention\": None,\n",
    "            \"use_random\": None,\n",
    "\n",
    "            # regularization strengths\n",
    "            \"lambda_mean0\": 1e-4,\n",
    "            \"lambda_ridge\": 1e-4,\n",
    "            \"lambda_orth_latent\": 1e-3,\n",
    "            \"lambda_film_identity\": 1e-4,\n",
    "            \"lambda_kan\": 0.0,\n",
    "            \"kan_reg_activation\": 1.0,\n",
    "            \"kan_reg_entropy\": 1.0,\n",
    "\n",
    "            # optimization\n",
    "            \"clip_grad\": 5.0,\n",
    "            \"lr\": 1e-3,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"max_epochs\": 100,\n",
    "            \"patience\": 10,\n",
    "            \"batch_size\": 256,\n",
    "            \"amp\": True,   # CUDA AMP\n",
    "\n",
    "            # regression loss\n",
    "            \"loss\": \"mse\",     # \"mse\" | \"mae\" | \"huber\"\n",
    "            \"huber_delta\": 1.0,\n",
    "        }\n",
    "        if cfg:\n",
    "            self.cfg.update(cfg)\n",
    "\n",
    "        self.device = (\n",
    "            device\n",
    "            or (torch.device(\"mps\") if torch.backends.mps.is_available() else None)\n",
    "            or (torch.device(\"cuda\") if torch.cuda.is_available() else None)\n",
    "            or torch.device(\"cpu\")\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.history_: Dict[str, list] = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    # Resolve flags: cfg → model attributes → defaults\n",
    "    def _flags(self) -> Tuple[bool, bool]:\n",
    "        cfg = self.cfg\n",
    "        use_att = cfg.get(\"use_attention\")\n",
    "        use_rnd = cfg.get(\"use_random\")\n",
    "        if use_att is None:\n",
    "            use_att = bool(getattr(self.model, \"use_attention\", True))\n",
    "        if use_rnd is None:\n",
    "            use_rnd = bool(getattr(self.model, \"use_random\", True))\n",
    "        return use_att, use_rnd\n",
    "\n",
    "    def _sum_kan_regularization(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        NOTE: If the model was constructed with branches disabled,\n",
    "        those modules won't exist and thus won't contribute here.\n",
    "        If the model *did* construct them but zeros outputs via flags,\n",
    "        this will still count them (there’s no safe generic way to carve them out).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from efficient_kan import KANLinear\n",
    "        except Exception:\n",
    "            return torch.tensor(0.0, device=next(self.model.parameters()).device)\n",
    "        reg = torch.tensor(0.0, device=next(self.model.parameters()).device)\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, KANLinear):\n",
    "                reg = reg + m.regularization_loss(\n",
    "                    regularize_activation=self.cfg[\"kan_reg_activation\"],\n",
    "                    regularize_entropy=self.cfg[\"kan_reg_entropy\"]\n",
    "                )\n",
    "        return reg\n",
    "\n",
    "    def compute_loss(self,\n",
    "                     y_true: torch.Tensor,\n",
    "                     y_hat: torch.Tensor,\n",
    "                     parts: Dict[str, Any],\n",
    "                     *,\n",
    "                     X_fix: torch.Tensor,\n",
    "                     pid_idx: Optional[torch.Tensor] = None,\n",
    "                     TC: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "        cfg = self.cfg\n",
    "        use_att, use_rnd = self._flags()\n",
    "\n",
    "        # 1) supervised regression loss\n",
    "        loss_pred = _regression_loss(y_hat, y_true, loss=cfg[\"loss\"], huber_delta=cfg[\"huber_delta\"])\n",
    "\n",
    "        # 2) random-output regularization (gated by use_random)\n",
    "        if use_rnd:\n",
    "            e_rand = parts.get(\"e_rand\", None)\n",
    "            if e_rand is None:\n",
    "                e_rand = torch.zeros_like(y_hat)\n",
    "            loss_mean0 = (e_rand.mean(dim=0) ** 2).sum()\n",
    "            loss_ridge = (e_rand ** 2).mean()\n",
    "        else:\n",
    "            loss_mean0 = torch.tensor(0.0, device=y_hat.device, dtype=y_hat.dtype)\n",
    "            loss_ridge = torch.tensor(0.0, device=y_hat.device, dtype=y_hat.dtype)\n",
    "\n",
    "        # 3) latent orthogonality (fixed vs FiLM'ed random) — gated by use_random\n",
    "        if use_rnd:\n",
    "            z_fix_list = parts.get(\"z_fix_list\", [])\n",
    "            z_rand_film_list = parts.get(\"z_rand_film_list\", [])\n",
    "            loss_orth = torch.tensor(0.0, device=y_hat.device)\n",
    "            if len(z_fix_list) and len(z_rand_film_list) and len(z_fix_list) == len(z_rand_film_list):\n",
    "                acc = 0.0\n",
    "                for zf, zr in zip(z_fix_list, z_rand_film_list):\n",
    "                    acc = acc + _orthogonality_penalty_latents(zf, zr)\n",
    "                loss_orth = acc / float(len(z_fix_list))\n",
    "        else:\n",
    "            loss_orth = torch.tensor(0.0, device=y_hat.device, dtype=y_hat.dtype)\n",
    "\n",
    "        # 4) FiLM identity — gated by use_random\n",
    "        if use_rnd:\n",
    "            gamma_list = parts.get(\"film_gamma_list\", [])\n",
    "            beta_list  = parts.get(\"film_beta_list\", [])\n",
    "            one = torch.tensor(1.0, device=y_hat.device, dtype=y_hat.dtype)\n",
    "            acc = 0.0\n",
    "            if len(gamma_list) and len(beta_list):\n",
    "                for g, b in zip(gamma_list, beta_list):\n",
    "                    acc = acc + ((g - one) ** 2).mean() + (b ** 2).mean()\n",
    "            loss_film = torch.as_tensor(acc, device=y_hat.device, dtype=y_hat.dtype) / float(max(1, len(gamma_list)))\n",
    "        else:\n",
    "            loss_film = torch.tensor(0.0, device=y_hat.device, dtype=y_hat.dtype)\n",
    "\n",
    "        # 5) KAN regularization (optional; branches disabled at construction won't contribute)\n",
    "        kan_reg = torch.tensor(0.0, device=y_hat.device, dtype=y_hat.dtype)\n",
    "        if cfg[\"lambda_kan\"] > 0:\n",
    "            kan_reg = self._sum_kan_regularization()\n",
    "\n",
    "        total = (\n",
    "            loss_pred\n",
    "            + cfg[\"lambda_mean0\"] * loss_mean0\n",
    "            + cfg[\"lambda_ridge\"] * loss_ridge\n",
    "            + cfg[\"lambda_orth_latent\"] * loss_orth\n",
    "            + cfg[\"lambda_film_identity\"] * loss_film\n",
    "            + cfg[\"lambda_kan\"] * kan_reg\n",
    "        )\n",
    "        parts_out = {\n",
    "            \"loss_total\": float(total.detach().cpu()),\n",
    "            \"loss_pred\":  float(loss_pred.detach().cpu()),\n",
    "            \"loss_mean0\": float(loss_mean0.detach().cpu()),\n",
    "            \"loss_ridge\": float(loss_ridge.detach().cpu()),\n",
    "            \"loss_orth\":  float(loss_orth.detach().cpu()),\n",
    "            \"loss_fi\":    float(loss_film.detach().cpu()),\n",
    "            \"loss_kan\":   float(kan_reg.detach().cpu()),\n",
    "        }\n",
    "        return total, parts_out\n",
    "\n",
    "    def fit(self, train_loader, val_loader=None, *, verbose=True):\n",
    "        cfg = self.cfg\n",
    "        model = self.model\n",
    "        device = self.device\n",
    "\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "\n",
    "        use_amp = bool(cfg.get(\"amp\", True) and torch.cuda.is_available())\n",
    "        scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "        best_val = float(\"inf\")\n",
    "        best_state = None\n",
    "        no_improve = 0\n",
    "\n",
    "        for epoch in range(1, cfg[\"max_epochs\"] + 1):\n",
    "            # --- train ---\n",
    "            model.train()\n",
    "            total_tr, n_tr = 0.0, 0\n",
    "            for batch in train_loader:\n",
    "                Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b = [b.to(device) for b in batch]\n",
    "                TC_in = TC_b if TC_b.size(1) > 0 else None\n",
    "                Zr_in = Zr_b if Zr_b.size(1) > 0 else None\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "                    y_hat, parts = model(\n",
    "                        X_fix=Xf_b, TC=TC_in, Zrand=Zr_in,\n",
    "                        y_lags=yl_b, dt_lags=dt_b,\n",
    "                        pid_idx=pid_b, pid_seen_mask=seen_b\n",
    "                    )\n",
    "                    loss, _ = self.compute_loss(\n",
    "                        y_true=y_b, y_hat=y_hat, parts=parts,\n",
    "                        X_fix=Xf_b, pid_idx=pid_b, TC=TC_in\n",
    "                    )\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                if cfg.get(\"clip_grad\", None):\n",
    "                    scaler.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"clip_grad\"])\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "\n",
    "                bs = Xf_b.size(0)\n",
    "                total_tr += float(loss.detach().cpu()) * bs\n",
    "                n_tr += bs\n",
    "\n",
    "            train_loss = total_tr / max(1, n_tr)\n",
    "            self.history_[\"train_loss\"].append(train_loss)\n",
    "\n",
    "            # --- validate ---\n",
    "            if val_loader is not None:\n",
    "                model.eval()\n",
    "                total_va, n_va = 0.0, 0\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_loader:\n",
    "                        Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b = [b.to(device) for b in batch]\n",
    "                        TC_in = TC_b if TC_b.size(1) > 0 else None\n",
    "                        Zr_in = Zr_b if Zr_b.size(1) > 0 else None\n",
    "                        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "                            y_hat, parts = model(\n",
    "                                X_fix=Xf_b, TC=TC_in, Zrand=Zr_in,\n",
    "                                y_lags=yl_b, dt_lags=dt_b,\n",
    "                                pid_idx=pid_b, pid_seen_mask=seen_b\n",
    "                            )\n",
    "                            l, _ = self.compute_loss(\n",
    "                                y_true=y_b, y_hat=y_hat, parts=parts,\n",
    "                                X_fix=Xf_b, pid_idx=pid_b, TC=TC_in\n",
    "                            )\n",
    "                        bs = Xf_b.size(0)\n",
    "                        total_va += float(l.detach().cpu()) * bs\n",
    "                        n_va += bs\n",
    "                val_loss = total_va / max(1, n_va)\n",
    "                self.history_[\"val_loss\"].append(val_loss)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:03d} | train {train_loss:.6f} | val {val_loss:.6f}\")\n",
    "\n",
    "                if val_loss < best_val - 1e-6:\n",
    "                    best_val = val_loss\n",
    "                    best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= cfg[\"patience\"]:\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping at epoch {epoch:03d} (best val {best_val:.6f})\")\n",
    "                        break\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:03d} | train {train_loss:.6f}\")\n",
    "\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, X_fix, TC, Zrand, y_lags, dt_lags, pid_idx=None, pid_seen_mask=None) -> torch.Tensor:\n",
    "        self.model.eval()\n",
    "        y_hat, _ = self.model(\n",
    "            X_fix=X_fix, TC=TC if (TC is not None and TC.size(1) > 0) else None, Zrand=Zrand if (Zrand is not None and Zrand.size(1) > 0) else None,\n",
    "            y_lags=y_lags, dt_lags=dt_lags,\n",
    "            pid_idx=pid_idx, pid_seen_mask=pid_seen_mask\n",
    "        )\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf6f7b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29fa7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Regression-only KAN evaluation (standalone, with fixed time & group-wise splits)\n",
    "# =========================\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GroupKFold, ParameterGrid\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from scipy.stats import t as student_t\n",
    "\n",
    "# ---------- PCA helpers ----------\n",
    "_VAR_EPS = 1e-8\n",
    "_STD_EPS = 1e-6\n",
    "_CLIP_Z  = 8.0\n",
    "\n",
    "@dataclass\n",
    "class PCAPipeline:\n",
    "    keep_mask: np.ndarray\n",
    "    mean_: np.ndarray\n",
    "    scale_: np.ndarray\n",
    "    pca: PCA\n",
    "\n",
    "def _fit_pca_pipeline(X_train: np.ndarray, var_ratio: float = 0.95, random_state: Optional[int] = None) -> PCAPipeline:\n",
    "    X = np.asarray(X_train, dtype=np.float64)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    var = X.var(axis=0)\n",
    "    keep = var > _VAR_EPS\n",
    "    if not np.any(keep):\n",
    "        pca = PCA(n_components=0, svd_solver='full', random_state=random_state)\n",
    "        return PCAPipeline(keep_mask=keep, mean_=np.array([], dtype=np.float64),\n",
    "                           scale_=np.array([], dtype=np.float64), pca=pca)\n",
    "\n",
    "    Xk = X[:, keep]\n",
    "    mean = Xk.mean(axis=0)\n",
    "    std  = np.maximum(Xk.std(axis=0), _STD_EPS)\n",
    "\n",
    "    Z = (Xk - mean) / std\n",
    "    Z = np.nan_to_num(Z, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    np.clip(Z, -_CLIP_Z, _CLIP_Z, out=Z)\n",
    "\n",
    "    pca = PCA(n_components=var_ratio, svd_solver='full', random_state=random_state)\n",
    "    pca.fit(Z)\n",
    "    if not np.isfinite(pca.components_).all():\n",
    "        raise RuntimeError(\"PCA components contain non-finite values after fit.\")\n",
    "\n",
    "    return PCAPipeline(keep_mask=keep, mean_=mean, scale_=std, pca=pca)\n",
    "\n",
    "def _transform_pca_pipeline(pipe: Optional[PCAPipeline], X: Optional[np.ndarray]) -> Optional[np.ndarray]:\n",
    "    if pipe is None or X is None:\n",
    "        return None\n",
    "\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    if pipe.keep_mask.size == 0 or not np.any(pipe.keep_mask):\n",
    "        return np.zeros((X.shape[0], 0), dtype=np.float32)\n",
    "\n",
    "    Xk = X[:, pipe.keep_mask]\n",
    "    Z = (Xk - pipe.mean_) / pipe.scale_\n",
    "    Z = np.nan_to_num(Z, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    np.clip(Z, -_CLIP_Z, _CLIP_Z, out=Z)\n",
    "\n",
    "    Z64 = np.ascontiguousarray(Z, dtype=np.float64)\n",
    "    CT  = np.ascontiguousarray(pipe.pca.components_.T, dtype=np.float64)\n",
    "    with np.errstate(over='ignore', invalid='ignore', divide='ignore'):\n",
    "        Xt = Z64 @ CT\n",
    "    Xt = np.nan_to_num(Xt, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    return Xt\n",
    "\n",
    "def _concat_safe(*arrays: Optional[np.ndarray]) -> np.ndarray:\n",
    "    parts = [a for a in arrays if a is not None and a.size > 0]\n",
    "    if not parts:\n",
    "        return np.zeros((0, 0), dtype=np.float32)\n",
    "    return np.concatenate(parts, axis=1).astype(np.float32)\n",
    "\n",
    "# ---------- Regression metrics ----------\n",
    "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    if y_true.ndim == 1: y_true = y_true[:, None]\n",
    "    if y_pred.ndim == 1: y_pred = y_pred[:, None]\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    out = {}\n",
    "    for j in range(y_true.shape[1]):\n",
    "        t = y_true[:, j]; p = y_pred[:, j]\n",
    "        mse = float(np.mean((p - t) ** 2))\n",
    "        rmse = float(np.sqrt(mse))\n",
    "        mae = float(np.mean(np.abs(p - t)))\n",
    "        var = float(np.var(t))\n",
    "        r2 = float(1.0 - (mse / var)) if var > 0 else np.nan\n",
    "        if np.std(t) > 0 and np.std(p) > 0:\n",
    "            r = float(np.corrcoef(t, p)[0, 1])\n",
    "        else:\n",
    "            r = np.nan\n",
    "        out[f\"task_{j+1}_MSE\"] = mse\n",
    "        out[f\"task_{j+1}_RMSE\"] = rmse\n",
    "        out[f\"task_{j+1}_MAE\"] = mae\n",
    "        out[f\"task_{j+1}_R2\"] = r2\n",
    "        out[f\"task_{j+1}_PearsonR\"] = r\n",
    "    for k in (\"MSE\",\"RMSE\",\"MAE\",\"R2\",\"PearsonR\"):\n",
    "        vals = [out[f\"task_{j+1}_{k}\"] for j in range(y_true.shape[1])]\n",
    "        out[f\"macro_{k}\"] = float(np.nanmean(vals))\n",
    "    return out\n",
    "\n",
    "def _print_regression_metrics(metrics: dict, title: str = \"Test metrics\"):\n",
    "    print(f\"\\n{title}:\")\n",
    "    macro_keys = [k for k in metrics.keys() if k.startswith(\"macro_\")]\n",
    "    for k in sorted(macro_keys):\n",
    "        print(f\"{k:>16}: {metrics[k]:.6f}\")\n",
    "    task_indices = sorted({int(k.split('_')[1]) for k in metrics.keys() if k.startswith(\"task_\")})\n",
    "    for j in task_indices:\n",
    "        print(f\"task_{j}: \" +\n",
    "              \", \".join(f\"{m}={metrics.get(f'task_{j}_{m}', np.nan):.6f}\"\n",
    "                        for m in (\"MSE\",\"RMSE\",\"MAE\",\"R2\",\"PearsonR\")))\n",
    "\n",
    "# ---------- Split helpers ----------\n",
    "def _split_cases(pid_array, test_fraction=0.2, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    u = np.unique(pid_array)\n",
    "    te_ids = rng.choice(u, size=max(1, int(len(u)*test_fraction)), replace=False)\n",
    "    te_mask = np.isin(pid_array, te_ids)\n",
    "    return np.where(~te_mask)[0], np.where(te_mask)[0]\n",
    "\n",
    "def _make_train_val_split(idx_array: np.ndarray, seed: int, val_frac: float) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "    idx_array = np.asarray(idx_array)\n",
    "    if len(idx_array) <= 10 or val_frac <= 0.0:\n",
    "        return idx_array, None\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm = rng.permutation(len(idx_array))\n",
    "    cut = max(1, int(val_frac * len(idx_array)))\n",
    "    va_sel, tr_sel = perm[:cut], perm[cut:]\n",
    "    return idx_array[tr_sel], idx_array[va_sel]\n",
    "\n",
    "def _make_train_val_split_cases(idx_array: np.ndarray, pid_idx: np.ndarray, seed: int, val_frac: float) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "    \"\"\"Group-wise validation: pick whole participants into val.\"\"\"\n",
    "    idx_array = np.asarray(idx_array)\n",
    "    if len(idx_array) <= 10 or val_frac <= 0.0:\n",
    "        return idx_array, None\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pids_tr = pid_idx[idx_array]\n",
    "    uniq = np.unique(pids_tr)\n",
    "    n_val_ids = max(1, int(val_frac * len(uniq)))\n",
    "    val_ids = rng.choice(uniq, size=n_val_ids, replace=False)\n",
    "    is_val = np.isin(pids_tr, val_ids)\n",
    "    return idx_array[~is_val], idx_array[is_val]\n",
    "\n",
    "def _split_time_last_fraction_per_subject(pid_idx, time_ix, test_fraction=0.30, min_meas=4):\n",
    "    \"\"\"Forward split within subject: last fraction -> test, earlier -> train.\"\"\"\n",
    "    pid = np.asarray(pid_idx); t = np.asarray(time_ix)\n",
    "    tr, te = [], []\n",
    "    for p in np.unique(pid):\n",
    "        idx = np.where(pid == p)[0]\n",
    "        order = idx[np.argsort(t[idx])]\n",
    "        if len(order) < min_meas:\n",
    "            tr.extend(order.tolist())      # keep entirely in train\n",
    "            continue\n",
    "        k = max(1, int(np.ceil(test_fraction * len(order))))\n",
    "        te.extend(order[-k:].tolist())     # last 30% -> test\n",
    "        tr.extend(order[:-k].tolist())     # rest -> train\n",
    "    return np.array(tr, int), np.array(te, int)\n",
    "\n",
    "def _time_kfold_splits_by_subject_global(pid_idx, time_ix, n_splits=5, min_points=4, indices: Optional[np.ndarray] = None):\n",
    "    \"\"\"\n",
    "    Per-subject contiguous K-fold time splits.\n",
    "    Returns list of (train_idx, test_idx) GLOBAL indices.\n",
    "    Subjects with < max(min_points, n_splits) contribute all rows to TRAIN for all folds.\n",
    "    \"\"\"\n",
    "    pid_idx = np.asarray(pid_idx); time_ix = np.asarray(time_ix)\n",
    "    if indices is None:\n",
    "        indices = np.arange(len(pid_idx), dtype=int)\n",
    "    else:\n",
    "        indices = np.asarray(indices, dtype=int)\n",
    "\n",
    "    # group rows by pid inside 'indices'\n",
    "    by_pid = {}\n",
    "    for p in np.unique(pid_idx[indices]):\n",
    "        rows = indices[pid_idx[indices] == p]\n",
    "        by_pid[p] = rows[np.argsort(time_ix[rows])]\n",
    "\n",
    "    folds = []\n",
    "    for i in range(n_splits):\n",
    "        test_rows = []\n",
    "        train_rows = []\n",
    "        for p, rows in by_pid.items():\n",
    "            n_i = len(rows)\n",
    "            if n_i >= max(min_points, n_splits):\n",
    "                cuts = np.floor(np.linspace(0, n_i, n_splits + 1)).astype(int)\n",
    "                a, b = cuts[i], cuts[i+1]\n",
    "                if b > a:\n",
    "                    test_rows.extend(rows[a:b].tolist())\n",
    "                # the rest go to train\n",
    "                keep_train = np.r_[0:a, b:n_i]\n",
    "                if keep_train.size:\n",
    "                    train_rows.extend(rows[keep_train].tolist())\n",
    "            else:\n",
    "                # not enough points: use all as train for this fold\n",
    "                train_rows.extend(rows.tolist())\n",
    "\n",
    "        test_rows = np.array(sorted(set(test_rows)), dtype=int)\n",
    "        train_rows = np.array(sorted(set(train_rows)), dtype=int)\n",
    "        folds.append((train_rows, test_rows))\n",
    "    return folds\n",
    "\n",
    "def _filter_time_test_min_measurements(pid_idx: np.ndarray, test_idx: np.ndarray, min_meas: int = 3):\n",
    "    pid = np.asarray(pid_idx)\n",
    "    counts = {pid_val: np.sum(pid == pid_val) for pid_val in np.unique(pid)}\n",
    "    keep = [i for i in test_idx if counts.get(pid[i], 0) >= min_meas]\n",
    "    return np.array(keep, dtype=int)\n",
    "\n",
    "# ---------- Dataset / Loader ----------\n",
    "class _KANDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns: (X_fix, TC, Zrand, y, y_lags, dt_lags, pid_idx, pid_seen_mask)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_fix: np.ndarray,\n",
    "        TC: Optional[np.ndarray],\n",
    "        Zrand: Optional[np.ndarray],\n",
    "        y: np.ndarray,\n",
    "        y_lags: np.ndarray,\n",
    "        dt_lags: np.ndarray,\n",
    "        pid_idx: np.ndarray,\n",
    "        pid_seen_mask: np.ndarray,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        N = X_fix.shape[0]\n",
    "        assert y.shape[0] == N and y_lags.shape[0] == N and dt_lags.shape[0] == N and pid_idx.shape[0] == N\n",
    "\n",
    "        def _to_tensor(a, dtype=torch.float32):\n",
    "            if a is None:\n",
    "                return torch.zeros((N, 0), dtype=dtype, device=device)\n",
    "            return torch.as_tensor(a, dtype=dtype, device=device)\n",
    "\n",
    "        self.X_fix  = _to_tensor(X_fix, torch.float32)\n",
    "        self.TC     = _to_tensor(TC,    torch.float32)\n",
    "        self.Zrand  = _to_tensor(Zrand, torch.float32)\n",
    "        self.y      = _to_tensor(y,     torch.float32)\n",
    "        self.y_lags = _to_tensor(y_lags, torch.float32)\n",
    "        self.dt     = _to_tensor(dt_lags, torch.float32)\n",
    "        self.pid    = torch.as_tensor(pid_idx, dtype=torch.long, device=device)\n",
    "        self.seen   = torch.as_tensor(pid_seen_mask.astype(bool), dtype=torch.bool, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X_fix.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            self.X_fix[i], self.TC[i], self.Zrand[i],\n",
    "            self.y[i], self.y_lags[i], self.dt[i],\n",
    "            self.pid[i], self.seen[i]\n",
    "        )\n",
    "\n",
    "def _make_loader_kan(\n",
    "    X_fix, TC, Zrand, y, y_lags, dt_lags,\n",
    "    pid_idx, pid_seen_mask,\n",
    "    batch_size: int,\n",
    "    shuffle: bool,\n",
    "    device: torch.device\n",
    ") -> DataLoader:\n",
    "    ds = _KANDataset(X_fix, TC, Zrand, y, y_lags, dt_lags, pid_idx, pid_seen_mask, device)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, drop_last=False, num_workers=0)\n",
    "\n",
    "# ---------- Prepare split & loaders ----------\n",
    "def _prepare_split_and_loaders_kan(\n",
    "    X_only_fixed: np.ndarray,\n",
    "    X_fixed_and_random: Optional[np.ndarray],\n",
    "    y: np.ndarray,\n",
    "    y_lags: np.ndarray,\n",
    "    dt_lags: np.ndarray,\n",
    "    pid_idx_full: np.ndarray,\n",
    "    indices_train: np.ndarray,\n",
    "    indices_val: Optional[np.ndarray],\n",
    "    indices_test: np.ndarray,\n",
    "    scenario: str,\n",
    "    batch_size: int,\n",
    "    device: torch.device,\n",
    "    random_state: int = 42,\n",
    "    pca_var_ratio: float = 0.95,\n",
    "):\n",
    "    # PCA fit on TRAIN only\n",
    "    of_pipe = _fit_pca_pipeline(X_only_fixed[indices_train], var_ratio=pca_var_ratio, random_state=random_state)\n",
    "    fr_pipe = None\n",
    "    if X_fixed_and_random is not None and X_fixed_and_random.shape[1] > 0:\n",
    "        fr_pipe = _fit_pca_pipeline(X_fixed_and_random[indices_train], var_ratio=pca_var_ratio, random_state=random_state)\n",
    "\n",
    "    def transform_block(idxs):\n",
    "        of = _transform_pca_pipeline(of_pipe, X_only_fixed[idxs])                              # OF\n",
    "        fr = _transform_pca_pipeline(fr_pipe, None if X_fixed_and_random is None else X_fixed_and_random[idxs])  # FR\n",
    "        X_fix = _concat_safe(of, fr)   # FIXED = OF ⊕ FR\n",
    "        TC    = of                     # TC    = OF   (for FiLM input)\n",
    "        Zr    = fr                     # RANDOM = FR\n",
    "        return X_fix, TC, Zr\n",
    "\n",
    "    Xf_tr, TC_tr, Zr_tr = transform_block(indices_train)\n",
    "    Xf_te, TC_te, Zr_te = transform_block(indices_test)\n",
    "    if indices_val is not None:\n",
    "        Xf_va, TC_va, Zr_va = transform_block(indices_val)\n",
    "    else:\n",
    "        Xf_va = TC_va = Zr_va = None\n",
    "\n",
    "    # seen masks\n",
    "    if scenario == \"cases\":\n",
    "        seen_pids = set(pid_idx_full[indices_train].tolist())\n",
    "        seen_tr = np.ones(indices_train.shape[0], dtype=bool)\n",
    "        seen_va = np.ones(indices_val.shape[0], dtype=bool) if indices_val is not None else None\n",
    "        seen_te = np.array([p in seen_pids for p in pid_idx_full[indices_test]], dtype=bool)\n",
    "    else:  # \"time\" : within-subject splits -> all test pids are seen\n",
    "        seen_tr = np.ones(indices_train.shape[0], dtype=bool)\n",
    "        seen_va = np.ones(indices_val.shape[0], dtype=bool) if indices_val is not None else None\n",
    "        seen_te = np.ones(indices_test.shape[0], dtype=bool)\n",
    "\n",
    "    tr_loader = _make_loader_kan(\n",
    "        Xf_tr, TC_tr, Zr_tr, y[indices_train], y_lags[indices_train], dt_lags[indices_train],\n",
    "        pid_idx_full[indices_train], seen_tr, batch_size=batch_size, shuffle=True, device=device\n",
    "    )\n",
    "    va_loader = None\n",
    "    if indices_val is not None:\n",
    "        va_loader = _make_loader_kan(\n",
    "            Xf_va, TC_va, Zr_va, y[indices_val], y_lags[indices_val], dt_lags[indices_val],\n",
    "            pid_idx_full[indices_val], seen_va, batch_size=batch_size, shuffle=False, device=device\n",
    "        )\n",
    "    te_loader = _make_loader_kan(\n",
    "        Xf_te, TC_te, Zr_te, y[indices_test], y_lags[indices_test], dt_lags[indices_test],\n",
    "        pid_idx_full[indices_test], seen_te, batch_size=batch_size, shuffle=False, device=device\n",
    "    )\n",
    "\n",
    "    preprocessors = {\n",
    "        \"of_pipe\": of_pipe,\n",
    "        \"fr_pipe\": fr_pipe,\n",
    "        \"d_fix\":   Xf_tr.shape[1],                                           # dim(OF ⊕ FR)\n",
    "        \"d_tc\":    (Xf_tr.shape[1] - (0 if Zr_tr is None else Zr_tr.shape[1])),  # dim(OF)\n",
    "        \"d_zrand\": 0 if Zr_tr is None else Zr_tr.shape[1],                   # dim(FR)\n",
    "        \"n_ids\":   int(len(np.unique(pid_idx_full)))\n",
    "    }\n",
    "    loaders = {\"train\": tr_loader, \"val\": va_loader, \"test\": te_loader}\n",
    "    return preprocessors, loaders\n",
    "\n",
    "# ---------- Fit once + evaluate ----------\n",
    "def _fit_eval_once_kan(\n",
    "    build_model_fn, wrapper_cls,\n",
    "    arch_params: Dict[str, Any],\n",
    "    train_params: Dict[str, Any],\n",
    "    X_only_fixed: np.ndarray,\n",
    "    X_fixed_and_random: Optional[np.ndarray],\n",
    "    y: np.ndarray,\n",
    "    y_lags: np.ndarray,\n",
    "    dt_lags: np.ndarray,\n",
    "    pid_idx_full: np.ndarray,\n",
    "    tr_idx: np.ndarray,\n",
    "    va_idx: Optional[np.ndarray],\n",
    "    te_idx: np.ndarray,\n",
    "    device: torch.device,\n",
    "    scenario: str,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    preprocessors, loaders = _prepare_split_and_loaders_kan(\n",
    "        X_only_fixed, X_fixed_and_random, y, y_lags, dt_lags, pid_idx_full,\n",
    "        tr_idx, va_idx, te_idx, scenario,\n",
    "        batch_size=train_params.get(\"batch_size\", 256),\n",
    "        device=device,\n",
    "        random_state=train_params.get(\"random_state\", 42),\n",
    "        pca_var_ratio=train_params.get(\"pca_var_ratio\", 0.95),\n",
    "    )\n",
    "\n",
    "    y_dim = y.shape[1] if y.ndim == 2 else 1\n",
    "    model = build_model_fn(\n",
    "        y_dim=y_dim,\n",
    "        d_fix=preprocessors[\"d_fix\"],\n",
    "        d_tc=preprocessors[\"d_tc\"],\n",
    "        d_zrand=preprocessors[\"d_zrand\"],\n",
    "        n_ids=preprocessors[\"n_ids\"],\n",
    "        **arch_params\n",
    "    ).to(device)\n",
    "\n",
    "    wrapper = wrapper_cls(model, cfg=train_params, device=device)\n",
    "\n",
    "    # Train\n",
    "    wrapper.fit(loaders[\"train\"], loaders[\"val\"], verbose=verbose)\n",
    "\n",
    "    # Predict on TEST\n",
    "    model.eval()\n",
    "    preds_all, y_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xf_b, TC_b, Zr_b, y_b, yl_b, dt_b, pid_b, seen_b in loaders[\"test\"]:\n",
    "            y_hat = wrapper.predict(\n",
    "                X_fix=Xf_b,\n",
    "                TC=TC_b if TC_b.size(1) > 0 else None,\n",
    "                Zrand=Zr_b if Zr_b.size(1) > 0 else None,\n",
    "                y_lags=yl_b,\n",
    "                dt_lags=dt_b,\n",
    "                pid_idx=pid_b,\n",
    "                pid_seen_mask=seen_b\n",
    "            )\n",
    "            preds_all.append(y_hat.cpu().numpy())\n",
    "            y_all.append(y_b.cpu().numpy())\n",
    "\n",
    "    y_pred_te = np.vstack(preds_all)\n",
    "    y_true_te = np.vstack(y_all)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = regression_metrics(y_true_te, y_pred_te)\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"preprocessors\": preprocessors,\n",
    "        \"wrapper\": wrapper,\n",
    "        \"model\": wrapper.model,\n",
    "        \"y_true_test\": y_true_te,\n",
    "        \"y_pred_test\": y_pred_te,\n",
    "    }\n",
    "\n",
    "# ---------- CV summarizer ----------\n",
    "def _summarize_cv_folds(results_folds: List[Dict[str, float]]) -> Dict[str, float]:\n",
    "    if not results_folds:\n",
    "        return {}\n",
    "    all_keys = set().union(*results_folds)\n",
    "    summary = {}\n",
    "    for k in sorted(all_keys):\n",
    "        vals = np.array([fold.get(k, np.nan) for fold in results_folds], dtype=float)\n",
    "        mask = np.isfinite(vals); n = int(mask.sum())\n",
    "        if n == 0:\n",
    "            m = low = high = np.nan\n",
    "        elif n == 1:\n",
    "            m = float(vals[mask][0]); low = high = np.nan\n",
    "        else:\n",
    "            m = float(np.nanmean(vals)); s = float(np.nanstd(vals, ddof=1)); se = s/np.sqrt(n)\n",
    "            tcrit = float(student_t.ppf(0.975, df=n-1))\n",
    "            low, high = m - tcrit*se, m + tcrit*se\n",
    "        summary[f\"{k}_mean\"] = m\n",
    "        summary[f\"{k}_95ci_low\"] = low\n",
    "        summary[f\"{k}_95ci_high\"] = high\n",
    "    return summary\n",
    "\n",
    "# ---------- Public entry (single / cv_only / nested_cv) ----------\n",
    "def run_training_and_eval_kan(\n",
    "    X_only_fixed: np.ndarray,\n",
    "    X_fixed_and_random: Optional[np.ndarray],\n",
    "    y: np.ndarray,\n",
    "    y_lags: np.ndarray,\n",
    "    dt_lags: np.ndarray,\n",
    "    pid_idx: np.ndarray,\n",
    "    time_index: np.ndarray,\n",
    "    build_model_fn,               # callable(y_dim, d_fix, d_tc, d_zrand, n_ids, **arch)\n",
    "    wrapper_cls,                  # e.g., KANMixedEffectsRegWrapper\n",
    "    *,\n",
    "    mode: str = \"single\",         # \"single\" | \"cv_only\" | \"nested_cv\"\n",
    "    scenario: str = \"cases\",      # \"cases\" | \"time\"\n",
    "    outer_folds: int = 5,\n",
    "    inner_folds: int = 3,\n",
    "    param_grid: Optional[Dict[str, List]] = None,\n",
    "    arch_defaults: Optional[Dict[str, Any]] = None,\n",
    "    train_defaults: Optional[Dict[str, Any]] = None,\n",
    "    device: Optional[torch.device] = None,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    device = (\n",
    "        device\n",
    "        or (torch.device(\"mps\") if torch.backends.mps.is_available() else None)\n",
    "        or (torch.device(\"cuda\") if torch.cuda.is_available() else None)\n",
    "        or torch.device(\"cpu\")\n",
    "    )\n",
    "\n",
    "    # sanitize\n",
    "    X_of = np.asarray(X_only_fixed, dtype=np.float32)\n",
    "    X_fr = None if X_fixed_and_random is None else np.asarray(X_fixed_and_random, dtype=np.float32)\n",
    "    y    = np.asarray(y, dtype=np.float32)\n",
    "    if y.ndim == 1: y = y[:, None]\n",
    "    y_lags = np.asarray(y_lags, dtype=np.float32)\n",
    "    dt_lags = np.asarray(dt_lags, dtype=np.float32)\n",
    "    pid_idx = np.asarray(pid_idx, dtype=np.int64)\n",
    "    time_ix = np.asarray(time_index)\n",
    "\n",
    "    arch_defaults = arch_defaults or {}\n",
    "    train_defaults = train_defaults or {}\n",
    "    rnd = int(train_defaults.get(\"random_state\", 42))\n",
    "    val_frac = float(train_defaults.get(\"val_fraction\", 0.10))\n",
    "    min_meas = int(train_defaults.get(\"min_meas_test\", 4))\n",
    "    time_test_frac = float(train_defaults.get(\"time_test_fraction\", 0.30))\n",
    "\n",
    "    # -------------------- SINGLE --------------------\n",
    "    if mode == \"single\":\n",
    "        if scenario == \"cases\":\n",
    "            tr_idx_all, te_idx = _split_cases(pid_idx, test_fraction=0.2, seed=rnd)\n",
    "            # group-wise validation (no participant leakage)\n",
    "            tr_idx, va_idx = _make_train_val_split_cases(tr_idx_all, pid_idx, seed=rnd, val_frac=val_frac)\n",
    "        elif scenario == \"time\":\n",
    "            tr_idx, te_idx = _split_time_last_fraction_per_subject(pid_idx, time_ix, test_fraction=time_test_frac, min_meas=min_meas)\n",
    "            # plain row-wise val inside train (you can replace with time-aware if desired)\n",
    "            tr_idx, va_idx = _make_train_val_split(tr_idx, seed=rnd, val_frac=val_frac)\n",
    "        else:\n",
    "            raise ValueError(\"scenario must be 'cases' or 'time'\")\n",
    "\n",
    "        res = _fit_eval_once_kan(\n",
    "            build_model_fn, wrapper_cls,\n",
    "            arch_defaults, train_defaults,\n",
    "            X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "            tr_idx, va_idx, te_idx,\n",
    "            device=device, scenario=scenario,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            _print_regression_metrics(res[\"metrics\"], title=\"Single-fit test metrics\")\n",
    "        return res\n",
    "\n",
    "    # -------------------- CV-ONLY --------------------\n",
    "    if mode == \"cv_only\":\n",
    "        fold_metrics: List[Dict[str, float]] = []\n",
    "\n",
    "        if scenario == \"cases\":\n",
    "            outer = GroupKFold(n_splits=outer_folds)\n",
    "            outer_iter = outer.split(X_of, y[:, 0], groups=pid_idx)\n",
    "\n",
    "            for fold_id, (tr_idx_all, te_idx) in enumerate(outer_iter, start=1):\n",
    "                # group-wise val inside the train fold\n",
    "                tr_idx, va_idx = _make_train_val_split_cases(np.asarray(tr_idx_all), pid_idx, seed=rnd + fold_id, val_frac=val_frac)\n",
    "                res = _fit_eval_once_kan(\n",
    "                    build_model_fn, wrapper_cls,\n",
    "                    arch_defaults, train_defaults,\n",
    "                    X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                    tr_idx, va_idx, te_idx,\n",
    "                    device=device, scenario=scenario,\n",
    "                    verbose=False\n",
    "                )\n",
    "                fold_metrics.append(res[\"metrics\"])\n",
    "                if verbose:\n",
    "                    print(f\"\\nFold {fold_id}:\")\n",
    "                    _print_regression_metrics(res[\"metrics\"], title=\"Per-fold test metrics\")\n",
    "\n",
    "        else:  # scenario == \"time\"\n",
    "            folds = _time_kfold_splits_by_subject_global(pid_idx, time_ix, n_splits=outer_folds, min_points=min_meas, indices=None)\n",
    "            for fold_id, (tr_idx_all, te_idx) in enumerate(folds, start=1):\n",
    "                # row-wise val inside the train fold (can be swapped for time-aware val if desired)\n",
    "                tr_idx, va_idx = _make_train_val_split(np.asarray(tr_idx_all), seed=rnd + fold_id, val_frac=val_frac)\n",
    "                if te_idx.size == 0 or tr_idx.size == 0:\n",
    "                    if verbose:\n",
    "                        print(f\"Fold {fold_id}: skipped (empty train/test).\")\n",
    "                    continue\n",
    "                res = _fit_eval_once_kan(\n",
    "                    build_model_fn, wrapper_cls,\n",
    "                    arch_defaults, train_defaults,\n",
    "                    X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                    tr_idx, va_idx, te_idx,\n",
    "                    device=device, scenario=scenario,\n",
    "                    verbose=False\n",
    "                )\n",
    "                fold_metrics.append(res[\"metrics\"])\n",
    "                if verbose:\n",
    "                    print(f\"\\nFold {fold_id}:\")\n",
    "                    _print_regression_metrics(res[\"metrics\"], title=\"Per-fold test metrics\")\n",
    "\n",
    "        cv_summary = _summarize_cv_folds(fold_metrics)\n",
    "        if verbose:\n",
    "            print(\"\\nCV averages (±95% CI):\")\n",
    "            for key in sorted(cv_summary.keys()):\n",
    "                if key.endswith(\"_mean\"):\n",
    "                    base = key[:-5]\n",
    "                    low = cv_summary.get(f\"{base}_95ci_low\", np.nan)\n",
    "                    high = cv_summary.get(f\"{base}_95ci_high\", np.nan)\n",
    "                    print(f\"{base:>20}: {cv_summary[key]:.6f}  (95% CI {low:.6f}, {high:.6f})\")\n",
    "\n",
    "        return {\n",
    "            \"cv_folds_metrics\": fold_metrics,\n",
    "            \"cv_summary\": cv_summary,\n",
    "        }\n",
    "\n",
    "    # -------------------- NESTED CV --------------------\n",
    "    if mode == \"nested_cv\":\n",
    "        if not param_grid:\n",
    "            param_grid = {\n",
    "                \"d_fix_latent\": [128, 256],\n",
    "                \"d_rand_latent\": [128],\n",
    "                \"n_kernels\": [4, 8],\n",
    "                \"dropout\": [0.0, 0.1],\n",
    "                \"lr\": [1e-3, 3e-4],\n",
    "                \"weight_decay\": [0.0, 1e-4],\n",
    "                \"batch_size\": [256],\n",
    "                \"max_epochs\": [100],\n",
    "                \"patience\": [10],\n",
    "            }\n",
    "\n",
    "        results_folds = []\n",
    "        best_score_global, best_params_global = -np.inf, None\n",
    "\n",
    "        # Outer iterator\n",
    "        if scenario == \"cases\":\n",
    "            outer = GroupKFold(n_splits=outer_folds)\n",
    "            outer_iter = list(outer.split(X_of, y[:, 0], groups=pid_idx))\n",
    "        else:\n",
    "            outer_iter = _time_kfold_splits_by_subject_global(pid_idx, time_ix, n_splits=outer_folds, min_points=min_meas, indices=None)\n",
    "\n",
    "        for fold_id, (tr_idx_all, te_idx) in enumerate(outer_iter, start=1):\n",
    "            if verbose:\n",
    "                print(f\"\\nOuter fold {fold_id}/{outer_folds}\")\n",
    "\n",
    "            # ----- INNER CV: select by macro_R2 (higher is better) -----\n",
    "            def inner_iter():\n",
    "                if scenario == \"cases\":\n",
    "                    inner = GroupKFold(n_splits=inner_folds)\n",
    "                    return list(inner.split(X_of[tr_idx_all], y[tr_idx_all, 0], groups=pid_idx[tr_idx_all]))\n",
    "                else:\n",
    "                    # per-subject time folds restricted to outer-train rows\n",
    "                    return _time_kfold_splits_by_subject_global(\n",
    "                        pid_idx, time_ix, n_splits=inner_folds, min_points=min_meas, indices=np.asarray(tr_idx_all)\n",
    "                    )\n",
    "\n",
    "            best_inner_score, best_inner_params = -np.inf, None\n",
    "            for params in ParameterGrid(param_grid):\n",
    "                arch_params = dict(arch_defaults)\n",
    "                train_params = dict(train_defaults)\n",
    "                for k, v in params.items():\n",
    "                    if k in (\"d_fix_latent\", \"d_rand_latent\", \"n_kernels\", \"dropout\"):\n",
    "                        arch_params[k] = v\n",
    "                    else:\n",
    "                        train_params[k] = v\n",
    "\n",
    "                inner_scores = []\n",
    "                for in_tr, in_va in inner_iter():\n",
    "                    # add group-wise val ONLY for 'cases' (no leakage)\n",
    "                    if scenario == \"cases\":\n",
    "                        tr_idx_inner, va_idx_inner = _make_train_val_split_cases(np.asarray(in_tr), pid_idx, seed=rnd + fold_id, val_frac=val_frac)\n",
    "                    else:\n",
    "                        tr_idx_inner, va_idx_inner = _make_train_val_split(np.asarray(in_tr), seed=rnd + fold_id, val_frac=val_frac)\n",
    "\n",
    "                    res_inner = _fit_eval_once_kan(\n",
    "                        build_model_fn, wrapper_cls,\n",
    "                        arch_params, train_params,\n",
    "                        X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                        tr_idx_inner, va_idx_inner, np.asarray(in_va),\n",
    "                        device=device, scenario=scenario,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    score = res_inner[\"metrics\"].get(\"macro_R2\", np.nan)\n",
    "                    inner_scores.append(score)\n",
    "\n",
    "                avg_score = float(np.nanmean(inner_scores)) if len(inner_scores) else -np.inf\n",
    "                if avg_score > best_inner_score:\n",
    "                    best_inner_score = avg_score\n",
    "                    best_inner_params = (arch_params, train_params)\n",
    "\n",
    "            if best_inner_params is None:\n",
    "                if verbose: print(\"No viable inner config; skipping outer fold.\")\n",
    "                continue\n",
    "\n",
    "            arch_params, train_params = best_inner_params\n",
    "\n",
    "            # final train/val split inside outer-train for the final fit before testing\n",
    "            if scenario == \"cases\":\n",
    "                tr_idx_outer, va_idx_outer = _make_train_val_split_cases(np.asarray(tr_idx_all), pid_idx, seed=rnd + fold_id * 17, val_frac=val_frac)\n",
    "            else:\n",
    "                tr_idx_outer, va_idx_outer = _make_train_val_split(np.asarray(tr_idx_all), seed=rnd + fold_id * 17, val_frac=val_frac)\n",
    "\n",
    "            res_outer = _fit_eval_once_kan(\n",
    "                build_model_fn, wrapper_cls,\n",
    "                arch_params, train_params,\n",
    "                X_of, X_fr, y, y_lags, dt_lags, pid_idx,\n",
    "                tr_idx_outer, va_idx_outer, np.asarray(te_idx),\n",
    "                device=device, scenario=scenario,\n",
    "                verbose=False\n",
    "            )\n",
    "            results_folds.append(res_outer)\n",
    "\n",
    "            score_outer = res_outer[\"metrics\"].get(\"macro_R2\", -np.inf)\n",
    "            if score_outer > best_score_global:\n",
    "                best_score_global = score_outer\n",
    "                best_params_global = (arch_params, train_params)\n",
    "\n",
    "            if verbose:\n",
    "                _print_regression_metrics(res_outer[\"metrics\"], title=\"Outer fold test metrics\")\n",
    "\n",
    "        # summarize folds\n",
    "        def _summarize_block(results_list: List[Dict[str, Any]], which: str = \"metrics\") -> Dict[str, float]:\n",
    "            keys = list(results_list[0][which].keys())\n",
    "            out = {}\n",
    "            for k in keys:\n",
    "                arr = np.array([res[which][k] for res in results_list], dtype=float)\n",
    "                m = float(np.nanmean(arr)); s = float(np.nanstd(arr, ddof=1)); n = len(arr)\n",
    "                se = s / np.sqrt(n) if n > 1 else np.nan\n",
    "                if n > 1:\n",
    "                    tcrit = float(student_t.ppf(0.975, df=n-1))\n",
    "                    ci = (m - tcrit * se, m + tcrit * se)\n",
    "                else:\n",
    "                    ci = (np.nan, np.nan)\n",
    "                out[k + \"_mean\"] = m\n",
    "                out[k + \"_95ci_low\"] = ci[0]\n",
    "                out[k + \"_95ci_high\"] = ci[1]\n",
    "            return out\n",
    "\n",
    "        cv_summary = _summarize_block(results_folds, \"metrics\")\n",
    "\n",
    "        if verbose and best_params_global is not None:\n",
    "            print(\"\\nBest params (by outer macro_R2):\")\n",
    "            arch_p, train_p = best_params_global\n",
    "            print(\"[ARCH]:\");   [print(f\"  {k}: {v}\") for k, v in arch_p.items()]\n",
    "            print(\"[TRAIN]:\");  [print(f\"  {k}: {v}\") for k, v in train_p.items()]\n",
    "        \n",
    "        print(\"\\nCross-validation results:\")\n",
    "        print(\"CV Summary:\")\n",
    "        print(cv_summary)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"outer_folds\": results_folds,\n",
    "            \"cv_summary\": cv_summary,\n",
    "            \"best_params\": (best_params_global[0], best_params_global[1]) if best_params_global else None,\n",
    "        }\n",
    "\n",
    "    raise ValueError(\"mode must be one of {'single','cv_only','nested_cv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ef43a",
   "metadata": {},
   "source": [
    "## Model test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a6c308",
   "metadata": {},
   "source": [
    "### Define variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35da766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ---------- targets & aux ----------\n",
    "y_raw      = EXP_reg_y.to_numpy(np.float32)\n",
    "y_np       = y_raw if y_raw.ndim == 2 else y_raw.reshape(-1, 1)\n",
    "\n",
    "y_lags_np  = EXP_reg_outcomes_lags.to_numpy(np.float32)\n",
    "dt_lags_np = EXP_reg_forecast_horizons.to_numpy(np.float32)\n",
    "\n",
    "# ---------- inputs ----------\n",
    "# keep the two blocks SEPARATE; evaluator will:\n",
    "#   - TC  <- only_fixed (after PCA)\n",
    "#   - X_fix <- concat(only_fixed_PCA, fixed_and_random_PCA)\n",
    "#   - Zrand <- fixed_and_random_PCA\n",
    "X_only_fixed_np        = EXP_reg_only_fixed.to_numpy(np.float32)\n",
    "X_fixed_and_random_np  = EXP_reg_fixed_and_random.to_numpy(np.float32)\n",
    "\n",
    "# ---------- ids & time ----------\n",
    "pid_raw    = EXP_reg_participant_id.to_numpy().ravel()\n",
    "pid_uniqs, pid_encoded = np.unique(pid_raw, return_inverse=True)\n",
    "pid_np     = pid_encoded.astype(np.int64)\n",
    "n_ids      = int(len(pid_uniqs))\n",
    "\n",
    "time_ix_np = EXP_reg_time.to_numpy().ravel()\n",
    "\n",
    "def build_model_fn(\n",
    "    *,\n",
    "    y_dim: int,\n",
    "    d_fix: int,\n",
    "    d_tc: int,      # PCA(only_fixed)\n",
    "    d_zrand: int,   # PCA(fixed_and_random)\n",
    "    n_ids: int,\n",
    "    **arch\n",
    "):\n",
    "    return KANAdditiveMixedEffects(\n",
    "        y_dim=y_dim,\n",
    "        d_fix=d_fix,     # concat(only_fixed_PCA, fixed_and_random_PCA)\n",
    "        d_tc=d_tc,       # only_fixed_PCA -> FiLM\n",
    "        d_zrand=d_zrand, # fixed_and_random_PCA -> random branch\n",
    "        n_ids=n_ids,\n",
    "        **arch\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "arch_defaults = dict(d_fix_latent=256, d_rand_latent=256, n_kernels=16, dropout=0.0)\n",
    "train_defaults = dict(\n",
    "    lr=3e-3, weight_decay=1e-5, batch_size=64,\n",
    "    max_epochs=100, patience=10, threshold=0.5,\n",
    "    lambda_mean0=1e-5, lambda_ridge=1e-5,\n",
    "    lambda_orth_latent=1e-5,\n",
    "    lambda_film_identity=1e-5,\n",
    "    lambda_kan=0,\n",
    "    random_state=42,\n",
    "    val_fraction=0.2, min_meas_test=3,\n",
    "    loss=\"mse\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba72c8e",
   "metadata": {},
   "source": [
    "### Simple cases split test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8348913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train 13.396842 | val 10.651239\n",
      "Epoch 002 | train 10.540825 | val 10.287268\n",
      "Epoch 003 | train 9.591392 | val 9.838263\n",
      "Epoch 004 | train 9.160029 | val 10.329488\n",
      "Epoch 005 | train 7.787717 | val 11.320188\n",
      "Epoch 006 | train 5.915417 | val 11.911542\n",
      "Epoch 007 | train 4.604090 | val 12.951723\n",
      "Epoch 008 | train 3.710806 | val 13.314583\n",
      "Epoch 009 | train 3.165755 | val 13.397321\n",
      "Epoch 010 | train 2.560015 | val 15.683063\n",
      "Epoch 011 | train 2.171884 | val 12.801417\n",
      "Epoch 012 | train 1.846762 | val 13.514762\n",
      "Epoch 013 | train 1.642524 | val 13.121567\n",
      "Early stopping at epoch 013 (best val 9.838263)\n",
      "\n",
      "Single-fit test metrics:\n",
      "       macro_MAE: 2.186242\n",
      "       macro_MSE: 10.216752\n",
      "  macro_PearsonR: 0.423506\n",
      "        macro_R2: 0.158055\n",
      "      macro_RMSE: 3.196365\n",
      "task_1: MSE=10.216752, RMSE=3.196365, MAE=2.186242, R2=0.158055, PearsonR=0.423506\n"
     ]
    }
   ],
   "source": [
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"single\",\n",
    "    scenario=\"cases\",\n",
    "    train_defaults=train_defaults\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664dd4b",
   "metadata": {},
   "source": [
    "### Simple time split test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9edf5e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train 15.191086 | val 10.176481\n",
      "Epoch 002 | train 11.078441 | val 9.775215\n",
      "Epoch 003 | train 10.360999 | val 9.474136\n",
      "Epoch 004 | train 9.841178 | val 9.219025\n",
      "Epoch 005 | train 8.132365 | val 9.806080\n",
      "Epoch 006 | train 6.187256 | val 9.917340\n",
      "Epoch 007 | train 4.364584 | val 11.813448\n",
      "Epoch 008 | train 3.225908 | val 10.781802\n",
      "Epoch 009 | train 2.544779 | val 11.750495\n",
      "Epoch 010 | train 2.147396 | val 12.068660\n",
      "Epoch 011 | train 1.714293 | val 11.302686\n",
      "Epoch 012 | train 1.449885 | val 11.919052\n",
      "Epoch 013 | train 1.283006 | val 11.595913\n",
      "Epoch 014 | train 1.236620 | val 11.143318\n",
      "Early stopping at epoch 014 (best val 9.219025)\n",
      "\n",
      "Single-fit test metrics:\n",
      "       macro_MAE: 2.200420\n",
      "       macro_MSE: 10.739383\n",
      "  macro_PearsonR: 0.594772\n",
      "        macro_R2: 0.311435\n",
      "      macro_RMSE: 3.277100\n",
      "task_1: MSE=10.739383, RMSE=3.277100, MAE=2.200420, R2=0.311435, PearsonR=0.594772\n"
     ]
    }
   ],
   "source": [
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"single\",\n",
    "    scenario=\"time\",\n",
    "    train_defaults=train_defaults,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea479b91",
   "metadata": {},
   "source": [
    "### Cases split test CV without parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11b8a291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.529446\n",
      "       macro_MSE: 11.403911\n",
      "  macro_PearsonR: 0.585850\n",
      "        macro_R2: 0.314946\n",
      "      macro_RMSE: 3.376968\n",
      "task_1: MSE=11.403911, RMSE=3.376968, MAE=2.529446, R2=0.314946, PearsonR=0.585850\n",
      "\n",
      "Fold 2:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.665854\n",
      "       macro_MSE: 12.105760\n",
      "  macro_PearsonR: 0.694971\n",
      "        macro_R2: 0.454990\n",
      "      macro_RMSE: 3.479333\n",
      "task_1: MSE=12.105760, RMSE=3.479333, MAE=2.665854, R2=0.454990, PearsonR=0.694971\n",
      "\n",
      "Fold 3:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.360177\n",
      "       macro_MSE: 10.981968\n",
      "  macro_PearsonR: 0.478410\n",
      "        macro_R2: 0.213245\n",
      "      macro_RMSE: 3.313905\n",
      "task_1: MSE=10.981968, RMSE=3.313905, MAE=2.360177, R2=0.213245, PearsonR=0.478410\n",
      "\n",
      "Fold 4:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.485425\n",
      "       macro_MSE: 11.500859\n",
      "  macro_PearsonR: 0.522263\n",
      "        macro_R2: 0.264160\n",
      "      macro_RMSE: 3.391292\n",
      "task_1: MSE=11.500859, RMSE=3.391292, MAE=2.485425, R2=0.264160, PearsonR=0.522263\n",
      "\n",
      "Fold 5:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.363382\n",
      "       macro_MSE: 10.318064\n",
      "  macro_PearsonR: 0.327577\n",
      "        macro_R2: 0.055417\n",
      "      macro_RMSE: 3.212174\n",
      "task_1: MSE=10.318064, RMSE=3.212174, MAE=2.363382, R2=0.055417, PearsonR=0.327577\n",
      "\n",
      "CV averages (±95% CI):\n",
      "           macro_MAE: 2.480857  (95% CI 2.322612, 2.639102)\n",
      "           macro_MSE: 11.262112  (95% CI 10.438823, 12.085401)\n",
      "      macro_PearsonR: 0.521814  (95% CI 0.353276, 0.690351)\n",
      "            macro_R2: 0.260551  (95% CI 0.079447, 0.441656)\n",
      "          macro_RMSE: 3.354734  (95% CI 3.231587, 3.477882)\n",
      "          task_1_MAE: 2.480857  (95% CI 2.322612, 2.639102)\n",
      "          task_1_MSE: 11.262112  (95% CI 10.438823, 12.085401)\n",
      "     task_1_PearsonR: 0.521814  (95% CI 0.353276, 0.690351)\n",
      "           task_1_R2: 0.260551  (95% CI 0.079447, 0.441656)\n",
      "         task_1_RMSE: 3.354734  (95% CI 3.231587, 3.477882)\n"
     ]
    }
   ],
   "source": [
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"cv_only\",\n",
    "    scenario=\"cases\",\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    outer_folds=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8ef24",
   "metadata": {},
   "source": [
    "### Time split test CV without parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f565faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.374766\n",
      "       macro_MSE: 10.984379\n",
      "  macro_PearsonR: 0.577343\n",
      "        macro_R2: 0.331923\n",
      "      macro_RMSE: 3.314269\n",
      "task_1: MSE=10.984379, RMSE=3.314269, MAE=2.374766, R2=0.331923, PearsonR=0.577343\n",
      "\n",
      "Fold 2:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.419275\n",
      "       macro_MSE: 11.041670\n",
      "  macro_PearsonR: 0.607669\n",
      "        macro_R2: 0.362640\n",
      "      macro_RMSE: 3.322901\n",
      "task_1: MSE=11.041670, RMSE=3.322901, MAE=2.419275, R2=0.362640, PearsonR=0.607669\n",
      "\n",
      "Fold 3:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.319470\n",
      "       macro_MSE: 10.507090\n",
      "  macro_PearsonR: 0.592105\n",
      "        macro_R2: 0.346858\n",
      "      macro_RMSE: 3.241464\n",
      "task_1: MSE=10.507090, RMSE=3.241464, MAE=2.319470, R2=0.346858, PearsonR=0.592105\n",
      "\n",
      "Fold 4:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.160115\n",
      "       macro_MSE: 9.371600\n",
      "  macro_PearsonR: 0.596119\n",
      "        macro_R2: 0.336463\n",
      "      macro_RMSE: 3.061307\n",
      "task_1: MSE=9.371600, RMSE=3.061307, MAE=2.160115, R2=0.336463, PearsonR=0.596119\n",
      "\n",
      "Fold 5:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.337335\n",
      "       macro_MSE: 10.711971\n",
      "  macro_PearsonR: 0.618375\n",
      "        macro_R2: 0.340953\n",
      "      macro_RMSE: 3.272915\n",
      "task_1: MSE=10.711971, RMSE=3.272915, MAE=2.337335, R2=0.340953, PearsonR=0.618375\n",
      "\n",
      "CV averages (±95% CI):\n",
      "           macro_MAE: 2.322192  (95% CI 2.200069, 2.444316)\n",
      "           macro_MSE: 10.523342  (95% CI 9.680387, 11.366297)\n",
      "      macro_PearsonR: 0.598322  (95% CI 0.578956, 0.617688)\n",
      "            macro_R2: 0.343767  (95% CI 0.328981, 0.358554)\n",
      "          macro_RMSE: 3.242571  (95% CI 3.110329, 3.374813)\n",
      "          task_1_MAE: 2.322192  (95% CI 2.200069, 2.444316)\n",
      "          task_1_MSE: 10.523342  (95% CI 9.680387, 11.366297)\n",
      "     task_1_PearsonR: 0.598322  (95% CI 0.578956, 0.617688)\n",
      "           task_1_R2: 0.343767  (95% CI 0.328981, 0.358554)\n",
      "         task_1_RMSE: 3.242571  (95% CI 3.110329, 3.374813)\n"
     ]
    }
   ],
   "source": [
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"cv_only\",\n",
    "    scenario=\"time\",\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    outer_folds=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef6057",
   "metadata": {},
   "source": [
    "### Time split test CV WITH parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outer fold 1/5\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "\n",
    "    \"d_fix_latent\": [64, 128],\n",
    "    \"d_rand_latent\": [64, 128],\n",
    "    \"n_kernels\": [4, 8],\n",
    "    \"dropout\": [0.0, 0.1, 0.2],\n",
    "\n",
    "    \"lr\": [1e-3, 3e-4],\n",
    "    \"weight_decay\": [0.0, 1e-4],\n",
    "    \"batch_size\": [256],\n",
    "    \"max_epochs\": [120],\n",
    "    \"patience\": [12],\n",
    "    \"lambda_contrast\": [1e-2, 5e-3, 2e-3],\n",
    "    \"contrast_temp\": [0.2, 0.1],\n",
    "    \"lambda_orth\": [1e-3, 5e-4],\n",
    "    \"lambda_mean0\": [1e-3],\n",
    "    \"lambda_ridge\": [1e-4, 5e-4],\n",
    "    \"lambda_kan\": [0.0, 1e-4],\n",
    "}\n",
    "\n",
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"nested_cv\",\n",
    "    scenario=\"time\",\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    outer_folds=5,\n",
    "    inner_folds=2,\n",
    "    param_grid=param_grid,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031db303",
   "metadata": {},
   "source": [
    "### Cases split test CV WITH parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc49eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"nested_cv\",\n",
    "    scenario=\"cases\",\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    outer_folds=5,\n",
    "    inner_folds=2,\n",
    "    param_grid=param_grid,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41973dac",
   "metadata": {},
   "source": [
    "### Abblation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ed1da668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.388597\n",
      "       macro_MSE: 11.249486\n",
      "  macro_PearsonR: 0.570898\n",
      "        macro_R2: 0.324222\n",
      "      macro_RMSE: 3.354025\n",
      "task_1: MSE=11.249486, RMSE=3.354025, MAE=2.388597, R2=0.324222, PearsonR=0.570898\n",
      "\n",
      "Fold 2:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.668198\n",
      "       macro_MSE: 12.413414\n",
      "  macro_PearsonR: 0.707859\n",
      "        macro_R2: 0.441139\n",
      "      macro_RMSE: 3.523268\n",
      "task_1: MSE=12.413414, RMSE=3.523268, MAE=2.668198, R2=0.441139, PearsonR=0.707859\n",
      "\n",
      "Fold 3:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.338246\n",
      "       macro_MSE: 11.083455\n",
      "  macro_PearsonR: 0.481935\n",
      "        macro_R2: 0.205974\n",
      "      macro_RMSE: 3.329182\n",
      "task_1: MSE=11.083455, RMSE=3.329182, MAE=2.338246, R2=0.205974, PearsonR=0.481935\n",
      "\n",
      "Fold 4:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.424448\n",
      "       macro_MSE: 11.718911\n",
      "  macro_PearsonR: 0.504823\n",
      "        macro_R2: 0.250208\n",
      "      macro_RMSE: 3.423290\n",
      "task_1: MSE=11.718911, RMSE=3.423290, MAE=2.424448, R2=0.250208, PearsonR=0.504823\n",
      "\n",
      "Fold 5:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.509053\n",
      "       macro_MSE: 12.089453\n",
      "  macro_PearsonR: 0.212602\n",
      "        macro_R2: -0.106747\n",
      "      macro_RMSE: 3.476989\n",
      "task_1: MSE=12.089453, RMSE=3.476989, MAE=2.509053, R2=-0.106747, PearsonR=0.212602\n",
      "\n",
      "CV averages (±95% CI):\n",
      "           macro_MAE: 2.465708  (95% CI 2.305278, 2.626138)\n",
      "           macro_MSE: 11.710944  (95% CI 11.018644, 12.403243)\n",
      "      macro_PearsonR: 0.495623  (95% CI 0.270848, 0.720398)\n",
      "            macro_R2: 0.222959  (95% CI -0.031199, 0.477117)\n",
      "          macro_RMSE: 3.421351  (95% CI 3.320260, 3.522442)\n",
      "          task_1_MAE: 2.465708  (95% CI 2.305278, 2.626138)\n",
      "          task_1_MSE: 11.710944  (95% CI 11.018644, 12.403243)\n",
      "     task_1_PearsonR: 0.495623  (95% CI 0.270848, 0.720398)\n",
      "           task_1_R2: 0.222959  (95% CI -0.031199, 0.477117)\n",
      "         task_1_RMSE: 3.421351  (95% CI 3.320260, 3.522442)\n"
     ]
    }
   ],
   "source": [
    "arch_defaults = dict(d_fix_latent=256, d_rand_latent=256, n_kernels=16, dropout=0.0, use_attention=True, use_random = False)\n",
    "\n",
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"cv_only\",\n",
    "    scenario=\"cases\",\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    outer_folds=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ab0f13cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.323098\n",
      "       macro_MSE: 11.131522\n",
      "  macro_PearsonR: 0.571979\n",
      "        macro_R2: 0.322973\n",
      "      macro_RMSE: 3.336394\n",
      "task_1: MSE=11.131522, RMSE=3.336394, MAE=2.323098, R2=0.322973, PearsonR=0.571979\n",
      "\n",
      "Fold 2:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.363097\n",
      "       macro_MSE: 11.013583\n",
      "  macro_PearsonR: 0.605279\n",
      "        macro_R2: 0.364261\n",
      "      macro_RMSE: 3.318672\n",
      "task_1: MSE=11.013583, RMSE=3.318672, MAE=2.363097, R2=0.364261, PearsonR=0.605279\n",
      "\n",
      "Fold 3:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.250372\n",
      "       macro_MSE: 10.605227\n",
      "  macro_PearsonR: 0.592987\n",
      "        macro_R2: 0.340757\n",
      "      macro_RMSE: 3.256567\n",
      "task_1: MSE=10.605227, RMSE=3.256567, MAE=2.250372, R2=0.340757, PearsonR=0.592987\n",
      "\n",
      "Fold 4:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.120395\n",
      "       macro_MSE: 9.421944\n",
      "  macro_PearsonR: 0.583765\n",
      "        macro_R2: 0.332899\n",
      "      macro_RMSE: 3.069518\n",
      "task_1: MSE=9.421944, RMSE=3.069518, MAE=2.120395, R2=0.332899, PearsonR=0.583765\n",
      "\n",
      "Fold 5:\n",
      "\n",
      "Per-fold test metrics:\n",
      "       macro_MAE: 2.481628\n",
      "       macro_MSE: 10.715067\n",
      "  macro_PearsonR: 0.599127\n",
      "        macro_R2: 0.340763\n",
      "      macro_RMSE: 3.273388\n",
      "task_1: MSE=10.715067, RMSE=3.273388, MAE=2.481628, R2=0.340763, PearsonR=0.599127\n",
      "\n",
      "CV averages (±95% CI):\n",
      "           macro_MAE: 2.307718  (95% CI 2.141216, 2.474220)\n",
      "           macro_MSE: 10.577469  (95% CI 9.732564, 11.422374)\n",
      "      macro_PearsonR: 0.590627  (95% CI 0.574351, 0.606904)\n",
      "            macro_R2: 0.340331  (95% CI 0.321403, 0.359259)\n",
      "          macro_RMSE: 3.250908  (95% CI 3.118714, 3.383102)\n",
      "          task_1_MAE: 2.307718  (95% CI 2.141216, 2.474220)\n",
      "          task_1_MSE: 10.577469  (95% CI 9.732564, 11.422374)\n",
      "     task_1_PearsonR: 0.590627  (95% CI 0.574351, 0.606904)\n",
      "           task_1_R2: 0.340331  (95% CI 0.321403, 0.359259)\n",
      "         task_1_RMSE: 3.250908  (95% CI 3.118714, 3.383102)\n"
     ]
    }
   ],
   "source": [
    "arch_defaults = dict(d_fix_latent=256, d_rand_latent=256, n_kernels=16, dropout=0.0, use_attention=False, use_random = True)\n",
    "\n",
    "results = run_training_and_eval_kan(\n",
    "    X_only_fixed_np, X_fixed_and_random_np, y_np, y_lags_np, dt_lags_np, pid_np, time_ix_np,\n",
    "    build_model_fn=build_model_fn,\n",
    "    wrapper_cls=KANMixedEffectsRegWrapper,\n",
    "    mode=\"cv_only\",\n",
    "    scenario=\"time\",\n",
    "    arch_defaults=arch_defaults,\n",
    "    train_defaults=train_defaults,\n",
    "    outer_folds=5\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
